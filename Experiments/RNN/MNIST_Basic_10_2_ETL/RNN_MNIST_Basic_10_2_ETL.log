Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            31,400
├─Linear: 1-2                            1,640
=================================================================
Total params: 33,040
Trainable params: 33,040
Non-trainable params: 0
=================================================================
Encoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            31,400
├─Linear: 1-2                            1,640
=================================================================
Total params: 33,040
Trainable params: 33,040
Non-trainable params: 0
=================================================================
Encoder Encoder to Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderEncoderToEncoder                  --
├─Linear: 1-1                            3,240
├─Linear: 1-2                            1,640
=================================================================
Total params: 4,880
Trainable params: 4,880
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            82
├─Linear: 1-2                            82
=================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            82
├─Linear: 1-2                            82
=================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            82
├─Linear: 1-2                            82
=================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            82
├─Linear: 1-2                            82
=================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            82
├─Linear: 1-2                            82
=================================================================
Total params: 164
Trainable params: 164
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            120
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            120
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            120
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            120
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            120
=================================================================
Total params: 120
Trainable params: 120
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            1,640
├─Linear: 1-2                            32,144
=================================================================
Total params: 33,784
Trainable params: 33,784
Non-trainable params: 0
=================================================================
[Epoch   1 (17.06s)]	ELBO: 1217.004, 1250.415, 1254.193, 1254.806, 1253.875 (1398.934)	Log prob: 1238.714, 1275.202, 1279.828, 1281.110, 1280.774 (1413.235)	KLD: 21.709, 24.787, 25.634, 26.304, 26.899 (14.301)	Grad: 0.227, 0.266, 0.297, 0.306, 0.332
[Epoch   2 (16.75s)]	ELBO: 1413.003, 1424.231, 1424.207, 1424.047, 1423.524 (1461.912)	Log prob: 1421.885, 1435.413, 1435.974, 1436.615, 1436.840 (1475.990)	KLD: 8.882, 11.182, 11.767, 12.567, 13.316 (14.078)	Grad: 0.118, 0.120, 0.148, 0.185, 0.228
[Epoch   3 (16.64s)]	ELBO: 1438.984, 1492.178, 1493.349, 1493.126, 1492.400 (1519.791)	Log prob: 1446.615, 1504.067, 1505.971, 1506.585, 1506.401 (1533.599)	KLD: 7.630, 11.890, 12.622, 13.459, 14.001 (13.808)	Grad: 0.091, 0.118, 0.165, 0.217, 0.272
[Epoch   4 (16.63s)]	ELBO: 1458.112, 1535.192, 1536.636, 1536.226, 1536.195 (1552.463)	Log prob: 1465.287, 1547.540, 1549.898, 1549.935, 1550.207 (1566.692)	KLD: 7.175, 12.346, 13.262, 13.709, 14.012 (14.229)	Grad: 0.097, 0.122, 0.170, 0.226, 0.285
[Epoch   5 (18.15s)]	ELBO: 1469.246, 1556.492, 1565.574, 1565.541, 1565.491 (1581.244)	Log prob: 1476.427, 1569.025, 1580.164, 1580.317, 1580.556 (1597.223)	KLD: 7.180, 12.533, 14.591, 14.778, 15.066 (15.979)	Grad: 0.097, 0.134, 0.186, 0.248, 0.315
[Epoch   6 (18.21s)]	ELBO: 1475.245, 1568.148, 1594.789, 1594.773, 1594.636 (1604.908)	Log prob: 1482.384, 1580.677, 1610.944, 1611.016, 1610.989 (1621.264)	KLD: 7.139, 12.529, 16.155, 16.243, 16.353 (16.356)	Grad: 0.113, 0.153, 0.201, 0.265, 0.338
[Epoch   7 (17.78s)]	ELBO: 1480.093, 1576.981, 1613.275, 1613.256, 1613.280 (1618.827)	Log prob: 1487.240, 1589.613, 1629.728, 1629.757, 1629.825 (1635.192)	KLD: 7.148, 12.633, 16.453, 16.501, 16.545 (16.365)	Grad: 0.131, 0.175, 0.223, 0.290, 0.367
[Epoch   8 (18.19s)]	ELBO: 1483.841, 1583.687, 1623.732, 1623.750, 1623.705 (1627.136)	Log prob: 1490.946, 1596.317, 1640.194, 1640.241, 1640.223 (1643.292)	KLD: 7.105, 12.630, 16.463, 16.490, 16.518 (16.157)	Grad: 0.138, 0.184, 0.232, 0.300, 0.380
[Epoch   9 (18.45s)]	ELBO: 1486.968, 1588.978, 1630.943, 1630.929, 1630.910 (1632.814)	Log prob: 1494.040, 1601.585, 1647.425, 1647.425, 1647.426 (1649.226)	KLD: 7.072, 12.608, 16.480, 16.498, 16.517 (16.411)	Grad: 0.141, 0.189, 0.236, 0.303, 0.382
[Epoch  10 (18.37s)]	ELBO: 1489.752, 1593.266, 1636.496, 1636.472, 1636.486 (1637.624)	Log prob: 1496.800, 1605.877, 1653.004, 1652.991, 1653.020 (1653.855)	KLD: 7.048, 12.611, 16.506, 16.519, 16.534 (16.231)	Grad: 0.141, 0.190, 0.238, 0.307, 0.387
[Epoch  11 (17.85s)]	ELBO: 1492.430, 1596.782, 1640.984, 1640.947, 1640.940 (1642.387)	Log prob: 1499.453, 1609.366, 1657.496, 1657.469, 1657.472 (1658.427)	KLD: 7.024, 12.585, 16.513, 16.522, 16.534 (16.041)	Grad: 0.142, 0.195, 0.245, 0.316, 0.399
[Epoch  12 (21.57s)]	ELBO: 1494.944, 1599.770, 1644.775, 1644.788, 1644.766 (1645.835)	Log prob: 1501.955, 1612.343, 1661.305, 1661.326, 1661.314 (1662.472)	KLD: 7.012, 12.573, 16.530, 16.538, 16.547 (16.637)	Grad: 0.140, 0.192, 0.240, 0.309, 0.391
[Epoch  13 (20.85s)]	ELBO: 1497.076, 1602.249, 1648.252, 1648.287, 1648.271 (1648.731)	Log prob: 1504.073, 1614.821, 1664.813, 1664.855, 1664.846 (1665.380)	KLD: 6.997, 12.573, 16.561, 16.567, 16.575 (16.649)	Grad: 0.141, 0.196, 0.249, 0.321, 0.405
[Epoch  14 (16.95s)]	ELBO: 1499.161, 1604.409, 1651.291, 1651.290, 1651.276 (1652.339)	Log prob: 1506.158, 1616.972, 1667.862, 1667.869, 1667.860 (1668.845)	KLD: 6.997, 12.563, 16.572, 16.578, 16.584 (16.506)	Grad: 0.143, 0.203, 0.261, 0.340, 0.430
[Epoch  15 (16.21s)]	ELBO: 1501.067, 1606.354, 1654.076, 1654.128, 1654.154 (1654.678)	Log prob: 1508.065, 1618.910, 1670.670, 1670.728, 1670.759 (1671.060)	KLD: 6.998, 12.556, 16.594, 16.600, 16.605 (16.382)	Grad: 0.143, 0.204, 0.262, 0.339, 0.428
[Epoch  16 (18.72s)]	ELBO: 1503.017, 1608.153, 1656.569, 1656.618, 1656.612 (1656.080)	Log prob: 1510.012, 1620.713, 1673.189, 1673.246, 1673.241 (1672.465)	KLD: 6.994, 12.558, 16.621, 16.627, 16.631 (16.386)	Grad: 0.142, 0.204, 0.262, 0.340, 0.430
[Epoch  17 (22.37s)]	ELBO: 1504.640, 1609.885, 1658.898, 1658.920, 1658.941 (1658.407)	Log prob: 1511.633, 1622.434, 1675.538, 1675.567, 1675.591 (1674.967)	KLD: 6.993, 12.549, 16.640, 16.646, 16.650 (16.560)	Grad: 0.142, 0.203, 0.256, 0.330, 0.416
[Epoch  18 (27.52s)]	ELBO: 1506.070, 1611.234, 1660.727, 1660.744, 1660.754 (1660.204)	Log prob: 1513.067, 1623.779, 1677.362, 1677.385, 1677.398 (1676.450)	KLD: 6.996, 12.545, 16.635, 16.641, 16.644 (16.246)	Grad: 0.143, 0.206, 0.264, 0.342, 0.433
[Epoch  19 (25.77s)]	ELBO: 1507.390, 1612.699, 1662.600, 1662.583, 1662.599 (1663.207)	Log prob: 1514.372, 1625.228, 1679.240, 1679.229, 1679.248 (1679.815)	KLD: 6.983, 12.530, 16.639, 16.645, 16.648 (16.607)	Grad: 0.143, 0.206, 0.261, 0.337, 0.425
[Epoch  20 (26.10s)]	ELBO: 1508.335, 1613.756, 1663.988, 1663.960, 1663.932 (1663.969)	Log prob: 1515.318, 1626.286, 1680.633, 1680.613, 1680.588 (1680.726)	KLD: 6.983, 12.529, 16.645, 16.653, 16.656 (16.757)	Grad: 0.143, 0.208, 0.266, 0.345, 0.437
[Epoch  21 (25.57s)]	ELBO: 1509.297, 1614.639, 1665.291, 1665.262, 1665.253 (1665.668)	Log prob: 1516.271, 1627.172, 1681.962, 1681.943, 1681.936 (1682.272)	KLD: 6.973, 12.532, 16.671, 16.681, 16.684 (16.604)	Grad: 0.143, 0.208, 0.264, 0.341, 0.430
[Epoch  22 (25.67s)]	ELBO: 1510.489, 1615.777, 1666.677, 1666.703, 1666.698 (1665.900)	Log prob: 1517.473, 1628.313, 1683.354, 1683.394, 1683.394 (1682.790)	KLD: 6.985, 12.537, 16.677, 16.691, 16.695 (16.890)	Grad: 0.141, 0.206, 0.262, 0.339, 0.429
[Epoch  23 (28.89s)]	ELBO: 1511.259, 1616.529, 1667.648, 1667.701, 1667.643 (1666.479)	Log prob: 1518.237, 1629.057, 1684.332, 1684.406, 1684.354 (1683.054)	KLD: 6.978, 12.529, 16.684, 16.705, 16.712 (16.575)	Grad: 0.143, 0.209, 0.265, 0.343, 0.433
[Epoch  24 (33.23s)]	ELBO: 1512.162, 1617.476, 1668.816, 1668.921, 1668.928 (1667.987)	Log prob: 1519.142, 1630.004, 1685.501, 1685.644, 1685.665 (1684.675)	KLD: 6.981, 12.528, 16.685, 16.724, 16.737 (16.688)	Grad: 0.142, 0.207, 0.262, 0.339, 0.428
[Epoch  25 (39.04s)]	ELBO: 1512.832, 1618.196, 1669.780, 1669.949, 1669.980 (1670.402)	Log prob: 1519.809, 1630.724, 1686.466, 1686.769, 1686.827 (1687.214)	KLD: 6.978, 12.527, 16.686, 16.821, 16.847 (16.811)	Grad: 0.141, 0.206, 0.261, 0.337, 0.426
[Epoch  26 (60.83s)]	ELBO: 1513.470, 1618.748, 1670.174, 1672.884, 1672.833 (1673.351)	Log prob: 1520.438, 1631.262, 1686.840, 1690.432, 1690.414 (1691.066)	KLD: 6.968, 12.512, 16.667, 17.549, 17.580 (17.715)	Grad: 0.141, 0.209, 0.267, 0.347, 0.439
[Epoch  27 (68.50s)]	ELBO: 1514.062, 1619.361, 1670.701, 1676.131, 1676.099 (1675.829)	Log prob: 1521.027, 1631.880, 1687.375, 1694.055, 1694.040 (1693.842)	KLD: 6.966, 12.519, 16.675, 17.923, 17.940 (18.013)	Grad: 0.142, 0.211, 0.270, 0.350, 0.443
[Epoch  28 (58.96s)]	ELBO: 1514.809, 1620.024, 1671.419, 1678.003, 1677.986 (1676.847)	Log prob: 1521.784, 1632.540, 1688.096, 1696.023, 1696.017 (1694.867)	KLD: 6.976, 12.516, 16.676, 18.022, 18.033 (18.020)	Grad: 0.139, 0.207, 0.264, 0.342, 0.434
[Epoch  29 (58.28s)]	ELBO: 1515.318, 1620.693, 1672.128, 1679.372, 1679.457 (1678.691)	Log prob: 1522.303, 1633.221, 1688.820, 1697.474, 1697.569 (1696.637)	KLD: 6.986, 12.525, 16.690, 18.102, 18.112 (17.946)	Grad: 0.140, 0.209, 0.267, 0.348, 0.441
[Epoch  30 (55.12s)]	ELBO: 1515.854, 1621.129, 1672.632, 1680.526, 1680.531 (1679.540)	Log prob: 1522.833, 1633.645, 1689.309, 1698.660, 1698.676 (1697.634)	KLD: 6.979, 12.518, 16.678, 18.133, 18.143 (18.095)	Grad: 0.141, 0.211, 0.270, 0.350, 0.443
[Epoch  31 (64.12s)]	ELBO: 1516.327, 1621.821, 1673.310, 1681.590, 1681.627 (1681.442)	Log prob: 1523.326, 1634.368, 1690.024, 1699.801, 1699.846 (1699.212)	KLD: 6.998, 12.546, 16.713, 18.210, 18.219 (17.769)	Grad: 0.141, 0.211, 0.269, 0.348, 0.439
[Epoch  32 (58.06s)]	ELBO: 1516.870, 1622.503, 1673.912, 1682.546, 1682.549 (1682.728)	Log prob: 1523.871, 1635.048, 1690.635, 1700.813, 1700.826 (1700.935)	KLD: 7.000, 12.545, 16.721, 18.267, 18.278 (18.207)	Grad: 0.141, 0.210, 0.268, 0.348, 0.440
[Epoch  33 (57.71s)]	ELBO: 1517.329, 1622.953, 1674.452, 1683.508, 1683.602 (1683.046)	Log prob: 1524.330, 1635.495, 1691.165, 1701.887, 1701.998 (1701.317)	KLD: 7.003, 12.542, 16.713, 18.380, 18.396 (18.271)	Grad: 0.142, 0.213, 0.272, 0.352, 0.446
[Epoch  34 (61.61s)]	ELBO: 1517.884, 1623.640, 1674.912, 1685.187, 1685.218 (1685.895)	Log prob: 1524.891, 1636.176, 1691.619, 1703.986, 1704.040 (1704.795)	KLD: 7.008, 12.536, 16.707, 18.798, 18.821 (18.900)	Grad: 0.141, 0.211, 0.268, 0.346, 0.438
[Epoch  35 (61.18s)]	ELBO: 1517.993, 1623.861, 1675.067, 1687.207, 1687.275 (1687.425)	Log prob: 1524.998, 1636.385, 1691.762, 1706.411, 1706.498 (1706.635)	KLD: 7.006, 12.523, 16.694, 19.205, 19.222 (19.210)	Grad: 0.143, 0.216, 0.275, 0.354, 0.446
[Epoch  36 (59.61s)]	ELBO: 1518.557, 1624.510, 1675.685, 1689.101, 1689.192 (1689.611)	Log prob: 1525.571, 1637.052, 1692.401, 1708.496, 1708.600 (1709.005)	KLD: 7.013, 12.542, 16.717, 19.395, 19.407 (19.393)	Grad: 0.141, 0.213, 0.269, 0.346, 0.437
[Epoch  37 (60.65s)]	ELBO: 1518.969, 1625.053, 1676.086, 1690.371, 1690.394 (1690.087)	Log prob: 1525.989, 1637.596, 1692.803, 1709.853, 1709.883 (1709.761)	KLD: 7.020, 12.542, 16.717, 19.482, 19.489 (19.675)	Grad: 0.141, 0.214, 0.273, 0.353, 0.446
[Epoch  38 (70.61s)]	ELBO: 1519.224, 1625.406, 1676.480, 1691.378, 1691.410 (1691.450)	Log prob: 1526.244, 1637.953, 1693.215, 1710.934, 1710.972 (1711.215)	KLD: 7.020, 12.548, 16.733, 19.556, 19.561 (19.765)	Grad: 0.142, 0.216, 0.276, 0.353, 0.445
[Epoch  39 (69.45s)]	ELBO: 1519.670, 1625.991, 1677.024, 1692.393, 1692.376 (1692.021)	Log prob: 1526.691, 1638.545, 1693.759, 1711.985, 1711.972 (1711.791)	KLD: 7.021, 12.553, 16.735, 19.592, 19.595 (19.770)	Grad: 0.143, 0.217, 0.275, 0.352, 0.443
[Epoch  40 (68.12s)]	ELBO: 1520.105, 1626.538, 1677.448, 1693.289, 1693.316 (1693.219)	Log prob: 1527.140, 1639.106, 1694.201, 1712.946, 1712.976 (1712.967)	KLD: 7.034, 12.567, 16.753, 19.656, 19.658 (19.748)	Grad: 0.142, 0.215, 0.274, 0.350, 0.441
[Epoch  41 (69.43s)]	ELBO: 1520.218, 1626.819, 1677.724, 1693.940, 1693.896 (1693.785)	Log prob: 1527.263, 1639.402, 1694.494, 1713.629, 1713.586 (1713.437)	KLD: 7.045, 12.583, 16.770, 19.690, 19.691 (19.652)	Grad: 0.142, 0.216, 0.274, 0.349, 0.439
[Epoch  42 (74.37s)]	ELBO: 1520.623, 1627.255, 1678.120, 1694.604, 1694.654 (1694.249)	Log prob: 1527.670, 1639.837, 1694.898, 1714.324, 1714.374 (1713.804)	KLD: 7.049, 12.582, 16.777, 19.719, 19.721 (19.555)	Grad: 0.141, 0.216, 0.275, 0.352, 0.443
[Epoch  43 (74.18s)]	ELBO: 1521.000, 1627.719, 1678.672, 1695.310, 1695.380 (1695.565)	Log prob: 1528.059, 1640.320, 1695.457, 1715.058, 1715.130 (1715.249)	KLD: 7.058, 12.601, 16.785, 19.747, 19.748 (19.683)	Grad: 0.142, 0.219, 0.281, 0.360, 0.453
[Epoch  44 (68.51s)]	ELBO: 1521.296, 1628.067, 1678.932, 1695.934, 1695.934 (1695.873)	Log prob: 1528.353, 1640.660, 1695.726, 1715.696, 1715.695 (1715.493)	KLD: 7.057, 12.594, 16.793, 19.760, 19.761 (19.620)	Grad: 0.142, 0.218, 0.281, 0.359, 0.452
[Epoch  45 (78.69s)]	ELBO: 1521.566, 1628.474, 1679.441, 1696.541, 1696.581 (1695.765)	Log prob: 1528.629, 1641.073, 1696.240, 1716.328, 1716.369 (1715.491)	KLD: 7.063, 12.599, 16.800, 19.788, 19.789 (19.726)	Grad: 0.141, 0.217, 0.276, 0.352, 0.442
[Epoch  46 (72.18s)]	ELBO: 1521.991, 1628.915, 1679.739, 1697.103, 1697.199 (1696.270)	Log prob: 1529.066, 1641.524, 1696.548, 1716.917, 1717.012 (1715.969)	KLD: 7.074, 12.609, 16.809, 19.813, 19.814 (19.699)	Grad: 0.141, 0.218, 0.278, 0.355, 0.446
[Epoch  47 (68.09s)]	ELBO: 1522.212, 1629.341, 1680.018, 1697.571, 1697.584 (1696.749)	Log prob: 1529.287, 1641.955, 1696.832, 1717.400, 1717.412 (1716.746)	KLD: 7.076, 12.613, 16.814, 19.830, 19.831 (19.998)	Grad: 0.142, 0.216, 0.278, 0.356, 0.449
[Epoch  48 (85.03s)]	ELBO: 1522.406, 1629.570, 1680.312, 1697.971, 1698.045 (1696.860)	Log prob: 1529.484, 1642.194, 1697.143, 1717.821, 1717.896 (1716.511)	KLD: 7.077, 12.623, 16.829, 19.850, 19.852 (19.650)	Grad: 0.142, 0.219, 0.281, 0.359, 0.452
[Epoch  49 (82.40s)]	ELBO: 1522.816, 1629.987, 1680.683, 1698.548, 1698.531 (1697.763)	Log prob: 1529.900, 1642.614, 1697.516, 1718.426, 1718.409 (1717.514)	KLD: 7.085, 12.628, 16.834, 19.877, 19.878 (19.752)	Grad: 0.140, 0.217, 0.279, 0.357, 0.449
[Epoch  50 (85.41s)]	ELBO: 1523.070, 1630.304, 1681.081, 1698.985, 1699.004 (1697.824)	Log prob: 1530.152, 1642.917, 1697.890, 1718.829, 1718.849 (1717.657)	KLD: 7.082, 12.613, 16.808, 19.844, 19.846 (19.833)	Grad: 0.141, 0.217, 0.279, 0.356, 0.448
[Epoch  51 (70.94s)]	ELBO: 1523.368, 1630.716, 1681.402, 1699.467, 1699.477 (1697.921)	Log prob: 1530.462, 1643.352, 1698.237, 1719.341, 1719.354 (1717.691)	KLD: 7.094, 12.635, 16.836, 19.875, 19.876 (19.770)	Grad: 0.141, 0.216, 0.277, 0.354, 0.445
[Epoch  52 (76.78s)]	ELBO: 1523.510, 1630.909, 1681.625, 1699.823, 1699.785 (1698.025)	Log prob: 1530.612, 1643.553, 1698.480, 1719.731, 1719.695 (1717.902)	KLD: 7.103, 12.644, 16.856, 19.909, 19.910 (19.877)	Grad: 0.140, 0.217, 0.278, 0.355, 0.447
[Epoch  53 (72.92s)]	ELBO: 1523.716, 1631.141, 1681.840, 1700.122, 1700.141 (1698.767)	Log prob: 1530.815, 1643.774, 1698.675, 1720.011, 1720.031 (1718.405)	KLD: 7.099, 12.632, 16.835, 19.889, 19.890 (19.638)	Grad: 0.141, 0.219, 0.283, 0.361, 0.454
[Epoch  54 (66.65s)]	ELBO: 1524.105, 1631.531, 1682.293, 1700.650, 1700.676 (1699.955)	Log prob: 1531.203, 1644.182, 1699.146, 1720.576, 1720.603 (1719.842)	KLD: 7.098, 12.650, 16.852, 19.925, 19.926 (19.888)	Grad: 0.140, 0.216, 0.278, 0.356, 0.447
[Epoch  55 (64.45s)]	ELBO: 1524.261, 1631.860, 1682.444, 1700.909, 1700.947 (1700.208)	Log prob: 1531.356, 1644.501, 1699.297, 1720.847, 1720.885 (1720.263)	KLD: 7.095, 12.643, 16.853, 19.938, 19.939 (20.054)	Grad: 0.144, 0.223, 0.290, 0.371, 0.465
[Epoch  56 (59.57s)]	ELBO: 1524.606, 1632.258, 1682.843, 1701.320, 1701.372 (1698.860)	Log prob: 1531.720, 1644.919, 1699.718, 1721.276, 1721.329 (1718.504)	KLD: 7.114, 12.661, 16.874, 19.955, 19.956 (19.643)	Grad: 0.141, 0.219, 0.282, 0.360, 0.451
[Epoch  57 (55.33s)]	ELBO: 1524.847, 1632.432, 1683.098, 1701.682, 1701.690 (1700.117)	Log prob: 1531.961, 1645.086, 1699.958, 1721.631, 1721.642 (1720.060)	KLD: 7.115, 12.654, 16.860, 19.949, 19.950 (19.942)	Grad: 0.140, 0.217, 0.279, 0.356, 0.448
[Epoch  58 (53.95s)]	ELBO: 1525.094, 1632.780, 1683.329, 1702.043, 1702.034 (1698.927)	Log prob: 1532.202, 1645.438, 1700.190, 1721.997, 1721.989 (1718.974)	KLD: 7.108, 12.660, 16.862, 19.954, 19.955 (20.046)	Grad: 0.141, 0.219, 0.282, 0.360, 0.452
[Epoch  59 (54.38s)]	ELBO: 1525.282, 1632.903, 1683.551, 1702.338, 1702.336 (1700.793)	Log prob: 1532.396, 1645.571, 1700.422, 1722.302, 1722.302 (1720.728)	KLD: 7.113, 12.668, 16.871, 19.965, 19.966 (19.936)	Grad: 0.141, 0.219, 0.282, 0.358, 0.449
[Epoch  60 (55.85s)]	ELBO: 1525.657, 1633.282, 1683.841, 1702.593, 1702.634 (1701.746)	Log prob: 1532.775, 1645.953, 1700.716, 1722.566, 1722.609 (1721.694)	KLD: 7.117, 12.671, 16.876, 19.973, 19.974 (19.948)	Grad: 0.142, 0.220, 0.282, 0.360, 0.452
[Epoch  61 (64.55s)]	ELBO: 1525.892, 1633.527, 1684.187, 1702.968, 1703.010 (1701.337)	Log prob: 1533.009, 1646.200, 1701.066, 1722.944, 1722.989 (1721.492)	KLD: 7.117, 12.673, 16.879, 19.977, 19.978 (20.155)	Grad: 0.140, 0.218, 0.282, 0.358, 0.449
[Epoch  62 (59.69s)]	ELBO: 1526.087, 1633.762, 1684.306, 1703.066, 1703.081 (1701.054)	Log prob: 1533.211, 1646.443, 1701.193, 1723.053, 1723.069 (1721.258)	KLD: 7.125, 12.680, 16.888, 19.988, 19.989 (20.204)	Grad: 0.141, 0.220, 0.286, 0.366, 0.460
[Epoch  63 (62.31s)]	ELBO: 1526.429, 1633.996, 1684.578, 1703.412, 1703.474 (1701.401)	Log prob: 1533.561, 1646.686, 1701.480, 1723.421, 1723.482 (1721.205)	KLD: 7.132, 12.691, 16.902, 20.008, 20.009 (19.804)	Grad: 0.140, 0.219, 0.283, 0.360, 0.451
[Epoch  64 (65.56s)]	ELBO: 1526.773, 1634.392, 1684.985, 1703.815, 1703.843 (1701.959)	Log prob: 1533.903, 1647.084, 1701.879, 1723.812, 1723.841 (1722.134)	KLD: 7.130, 12.692, 16.895, 19.996, 19.998 (20.175)	Grad: 0.139, 0.216, 0.279, 0.354, 0.445
[Epoch  65 (66.14s)]	ELBO: 1526.779, 1634.354, 1684.885, 1703.975, 1704.008 (1701.708)	Log prob: 1533.915, 1647.055, 1701.793, 1724.002, 1724.034 (1721.678)	KLD: 7.136, 12.700, 16.909, 20.025, 20.027 (19.971)	Grad: 0.141, 0.220, 0.287, 0.366, 0.458
[Epoch  66 (68.27s)]	ELBO: 1527.170, 1634.824, 1685.319, 1704.254, 1704.324 (1702.012)	Log prob: 1534.306, 1647.524, 1702.235, 1724.291, 1724.362 (1721.845)	KLD: 7.135, 12.702, 16.916, 20.038, 20.039 (19.833)	Grad: 0.138, 0.215, 0.278, 0.354, 0.443
[Epoch  67 (63.52s)]	ELBO: 1527.369, 1634.887, 1685.411, 1704.442, 1704.502 (1701.861)	Log prob: 1534.509, 1647.596, 1702.337, 1724.474, 1724.536 (1721.868)	KLD: 7.140, 12.709, 16.924, 20.033, 20.034 (20.007)	Grad: 0.140, 0.219, 0.285, 0.363, 0.456
[Epoch  68 (63.18s)]	ELBO: 1527.648, 1635.186, 1685.756, 1704.749, 1704.794 (1702.993)	Log prob: 1534.792, 1647.901, 1702.688, 1724.800, 1724.846 (1722.787)	KLD: 7.144, 12.715, 16.931, 20.051, 20.052 (19.795)	Grad: 0.140, 0.218, 0.281, 0.357, 0.448
[Epoch  69 (70.62s)]	ELBO: 1527.656, 1635.272, 1685.742, 1704.846, 1704.894 (1703.441)	Log prob: 1534.797, 1647.989, 1702.666, 1724.884, 1724.933 (1723.421)	KLD: 7.141, 12.718, 16.925, 20.039, 20.040 (19.980)	Grad: 0.139, 0.217, 0.281, 0.356, 0.446
[Epoch  70 (70.25s)]	ELBO: 1527.979, 1635.521, 1685.941, 1704.996, 1704.985 (1703.166)	Log prob: 1535.121, 1648.240, 1702.860, 1725.040, 1725.030 (1723.250)	KLD: 7.142, 12.718, 16.920, 20.044, 20.046 (20.084)	Grad: 0.140, 0.219, 0.284, 0.364, 0.458
[Epoch  71 (76.74s)]	ELBO: 1528.377, 1635.783, 1686.281, 1705.381, 1705.390 (1703.279)	Log prob: 1535.527, 1648.521, 1703.233, 1725.456, 1725.467 (1723.146)	KLD: 7.151, 12.739, 16.951, 20.075, 20.077 (19.867)	Grad: 0.139, 0.217, 0.284, 0.362, 0.454
[Epoch  72 (74.83s)]	ELBO: 1528.430, 1635.931, 1686.415, 1705.524, 1705.562 (1703.584)	Log prob: 1535.583, 1648.672, 1703.364, 1725.588, 1725.629 (1723.654)	KLD: 7.154, 12.741, 16.950, 20.065, 20.067 (20.070)	Grad: 0.140, 0.219, 0.286, 0.365, 0.457
[Epoch  73 (76.28s)]	ELBO: 1528.653, 1636.111, 1686.516, 1705.720, 1705.727 (1703.848)	Log prob: 1535.802, 1648.846, 1703.454, 1725.785, 1725.795 (1723.825)	KLD: 7.148, 12.734, 16.938, 20.066, 20.068 (19.976)	Grad: 0.139, 0.219, 0.285, 0.364, 0.456
[Epoch  74 (70.47s)]	ELBO: 1529.013, 1636.472, 1686.879, 1706.005, 1706.033 (1702.639)	Log prob: 1536.175, 1649.226, 1703.847, 1726.097, 1726.126 (1722.644)	KLD: 7.162, 12.753, 16.968, 20.092, 20.093 (20.005)	Grad: 0.138, 0.216, 0.281, 0.359, 0.452
[Epoch  75 (65.10s)]	ELBO: 1529.138, 1636.558, 1686.981, 1706.127, 1706.127 (1704.173)	Log prob: 1536.300, 1649.304, 1703.939, 1726.204, 1726.205 (1724.090)	KLD: 7.161, 12.747, 16.958, 20.077, 20.079 (19.918)	Grad: 0.139, 0.218, 0.284, 0.362, 0.455
[Epoch  76 (72.58s)]	ELBO: 1529.473, 1636.835, 1687.252, 1706.443, 1706.428 (1703.841)	Log prob: 1536.634, 1649.589, 1704.221, 1726.539, 1726.526 (1724.127)	KLD: 7.160, 12.753, 16.969, 20.097, 20.099 (20.286)	Grad: 0.139, 0.217, 0.282, 0.359, 0.450
[Epoch  77 (77.66s)]	ELBO: 1529.652, 1637.029, 1687.342, 1706.531, 1706.585 (1704.538)	Log prob: 1536.813, 1649.784, 1704.312, 1726.623, 1726.682 (1724.667)	KLD: 7.161, 12.755, 16.971, 20.093, 20.096 (20.129)	Grad: 0.139, 0.218, 0.285, 0.364, 0.457
[Epoch  78 (71.13s)]	ELBO: 1529.870, 1637.169, 1687.461, 1706.670, 1706.702 (1704.039)	Log prob: 1537.027, 1649.922, 1704.418, 1726.771, 1726.805 (1724.079)	KLD: 7.156, 12.752, 16.958, 20.101, 20.104 (20.040)	Grad: 0.138, 0.217, 0.285, 0.364, 0.458
[Epoch  79 (67.25s)]	ELBO: 1530.179, 1637.423, 1687.782, 1707.026, 1707.098 (1704.420)	Log prob: 1537.351, 1650.186, 1704.762, 1727.131, 1727.210 (1724.578)	KLD: 7.171, 12.763, 16.981, 20.107, 20.110 (20.158)	Grad: 0.137, 0.215, 0.280, 0.356, 0.447
[Epoch  80 (66.48s)]	ELBO: 1530.273, 1637.444, 1687.834, 1707.041, 1707.072 (1704.619)	Log prob: 1537.447, 1650.209, 1704.802, 1727.138, 1727.172 (1724.520)	KLD: 7.172, 12.764, 16.969, 20.098, 20.100 (19.900)	Grad: 0.139, 0.218, 0.285, 0.364, 0.457
[Epoch  81 (65.14s)]	ELBO: 1530.517, 1637.642, 1687.994, 1707.213, 1707.262 (1704.808)	Log prob: 1537.686, 1650.416, 1704.981, 1727.324, 1727.377 (1724.616)	KLD: 7.169, 12.774, 16.986, 20.112, 20.114 (19.808)	Grad: 0.139, 0.218, 0.284, 0.363, 0.456
[Epoch  82 (56.07s)]	ELBO: 1530.701, 1637.925, 1688.139, 1707.452, 1707.460 (1704.863)	Log prob: 1537.882, 1650.707, 1705.132, 1727.571, 1727.582 (1724.947)	KLD: 7.181, 12.781, 16.993, 20.121, 20.123 (20.084)	Grad: 0.137, 0.215, 0.280, 0.356, 0.447
[Epoch  83 (61.34s)]	ELBO: 1530.970, 1638.156, 1688.327, 1707.591, 1707.623 (1704.956)	Log prob: 1538.145, 1650.935, 1705.310, 1727.711, 1727.747 (1724.818)	KLD: 7.174, 12.779, 16.985, 20.121, 20.124 (19.863)	Grad: 0.138, 0.217, 0.286, 0.367, 0.461
[Epoch  84 (67.18s)]	ELBO: 1531.148, 1638.296, 1688.447, 1707.721, 1707.751 (1705.063)	Log prob: 1538.329, 1651.081, 1705.448, 1727.853, 1727.885 (1725.305)	KLD: 7.181, 12.788, 17.000, 20.132, 20.134 (20.242)	Grad: 0.138, 0.216, 0.283, 0.361, 0.454
[Epoch  85 (70.11s)]	ELBO: 1531.292, 1638.453, 1688.558, 1707.875, 1707.880 (1705.751)	Log prob: 1538.484, 1651.251, 1705.568, 1728.007, 1728.015 (1725.613)	KLD: 7.192, 12.797, 17.009, 20.132, 20.135 (19.862)	Grad: 0.139, 0.218, 0.286, 0.366, 0.460
[Epoch  86 (72.84s)]	ELBO: 1531.503, 1638.565, 1688.660, 1707.999, 1708.023 (1705.584)	Log prob: 1538.692, 1651.368, 1705.672, 1728.145, 1728.172 (1725.764)	KLD: 7.189, 12.802, 17.011, 20.146, 20.149 (20.180)	Grad: 0.138, 0.218, 0.287, 0.368, 0.462
[Epoch  87 (77.46s)]	ELBO: 1531.643, 1638.651, 1688.806, 1708.144, 1708.231 (1706.516)	Log prob: 1538.833, 1651.455, 1705.825, 1728.296, 1728.387 (1726.515)	KLD: 7.190, 12.805, 17.017, 20.152, 20.155 (19.999)	Grad: 0.137, 0.216, 0.283, 0.361, 0.453
[Epoch  88 (72.68s)]	ELBO: 1531.877, 1639.001, 1689.034, 1708.358, 1708.384 (1706.769)	Log prob: 1539.068, 1651.809, 1706.053, 1728.510, 1728.541 (1726.697)	KLD: 7.192, 12.807, 17.019, 20.153, 20.157 (19.929)	Grad: 0.137, 0.215, 0.282, 0.361, 0.453
[Epoch  89 (79.20s)]	ELBO: 1532.222, 1639.221, 1689.303, 1708.549, 1708.603 (1706.214)	Log prob: 1539.416, 1652.034, 1706.327, 1728.710, 1728.767 (1726.376)	KLD: 7.194, 12.812, 17.024, 20.162, 20.165 (20.162)	Grad: 0.137, 0.215, 0.283, 0.361, 0.454
[Epoch  90 (84.19s)]	ELBO: 1532.296, 1639.247, 1689.331, 1708.746, 1708.790 (1706.875)	Log prob: 1539.495, 1652.058, 1706.360, 1728.905, 1728.953 (1726.829)	KLD: 7.197, 12.811, 17.028, 20.160, 20.162 (19.954)	Grad: 0.138, 0.217, 0.285, 0.363, 0.455
[Epoch  91 (85.26s)]	ELBO: 1532.499, 1639.260, 1689.322, 1708.651, 1708.721 (1707.256)	Log prob: 1539.692, 1652.072, 1706.344, 1728.811, 1728.884 (1727.438)	KLD: 7.193, 12.812, 17.023, 20.159, 20.163 (20.182)	Grad: 0.138, 0.218, 0.287, 0.368, 0.462
[Epoch  92 (97.46s)]	ELBO: 1532.949, 1639.661, 1689.739, 1709.137, 1709.116 (1706.158)	Log prob: 1540.146, 1652.486, 1706.776, 1729.309, 1729.291 (1726.227)	KLD: 7.198, 12.825, 17.035, 20.171, 20.175 (20.068)	Grad: 0.137, 0.216, 0.284, 0.362, 0.453
[Epoch  93 (87.34s)]	ELBO: 1532.931, 1639.758, 1689.673, 1709.105, 1709.107 (1706.455)	Log prob: 1540.129, 1652.582, 1706.707, 1729.276, 1729.282 (1726.589)	KLD: 7.200, 12.824, 17.035, 20.170, 20.174 (20.134)	Grad: 0.137, 0.217, 0.285, 0.365, 0.458
[Epoch  94 (72.85s)]	ELBO: 1533.083, 1639.792, 1689.885, 1709.259, 1709.289 (1706.409)	Log prob: 1540.290, 1652.632, 1706.935, 1729.445, 1729.481 (1726.344)	KLD: 7.207, 12.843, 17.049, 20.187, 20.192 (19.935)	Grad: 0.137, 0.217, 0.284, 0.362, 0.454
[Epoch  95 (70.00s)]	ELBO: 1533.277, 1639.992, 1689.904, 1709.292, 1709.321 (1706.455)	Log prob: 1540.493, 1652.840, 1706.962, 1729.497, 1729.529 (1726.730)	KLD: 7.215, 12.848, 17.058, 20.204, 20.209 (20.275)	Grad: 0.137, 0.215, 0.283, 0.362, 0.455
[Epoch  96 (76.46s)]	ELBO: 1533.568, 1640.175, 1690.183, 1709.644, 1709.574 (1706.564)	Log prob: 1540.782, 1653.022, 1707.239, 1729.844, 1729.777 (1726.726)	KLD: 7.214, 12.848, 17.056, 20.200, 20.204 (20.162)	Grad: 0.138, 0.216, 0.283, 0.361, 0.452
[Epoch  97 (78.71s)]	ELBO: 1533.584, 1640.278, 1690.206, 1709.587, 1709.537 (1707.137)	Log prob: 1540.803, 1653.131, 1707.265, 1729.790, 1729.744 (1727.156)	KLD: 7.219, 12.853, 17.058, 20.203, 20.207 (20.019)	Grad: 0.137, 0.217, 0.286, 0.366, 0.459
[Epoch  98 (75.71s)]	ELBO: 1533.954, 1640.442, 1690.373, 1709.783, 1709.789 (1707.213)	Log prob: 1541.170, 1653.285, 1707.427, 1729.966, 1729.977 (1727.588)	KLD: 7.216, 12.843, 17.054, 20.185, 20.189 (20.375)	Grad: 0.136, 0.216, 0.283, 0.362, 0.455
[Epoch  99 (78.61s)]	ELBO: 1534.060, 1640.601, 1690.420, 1709.876, 1709.883 (1707.247)	Log prob: 1541.280, 1653.465, 1707.494, 1730.102, 1730.115 (1727.456)	KLD: 7.220, 12.864, 17.075, 20.226, 20.231 (20.209)	Grad: 0.136, 0.215, 0.282, 0.359, 0.449
[Epoch 100 (75.87s)]	ELBO: 1534.248, 1640.625, 1690.595, 1709.988, 1710.028 (1707.472)	Log prob: 1541.468, 1653.485, 1707.660, 1730.203, 1730.246 (1727.732)	KLD: 7.220, 12.859, 17.065, 20.215, 20.220 (20.259)	Grad: 0.137, 0.215, 0.284, 0.362, 0.454
Best epoch(s): [100]	Training time(s): 5741.54s (5741.54s)	Best ELBO: 1710.028 (1707.472)	Best log prob: 1730.246 (1727.732)
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
Avg. mu: -0.058, 0.123, 0.026, -0.024, -0.029, -0.124, -0.104, 0.044, -0.007, -0.004
Avg. var: 0.002, 0.002, 0.004, 0.006, 0.014, 0.020, 0.049, 0.039, 1.004, 1.000
Max. mu: 4.344, 4.758, 3.855, 3.181, 2.958, 3.995, 3.866, 5.143, 0.206, 0.318
Max. var: 0.037, 0.021, 0.031, 0.033, 0.060, 0.145, 0.183, 0.133, 1.049, 1.077
Min. mu: -4.824, -3.602, -4.339, -3.969, -3.719, -3.828, -4.315, -3.855, -0.231, -0.355
Min. var: 0.000, 0.000, 0.001, 0.001, 0.004, 0.005, 0.012, 0.014, 0.942, 0.951
Cov. mu:
[[1.751 0.187 -0.043 0.087 -0.021 0.028 -0.087 -0.158 -0.000 0.007]
 [0.187 1.249 0.054 -0.012 -0.039 -0.021 -0.027 0.019 -0.011 -0.021]
 [-0.043 0.054 1.010 0.051 0.017 0.033 -0.034 -0.019 0.002 -0.005]
 [0.087 -0.012 0.051 1.266 -0.011 0.024 -0.053 -0.049 -0.002 0.002]
 [-0.021 -0.039 0.017 -0.011 0.924 -0.025 0.011 0.042 -0.001 0.005]
 [0.028 -0.021 0.033 0.024 -0.025 1.019 0.013 0.017 0.002 0.001]
 [-0.087 -0.027 -0.034 -0.053 0.011 0.013 0.997 0.041 0.000 -0.001]
 [-0.158 0.019 -0.019 -0.049 0.042 0.017 0.041 0.947 -0.004 -0.006]
 [-0.000 -0.011 0.002 -0.002 -0.001 0.002 0.000 -0.004 0.002 0.003]
 [0.007 -0.021 -0.005 0.002 0.005 0.001 -0.001 -0.006 0.003 0.006]]
