Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            882,080
├─Linear: 1-2                            6,480
=================================================================
Total params: 888,560
Trainable params: 888,560
Non-trainable params: 0
=================================================================
Encoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            882,080
├─Linear: 1-2                            6,480
=================================================================
Total params: 888,560
Trainable params: 888,560
Non-trainable params: 0
=================================================================
Encoder Encoder to Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderEncoderToEncoder                  --
├─Linear: 1-1                            12,880
├─Linear: 1-2                            6,480
=================================================================
Total params: 19,360
Trainable params: 19,360
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            324
├─Linear: 1-2                            324
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            324
├─Linear: 1-2                            324
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            324
├─Linear: 1-2                            324
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            324
├─Linear: 1-2                            324
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            324
├─Linear: 1-2                            324
=================================================================
Total params: 648
Trainable params: 648
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            400
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            400
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            400
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            400
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            400
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            6,480
├─Linear: 1-2                            893,025
=================================================================
Total params: 899,505
Trainable params: 899,505
Non-trainable params: 0
=================================================================
[Epoch   1 (46.81s)]	ELBO: 12402.729, 13280.466, 13478.753, 13479.540, 13405.863 (20181.019)	Log prob: 12700.176, 13660.259, 13898.242, 13937.847, 13898.884 (20415.611)	KLD: 297.446, 82.345, 39.699, 38.817, 34.713 (234.591)	Grad: 80.148, 28.436, 10.595, 15.948, 12.114
[Epoch   2 (45.88s)]	ELBO: 20149.830, 20174.936, 20165.021, 20141.527, 20123.930 (20487.412)	Log prob: 20306.598, 20333.895, 20326.617, 20305.736, 20291.277 (20608.903)	KLD: 156.769, 2.189, 2.641, 2.612, 3.138 (121.491)	Grad: 35.559, 8.950, 2.881, 2.782, 2.883
[Epoch   3 (46.38s)]	ELBO: 20303.191, 20311.334, 20299.008, 20277.697, 20260.871 (20534.311)	Log prob: 20404.635, 20414.613, 20405.348, 20387.348, 20374.625 (20639.084)	KLD: 101.438, 1.837, 3.065, 3.307, 4.102 (104.773)	Grad: 31.233, 3.245, 1.687, 1.439, 1.385
[Epoch   4 (40.99s)]	ELBO: 20356.131, 20364.100, 20353.021, 20336.094, 20322.215 (20609.181)	Log prob: 20435.848, 20445.902, 20438.396, 20425.012, 20415.121 (20691.815)	KLD: 79.714, 2.086, 3.576, 3.540, 3.990 (82.635)	Grad: 27.549, 3.011, 1.873, 1.329, 1.369
[Epoch   5 (40.98s)]	ELBO: 20402.578, 20412.445, 20408.832, 20397.432, 20387.953 (20613.802)	Log prob: 20469.271, 20481.496, 20481.791, 20473.973, 20468.498 (20701.043)	KLD: 66.696, 2.352, 3.909, 3.585, 4.004 (87.240)	Grad: 23.275, 2.587, 2.686, 1.735, 1.661
[Epoch   6 (37.23s)]	ELBO: 20435.357, 20446.094, 20442.939, 20435.066, 20426.744 (20539.558)	Log prob: 20493.033, 20506.447, 20507.422, 20503.285, 20498.570 (20623.407)	KLD: 57.674, 2.675, 4.132, 3.743, 3.604 (83.850)	Grad: 24.915, 3.350, 3.744, 2.499, 2.783
[Epoch   7 (37.60s)]	ELBO: 20447.105, 20459.168, 20459.168, 20452.447, 20445.047 (20488.313)	Log prob: 20498.725, 20513.885, 20518.094, 20515.068, 20511.068 (20570.710)	KLD: 51.623, 3.087, 4.215, 3.698, 3.394 (82.397)	Grad: 26.736, 3.384, 3.505, 2.509, 2.856
[Epoch   8 (37.02s)]	ELBO: 20430.533, 20439.041, 20435.658, 20426.314, 20418.162 (20742.200)	Log prob: 20473.055, 20484.812, 20485.998, 20480.459, 20475.816 (20793.824)	KLD: 42.527, 3.248, 4.563, 3.811, 3.505 (51.624)	Grad: 24.976, 3.944, 4.230, 3.432, 3.470
[Epoch   9 (36.41s)]	ELBO: 20482.104, 20504.697, 20513.709, 20512.330, 20507.887 (20646.107)	Log prob: 20521.184, 20547.635, 20561.131, 20563.420, 20561.867 (20714.275)	KLD: 39.084, 3.857, 4.483, 3.663, 2.890 (68.168)	Grad: 18.877, 2.840, 3.001, 2.121, 1.955
[Epoch  10 (37.18s)]	ELBO: 20515.320, 20556.178, 20571.715, 20569.512, 20563.111 (20868.918)	Log prob: 20552.152, 20598.758, 20620.309, 20622.102, 20618.041 (20927.621)	KLD: 36.834, 5.746, 6.017, 3.990, 2.347 (58.704)	Grad: 15.601, 3.225, 3.434, 2.874, 2.162
[Epoch  11 (37.65s)]	ELBO: 20570.748, 20641.588, 20649.854, 20645.666, 20641.215 (20897.697)	Log prob: 20604.850, 20682.955, 20698.020, 20697.012, 20694.979 (20954.894)	KLD: 34.102, 7.270, 6.794, 3.180, 2.424 (57.197)	Grad: 21.823, 3.923, 4.783, 3.720, 2.446
[Epoch  12 (37.06s)]	ELBO: 20617.650, 20661.049, 20660.650, 20655.840, 20653.299 (20914.820)	Log prob: 20652.146, 20702.137, 20707.541, 20705.557, 20705.289 (20956.836)	KLD: 34.503, 6.580, 5.813, 2.817, 2.276 (42.017)	Grad: 21.478, 5.247, 5.369, 4.180, 2.616
[Epoch  13 (37.49s)]	ELBO: 20672.408, 20695.959, 20693.660, 20689.863, 20686.799 (20999.149)	Log prob: 20707.580, 20736.668, 20738.604, 20737.443, 20736.492 (21048.589)	KLD: 35.170, 5.539, 4.232, 2.644, 2.107 (49.440)	Grad: 25.881, 5.326, 5.034, 4.328, 2.641
[Epoch  14 (40.19s)]	ELBO: 20744.912, 20757.133, 20756.768, 20754.027, 20752.678 (21020.639)	Log prob: 20780.604, 20797.359, 20800.217, 20800.023, 20800.629 (21068.479)	KLD: 35.690, 4.537, 3.221, 2.547, 1.956 (47.840)	Grad: 19.054, 3.805, 3.558, 2.945, 1.968
[Epoch  15 (41.17s)]	ELBO: 20796.768, 20806.203, 20810.230, 20807.398, 20808.010 (21070.999)	Log prob: 20831.398, 20844.947, 20852.215, 20851.797, 20854.359 (21117.020)	KLD: 34.633, 4.115, 3.238, 2.412, 1.950 (46.019)	Grad: 13.685, 2.324, 1.871, 1.712, 1.150
[Epoch  16 (42.77s)]	ELBO: 20772.629, 20775.416, 20775.121, 20773.359, 20770.354 (20996.050)	Log prob: 20805.332, 20812.135, 20815.174, 20815.885, 20814.818 (21034.866)	KLD: 32.701, 4.015, 3.339, 2.469, 1.939 (38.818)	Grad: 20.104, 4.056, 3.108, 2.951, 2.214
[Epoch  17 (42.47s)]	ELBO: 20792.021, 20793.086, 20792.715, 20789.705, 20787.357 (21021.495)	Log prob: 20822.934, 20827.479, 20830.297, 20829.703, 20829.191 (21060.946)	KLD: 30.910, 3.481, 3.190, 2.418, 1.834 (39.452)	Grad: 20.394, 3.822, 3.243, 2.901, 1.918
[Epoch  18 (38.64s)]	ELBO: 20821.760, 20830.230, 20835.572, 20833.635, 20832.639 (21117.520)	Log prob: 20852.121, 20864.002, 20872.377, 20872.859, 20873.586 (21162.852)	KLD: 30.365, 3.399, 3.042, 2.420, 1.721 (45.332)	Grad: 13.758, 2.064, 1.721, 1.463, 1.159
[Epoch  19 (38.20s)]	ELBO: 20867.268, 20878.305, 20884.309, 20883.230, 20882.104 (21150.755)	Log prob: 20896.971, 20911.611, 20920.770, 20922.166, 20922.885 (21192.937)	KLD: 29.699, 3.611, 3.147, 2.479, 1.850 (42.180)	Grad: 12.349, 1.801, 1.380, 1.354, 1.056
[Epoch  20 (37.75s)]	ELBO: 20885.307, 20892.648, 20897.879, 20896.941, 20894.027 (21174.422)	Log prob: 20914.762, 20925.840, 20934.295, 20935.807, 20934.830 (21214.383)	KLD: 29.457, 3.737, 3.224, 2.450, 1.935 (39.960)	Grad: 15.840, 2.334, 1.828, 1.703, 1.360
[Epoch  21 (37.86s)]	ELBO: 20938.822, 20944.803, 20949.006, 20945.982, 20944.238 (21212.764)	Log prob: 20968.264, 20977.924, 20985.211, 20984.645, 20984.941 (21253.118)	KLD: 29.440, 3.683, 3.083, 2.455, 2.040 (40.353)	Grad: 11.296, 1.820, 1.181, 1.282, 0.983
[Epoch  22 (37.98s)]	ELBO: 20844.922, 20841.309, 20839.125, 20834.732, 20830.645 (21091.606)	Log prob: 20873.393, 20873.307, 20874.203, 20872.104, 20869.842 (21124.593)	KLD: 28.475, 3.523, 3.083, 2.287, 1.825 (32.986)	Grad: 33.034, 5.001, 3.171, 4.286, 3.115
[Epoch  23 (37.38s)]	ELBO: 20945.168, 20946.061, 20946.178, 20943.361, 20940.889 (21215.838)	Log prob: 20972.166, 20976.309, 20979.172, 20978.943, 20978.398 (21256.776)	KLD: 26.998, 3.245, 2.753, 2.589, 1.924 (40.938)	Grad: 20.385, 2.851, 2.332, 2.053, 1.866
[Epoch  24 (35.71s)]	ELBO: 20999.766, 21001.871, 21003.742, 21001.932, 20998.979 (21251.769)	Log prob: 21026.438, 21031.916, 21036.559, 21036.973, 21035.707 (21292.073)	KLD: 26.671, 3.376, 2.766, 2.222, 1.691 (40.305)	Grad: 11.897, 1.765, 1.364, 1.250, 1.058
[Epoch  25 (33.91s)]	ELBO: 20912.736, 20912.180, 20911.119, 20908.180, 20903.959 (21230.327)	Log prob: 20938.428, 20941.236, 20942.914, 20942.066, 20939.453 (21266.833)	KLD: 25.694, 3.359, 2.738, 2.098, 1.603 (36.506)	Grad: 24.809, 3.912, 3.038, 3.101, 2.235
[Epoch  26 (35.67s)]	ELBO: 21016.357, 21017.662, 21017.738, 21016.309, 21014.242 (21265.198)	Log prob: 21042.537, 21047.148, 21049.973, 21050.779, 21050.365 (21302.392)	KLD: 26.182, 3.311, 2.744, 2.237, 1.643 (37.194)	Grad: 11.155, 1.868, 1.509, 1.236, 1.072
[Epoch  27 (35.79s)]	ELBO: 21033.486, 21036.168, 21037.484, 21035.578, 21033.359 (21268.765)	Log prob: 21059.074, 21065.119, 21069.129, 21069.357, 21068.498 (21307.990)	KLD: 25.591, 3.363, 2.688, 2.141, 1.359 (39.225)	Grad: 10.068, 1.495, 1.196, 1.087, 0.906
[Epoch  28 (34.82s)]	ELBO: 21036.070, 21040.594, 21041.453, 21038.699, 21036.367 (21280.688)	Log prob: 21060.992, 21069.074, 21072.711, 21072.000, 21071.033 (21318.454)	KLD: 24.921, 3.559, 2.775, 2.049, 1.361 (37.766)	Grad: 10.240, 1.493, 1.247, 1.109, 0.960
[Epoch  29 (34.48s)]	ELBO: 21018.049, 21025.621, 21026.887, 21023.527, 21021.684 (21294.025)	Log prob: 21042.166, 21053.740, 21057.857, 21056.598, 21056.152 (21330.145)	KLD: 24.116, 4.003, 2.851, 2.099, 1.398 (36.120)	Grad: 13.219, 2.072, 1.642, 1.521, 1.145
[Epoch  30 (34.66s)]	ELBO: 21024.426, 21037.410, 21037.072, 21033.865, 21031.996 (21323.209)	Log prob: 21048.074, 21066.287, 21069.402, 21068.445, 21068.078 (21359.732)	KLD: 23.648, 5.226, 3.455, 2.245, 1.509 (36.522)	Grad: 20.834, 3.018, 2.473, 2.217, 1.599
[Epoch  31 (37.80s)]	ELBO: 21049.402, 21100.680, 21102.062, 21098.654, 21096.598 (21354.718)	Log prob: 21072.439, 21130.711, 21135.463, 21134.256, 21133.617 (21392.539)	KLD: 23.038, 6.989, 3.370, 2.206, 1.422 (37.820)	Grad: 11.118, 2.422, 1.760, 1.399, 1.088
[Epoch  32 (37.24s)]	ELBO: 21055.234, 21146.326, 21147.291, 21143.303, 21141.291 (21320.600)	Log prob: 21077.686, 21176.748, 21180.910, 21179.125, 21178.553 (21360.314)	KLD: 22.450, 7.972, 3.195, 2.209, 1.430 (39.713)	Grad: 10.579, 2.624, 1.582, 1.251, 1.032
[Epoch  33 (37.24s)]	ELBO: 21001.109, 21104.447, 21100.654, 21096.252, 21094.879 (20880.654)	Log prob: 21023.279, 21134.678, 21134.021, 21132.045, 21132.406 (20915.520)	KLD: 22.169, 8.061, 3.135, 2.431, 1.729 (34.866)	Grad: 24.093, 5.584, 3.403, 2.792, 1.952
[Epoch  34 (39.58s)]	ELBO: 20931.270, 21055.580, 21053.244, 21047.371, 21046.621 (21385.275)	Log prob: 20951.209, 21083.553, 21084.004, 21081.102, 21082.303 (21419.619)	KLD: 19.942, 8.030, 2.785, 2.972, 1.953 (34.345)	Grad: 24.769, 6.021, 3.102, 3.143, 2.033
[Epoch  35 (37.93s)]	ELBO: 21001.980, 21151.508, 21156.404, 21153.637, 21153.994 (21432.895)	Log prob: 21020.615, 21179.295, 21187.732, 21187.885, 21189.973 (21467.438)	KLD: 18.635, 9.153, 3.539, 2.926, 1.723 (34.544)	Grad: 22.900, 7.009, 4.429, 2.442, 1.932
[Epoch  36 (37.94s)]	ELBO: 21017.346, 21185.855, 21191.219, 21188.781, 21187.941 (21487.073)	Log prob: 21036.092, 21214.686, 21223.934, 21224.088, 21224.738 (21522.034)	KLD: 18.748, 10.085, 3.883, 2.596, 1.482 (34.962)	Grad: 27.418, 7.128, 4.246, 3.524, 2.569
[Epoch  37 (37.46s)]	ELBO: 21140.621, 21296.059, 21305.553, 21305.674, 21305.229 (21503.100)	Log prob: 21160.902, 21326.107, 21338.816, 21341.715, 21342.754 (21541.551)	KLD: 20.281, 9.765, 3.222, 2.771, 1.491 (38.451)	Grad: 13.580, 4.329, 2.060, 1.694, 1.388
[Epoch  38 (36.99s)]	ELBO: 21163.994, 21274.465, 21281.006, 21277.676, 21275.803 (21620.554)	Log prob: 21185.506, 21304.697, 21314.674, 21313.652, 21313.201 (21658.355)	KLD: 21.508, 8.721, 3.439, 2.306, 1.419 (37.802)	Grad: 27.746, 9.122, 4.295, 3.886, 2.809
[Epoch  39 (36.85s)]	ELBO: 21314.467, 21395.254, 21399.955, 21396.068, 21395.604 (21640.390)	Log prob: 21338.010, 21426.312, 21434.516, 21432.916, 21433.855 (21675.954)	KLD: 23.541, 7.514, 3.504, 2.290, 1.408 (35.563)	Grad: 14.787, 5.541, 2.316, 2.066, 1.729
[Epoch  40 (37.62s)]	ELBO: 21400.758, 21450.717, 21452.375, 21449.258, 21449.045 (21685.509)	Log prob: 21425.984, 21482.355, 21486.793, 21485.928, 21487.121 (21726.289)	KLD: 25.228, 6.408, 2.787, 2.243, 1.412 (40.780)	Grad: 14.387, 6.240, 2.025, 2.184, 1.507
[Epoch  41 (38.19s)]	ELBO: 21417.869, 21451.354, 21452.529, 21449.285, 21448.191 (21642.702)	Log prob: 21444.141, 21483.516, 21487.240, 21486.115, 21486.438 (21683.562)	KLD: 26.271, 5.890, 2.547, 2.122, 1.418 (40.861)	Grad: 19.372, 6.035, 2.230, 2.495, 1.847
[Epoch  42 (38.96s)]	ELBO: 21483.590, 21509.537, 21510.742, 21508.012, 21507.627 (21735.719)	Log prob: 21509.875, 21541.656, 21545.363, 21544.812, 21545.779 (21775.059)	KLD: 26.287, 5.833, 2.496, 2.188, 1.349 (39.339)	Grad: 11.594, 4.394, 1.582, 1.655, 1.287
[Epoch  43 (36.77s)]	ELBO: 21499.363, 21519.504, 21521.574, 21517.959, 21517.848 (21730.317)	Log prob: 21525.979, 21551.746, 21556.117, 21554.473, 21555.727 (21771.374)	KLD: 26.620, 5.620, 2.304, 1.968, 1.366 (41.057)	Grad: 17.463, 5.536, 2.205, 2.251, 1.671
[Epoch  44 (37.64s)]	ELBO: 21509.523, 21526.049, 21527.926, 21524.996, 21526.139 (21713.354)	Log prob: 21536.248, 21558.465, 21562.799, 21562.043, 21564.609 (21755.533)	KLD: 26.727, 5.693, 2.453, 2.180, 1.418 (42.180)	Grad: 17.954, 5.573, 2.135, 2.191, 1.659
[Epoch  45 (37.11s)]	ELBO: 21515.203, 21539.271, 21540.467, 21538.277, 21538.820 (21715.733)	Log prob: 21541.492, 21571.305, 21574.979, 21574.871, 21576.734 (21757.582)	KLD: 26.286, 5.748, 2.477, 2.081, 1.321 (41.849)	Grad: 20.325, 5.118, 2.258, 2.191, 1.759
[Epoch  46 (36.84s)]	ELBO: 21553.863, 21579.688, 21583.430, 21580.943, 21581.143 (21797.031)	Log prob: 21580.066, 21611.828, 21618.084, 21617.547, 21619.344 (21837.370)	KLD: 26.199, 5.941, 2.514, 1.951, 1.599 (40.339)	Grad: 16.369, 5.718, 2.076, 2.303, 1.750
[Epoch  47 (38.28s)]	ELBO: 21559.055, 21613.541, 21617.605, 21617.055, 21618.859 (21842.649)	Log prob: 21585.264, 21646.516, 21653.510, 21655.121, 21658.771 (21883.255)	KLD: 26.212, 6.762, 2.930, 2.160, 1.846 (40.605)	Grad: 17.683, 6.214, 2.184, 2.377, 2.043
[Epoch  48 (38.13s)]	ELBO: 21590.646, 21683.656, 21690.322, 21690.152, 21689.584 (21872.822)	Log prob: 21616.766, 21717.508, 21727.406, 21729.311, 21730.918 (21915.097)	KLD: 26.117, 7.737, 3.227, 2.079, 2.176 (42.275)	Grad: 15.155, 6.687, 2.220, 2.615, 2.147
[Epoch  49 (38.07s)]	ELBO: 21592.002, 21699.775, 21706.100, 21703.535, 21702.816 (21921.276)	Log prob: 21618.309, 21734.164, 21743.854, 21743.191, 21744.479 (21962.345)	KLD: 26.313, 8.078, 3.366, 1.901, 2.008 (41.068)	Grad: 20.663, 8.096, 2.778, 3.195, 2.633
[Epoch  50 (37.85s)]	ELBO: 21633.822, 21749.756, 21756.174, 21752.939, 21751.684 (21934.191)	Log prob: 21660.441, 21784.490, 21794.115, 21792.791, 21793.260 (21977.916)	KLD: 26.617, 8.121, 3.207, 1.905, 1.725 (43.725)	Grad: 15.406, 5.876, 2.205, 2.489, 1.862
[Epoch  51 (36.50s)]	ELBO: 21662.143, 21779.580, 21785.379, 21782.607, 21781.072 (21956.966)	Log prob: 21688.938, 21814.590, 21823.496, 21822.398, 21822.527 (21999.840)	KLD: 26.793, 8.217, 3.105, 1.674, 1.669 (42.874)	Grad: 15.686, 7.209, 2.561, 2.752, 1.984
[Epoch  52 (37.47s)]	ELBO: 21659.691, 21762.996, 21764.922, 21760.666, 21759.250 (21973.243)	Log prob: 21686.408, 21797.869, 21802.959, 21800.379, 21800.605 (22017.744)	KLD: 26.716, 8.159, 3.161, 1.678, 1.641 (44.501)	Grad: 19.948, 9.464, 3.423, 3.633, 2.658
[Epoch  53 (36.81s)]	ELBO: 21705.344, 21810.484, 21813.129, 21810.465, 21807.422 (21947.288)	Log prob: 21733.127, 21846.334, 21851.947, 21850.814, 21849.334 (21991.160)	KLD: 27.785, 8.066, 2.961, 1.538, 1.566 (43.871)	Grad: 15.750, 5.137, 2.033, 2.121, 1.666
[Epoch  54 (37.32s)]	ELBO: 21751.219, 21844.352, 21845.410, 21843.430, 21840.297 (22034.298)	Log prob: 21780.154, 21881.379, 21885.311, 21884.834, 21882.936 (22080.190)	KLD: 28.935, 8.089, 2.874, 1.504, 1.236 (45.892)	Grad: 15.970, 5.942, 2.306, 2.237, 1.664
[Epoch  55 (36.48s)]	ELBO: 21787.750, 21861.947, 21861.660, 21858.875, 21856.893 (22040.029)	Log prob: 21818.000, 21900.072, 21902.621, 21901.359, 21900.566 (22085.125)	KLD: 30.252, 7.868, 2.843, 1.521, 1.193 (45.096)	Grad: 23.658, 7.204, 3.099, 2.857, 2.115
[Epoch  56 (36.67s)]	ELBO: 21839.584, 21915.861, 21917.578, 21915.312, 21914.221 (22114.627)	Log prob: 21870.248, 21954.104, 21958.590, 21957.686, 21957.514 (22159.793)	KLD: 30.668, 7.573, 2.779, 1.354, 0.913 (45.166)	Grad: 13.688, 5.077, 2.328, 2.188, 1.403
[Epoch  57 (38.05s)]	ELBO: 21861.334, 21952.311, 21957.359, 21955.496, 21953.814 (22146.439)	Log prob: 21892.381, 21990.922, 21999.227, 21998.717, 21997.887 (22188.127)	KLD: 31.045, 7.567, 3.256, 1.355, 0.851 (41.690)	Grad: 14.519, 6.033, 2.352, 2.000, 1.453
[Epoch  58 (37.42s)]	ELBO: 21886.611, 22001.836, 22011.510, 22009.291, 22007.797 (22152.386)	Log prob: 21917.318, 22040.436, 22054.186, 22053.506, 22052.797 (22197.031)	KLD: 30.705, 7.892, 4.082, 1.535, 0.789 (44.646)	Grad: 15.683, 6.779, 3.309, 2.108, 1.660
[Epoch  59 (36.98s)]	ELBO: 21890.203, 22028.316, 22037.066, 22034.281, 22034.076 (22184.339)	Log prob: 21920.100, 22066.291, 22078.971, 22077.654, 22078.354 (22231.341)	KLD: 29.898, 8.072, 3.934, 1.476, 0.897 (47.001)	Grad: 15.798, 5.979, 2.898, 1.751, 1.463
[Epoch  60 (37.17s)]	ELBO: 21871.076, 22017.314, 22022.375, 22020.512, 22019.371 (22018.893)	Log prob: 21900.402, 22054.873, 22063.775, 22063.402, 22063.072 (22071.960)	KLD: 29.330, 8.233, 3.835, 1.493, 0.813 (53.066)	Grad: 27.501, 9.691, 4.181, 3.035, 2.355
[Epoch  61 (36.68s)]	ELBO: 21871.875, 22026.402, 22031.932, 22028.676, 22028.213 (22236.846)	Log prob: 21900.584, 22063.008, 22072.135, 22070.309, 22070.668 (22277.823)	KLD: 28.703, 7.899, 3.601, 1.430, 0.824 (40.978)	Grad: 20.212, 7.502, 3.389, 2.533, 1.764
[Epoch  62 (38.01s)]	ELBO: 21902.586, 22073.135, 22078.504, 22075.703, 22075.199 (22231.808)	Log prob: 21930.924, 22109.592, 22118.582, 22117.291, 22117.598 (22276.857)	KLD: 28.339, 8.122, 3.617, 1.506, 0.810 (45.050)	Grad: 20.324, 5.639, 2.312, 1.578, 1.461
[Epoch  63 (36.77s)]	ELBO: 21928.018, 22086.949, 22089.684, 22087.037, 22086.723 (22224.752)	Log prob: 21956.332, 22123.422, 22129.709, 22128.447, 22128.871 (22263.603)	KLD: 28.314, 8.158, 3.552, 1.387, 0.740 (38.851)	Grad: 16.476, 8.778, 3.515, 2.155, 1.632
[Epoch  64 (38.07s)]	ELBO: 21928.916, 22102.559, 22107.473, 22104.992, 22105.117 (22270.166)	Log prob: 21956.516, 22138.234, 22146.578, 22145.389, 22146.248 (22308.898)	KLD: 27.603, 8.073, 3.432, 1.294, 0.733 (38.734)	Grad: 17.234, 6.792, 2.803, 1.687, 1.408
[Epoch  65 (39.20s)]	ELBO: 21949.889, 22136.328, 22141.348, 22139.166, 22139.586 (22279.563)	Log prob: 21977.441, 22172.037, 22180.506, 22179.695, 22180.818 (22322.317)	KLD: 27.548, 8.163, 3.450, 1.373, 0.689 (42.754)	Grad: 14.665, 5.185, 2.184, 1.442, 1.124
[Epoch  66 (37.85s)]	ELBO: 21952.305, 22139.320, 22144.178, 22141.113, 22141.309 (22252.994)	Log prob: 21979.609, 22174.887, 22183.406, 22181.674, 22182.572 (22297.746)	KLD: 27.302, 8.263, 3.666, 1.332, 0.703 (44.753)	Grad: 19.595, 6.626, 2.630, 1.670, 1.399
[Epoch  67 (38.40s)]	ELBO: 21972.441, 22164.775, 22171.260, 22169.012, 22168.906 (22333.374)	Log prob: 21999.652, 22200.285, 22210.279, 22209.297, 22209.895 (22373.850)	KLD: 27.212, 8.295, 3.507, 1.271, 0.706 (40.476)	Grad: 13.680, 6.137, 2.263, 1.360, 1.065
[Epoch  68 (38.20s)]	ELBO: 21996.096, 22194.467, 22202.598, 22201.020, 22200.410 (22344.992)	Log prob: 22023.078, 22229.803, 22241.645, 22241.312, 22241.447 (22385.887)	KLD: 26.985, 8.346, 3.715, 1.250, 0.742 (40.897)	Grad: 9.990, 4.821, 2.109, 1.150, 0.948
[Epoch  69 (37.62s)]	ELBO: 21979.326, 22182.090, 22192.035, 22189.717, 22190.486 (22060.264)	Log prob: 22005.852, 22217.232, 22231.008, 22229.973, 22231.416 (22111.307)	KLD: 26.529, 8.612, 3.830, 1.280, 0.681 (51.044)	Grad: 16.536, 6.808, 2.402, 1.425, 1.198
[Epoch  70 (36.97s)]	ELBO: 21951.090, 22142.432, 22153.893, 22151.553, 22152.430 (22362.954)	Log prob: 21977.516, 22177.441, 22192.928, 22191.744, 22193.434 (22401.691)	KLD: 26.417, 8.594, 4.020, 1.158, 0.813 (38.737)	Grad: 22.999, 11.287, 4.063, 2.470, 1.715
[Epoch  71 (36.72s)]	ELBO: 21992.445, 22209.941, 22229.801, 22228.996, 22229.104 (22392.680)	Log prob: 22018.539, 22245.131, 22269.459, 22269.996, 22270.875 (22434.679)	KLD: 26.096, 9.097, 4.468, 1.336, 0.772 (41.997)	Grad: 16.703, 6.485, 2.539, 1.538, 1.301
[Epoch  72 (38.00s)]	ELBO: 22013.482, 22245.943, 22268.621, 22268.562, 22269.807 (22414.214)	Log prob: 22039.510, 22281.242, 22308.521, 22309.838, 22312.055 (22457.269)	KLD: 26.023, 9.277, 4.605, 1.374, 0.968 (43.054)	Grad: 16.974, 6.873, 2.570, 1.475, 1.210
[Epoch  73 (38.82s)]	ELBO: 22030.410, 22285.059, 22314.928, 22316.592, 22319.107 (22476.572)	Log prob: 22056.297, 22320.600, 22355.521, 22358.744, 22362.225 (22521.324)	KLD: 25.887, 9.651, 5.057, 1.558, 0.966 (44.752)	Grad: 14.632, 6.733, 2.468, 1.368, 1.178
[Epoch  74 (36.50s)]	ELBO: 22016.004, 22301.963, 22334.176, 22336.365, 22338.906 (22502.350)	Log prob: 22042.074, 22338.059, 22375.496, 22379.354, 22382.959 (22548.915)	KLD: 26.071, 10.025, 5.225, 1.670, 1.063 (46.564)	Grad: 16.012, 8.307, 3.070, 1.772, 1.388
[Epoch  75 (36.84s)]	ELBO: 22027.160, 22349.850, 22370.953, 22375.555, 22376.541 (22511.029)	Log prob: 22053.461, 22386.928, 22413.170, 22419.465, 22421.428 (22559.603)	KLD: 26.300, 10.776, 5.142, 1.691, 0.984 (48.575)	Grad: 19.205, 9.023, 3.536, 2.365, 1.793
[Epoch  76 (37.59s)]	ELBO: 22022.756, 22385.436, 22405.473, 22409.547, 22411.129 (22581.723)	Log prob: 22049.422, 22423.383, 22448.254, 22453.885, 22456.527 (22624.430)	KLD: 26.670, 11.277, 4.830, 1.559, 1.064 (42.707)	Grad: 17.716, 6.996, 2.443, 1.748, 1.283
[Epoch  77 (37.81s)]	ELBO: 22011.012, 22396.090, 22410.355, 22410.434, 22411.066 (22575.710)	Log prob: 22037.598, 22433.965, 22452.918, 22454.602, 22456.309 (22623.680)	KLD: 26.584, 11.292, 4.689, 1.603, 1.072 (47.971)	Grad: 22.281, 12.263, 3.711, 2.658, 1.943
[Epoch  78 (37.90s)]	ELBO: 22044.807, 22453.045, 22466.246, 22465.348, 22462.947 (22574.255)	Log prob: 22071.697, 22491.354, 22509.178, 22509.732, 22508.621 (22620.146)	KLD: 26.889, 11.423, 4.623, 1.448, 1.293 (45.892)	Grad: 17.156, 9.711, 3.294, 2.077, 1.605
[Epoch  79 (38.27s)]	ELBO: 22057.562, 22483.432, 22494.744, 22493.156, 22490.746 (22661.296)	Log prob: 22084.672, 22522.031, 22537.676, 22537.469, 22536.201 (22706.860)	KLD: 27.109, 11.489, 4.331, 1.385, 1.138 (45.563)	Grad: 14.411, 8.286, 3.218, 1.980, 1.542
[Epoch  80 (38.83s)]	ELBO: 22029.047, 22475.572, 22485.943, 22483.812, 22482.426 (22664.722)	Log prob: 22055.688, 22513.896, 22528.410, 22527.545, 22527.266 (22707.413)	KLD: 26.641, 11.682, 4.144, 1.268, 1.101 (42.691)	Grad: 19.313, 9.304, 3.456, 2.120, 1.627
[Epoch  81 (38.75s)]	ELBO: 22049.541, 22505.951, 22514.127, 22511.391, 22510.496 (22667.695)	Log prob: 22076.266, 22544.320, 22556.609, 22555.055, 22555.049 (22714.074)	KLD: 26.726, 11.642, 4.115, 1.178, 0.896 (46.380)	Grad: 20.792, 8.846, 2.974, 1.876, 1.562
[Epoch  82 (37.97s)]	ELBO: 22064.783, 22532.357, 22539.729, 22537.205, 22536.336 (22646.549)	Log prob: 22091.627, 22571.002, 22582.283, 22580.875, 22580.896 (22693.563)	KLD: 26.839, 11.802, 3.911, 1.116, 0.892 (47.014)	Grad: 14.090, 8.807, 2.697, 1.517, 1.365
[Epoch  83 (38.41s)]	ELBO: 22042.703, 22518.014, 22523.338, 22521.480, 22520.496 (22668.852)	Log prob: 22069.312, 22556.486, 22565.686, 22564.844, 22564.760 (22714.821)	KLD: 26.607, 11.867, 3.877, 1.013, 0.905 (45.970)	Grad: 23.359, 11.094, 3.164, 1.872, 1.580
[Epoch  84 (36.46s)]	ELBO: 22089.258, 22579.146, 22584.225, 22580.877, 22580.334 (22687.938)	Log prob: 22116.002, 22617.902, 22626.604, 22624.316, 22624.619 (22731.534)	KLD: 26.744, 12.011, 3.621, 1.063, 0.845 (43.596)	Grad: 13.722, 7.688, 2.488, 1.476, 1.182
[Epoch  85 (37.62s)]	ELBO: 22072.660, 22569.316, 22572.734, 22571.303, 22569.648 (22682.097)	Log prob: 22099.398, 22608.090, 22614.947, 22614.482, 22613.521 (22726.513)	KLD: 26.736, 12.041, 3.444, 0.959, 0.693 (44.416)	Grad: 24.426, 8.202, 2.663, 1.794, 1.730
[Epoch  86 (37.28s)]	ELBO: 22074.041, 22570.785, 22574.486, 22571.885, 22571.027 (22661.522)	Log prob: 22100.473, 22609.344, 22616.672, 22615.053, 22614.922 (22710.150)	KLD: 26.432, 12.127, 3.632, 0.976, 0.729 (48.627)	Grad: 19.418, 11.098, 2.645, 1.697, 1.490
[Epoch  87 (37.58s)]	ELBO: 22085.238, 22592.064, 22595.094, 22592.992, 22592.064 (22683.461)	Log prob: 22112.057, 22631.146, 22637.723, 22636.609, 22636.445 (22726.627)	KLD: 26.820, 12.261, 3.548, 0.981, 0.771 (43.167)	Grad: 18.832, 9.904, 2.671, 1.626, 1.427
[Epoch  88 (38.43s)]	ELBO: 22078.840, 22585.441, 22588.270, 22587.059, 22586.191 (22743.711)	Log prob: 22105.426, 22624.270, 22630.439, 22630.135, 22629.967 (22785.887)	KLD: 26.581, 12.248, 3.337, 0.910, 0.702 (42.175)	Grad: 21.211, 11.313, 2.868, 1.710, 1.570
[Epoch  89 (37.90s)]	ELBO: 22109.432, 22626.455, 22628.188, 22626.625, 22626.785 (22721.347)	Log prob: 22136.076, 22665.559, 22670.715, 22669.988, 22670.834 (22765.645)	KLD: 26.645, 12.458, 3.424, 0.833, 0.693 (44.298)	Grad: 16.217, 9.819, 2.672, 1.511, 1.310
[Epoch  90 (36.91s)]	ELBO: 22107.963, 22636.959, 22639.275, 22637.641, 22637.891 (22725.827)	Log prob: 22134.885, 22676.377, 22681.965, 22681.145, 22681.990 (22773.623)	KLD: 26.921, 12.498, 3.272, 0.817, 0.598 (47.795)	Grad: 17.125, 8.625, 2.409, 1.474, 1.251
[Epoch  91 (36.64s)]	ELBO: 22123.188, 22659.455, 22661.621, 22660.066, 22659.615 (22763.764)	Log prob: 22150.172, 22699.133, 22704.643, 22703.938, 22704.174 (22808.656)	KLD: 26.982, 12.692, 3.347, 0.847, 0.689 (44.891)	Grad: 17.826, 8.735, 2.558, 1.413, 1.295
[Epoch  92 (36.91s)]	ELBO: 22123.324, 22660.225, 22661.668, 22660.447, 22660.262 (22630.486)	Log prob: 22150.131, 22699.668, 22704.451, 22704.129, 22704.678 (22682.840)	KLD: 26.807, 12.635, 3.340, 0.896, 0.738 (52.352)	Grad: 18.720, 9.939, 2.897, 1.690, 1.424
[Epoch  93 (37.58s)]	ELBO: 22095.748, 22638.906, 22641.088, 22639.533, 22639.879 (22791.064)	Log prob: 22122.527, 22678.566, 22684.074, 22683.398, 22684.445 (22834.493)	KLD: 26.777, 12.887, 3.325, 0.880, 0.700 (43.428)	Grad: 25.968, 11.267, 2.770, 1.837, 1.480
[Epoch  94 (37.09s)]	ELBO: 22129.949, 22678.961, 22681.350, 22680.762, 22679.529 (22771.695)	Log prob: 22156.750, 22718.703, 22724.451, 22724.738, 22724.148 (22818.875)	KLD: 26.804, 12.942, 3.354, 0.871, 0.649 (47.180)	Grad: 17.120, 8.564, 2.454, 1.474, 1.217
[Epoch  95 (37.40s)]	ELBO: 22125.766, 22685.146, 22687.078, 22686.369, 22686.615 (22748.312)	Log prob: 22152.258, 22724.547, 22729.965, 22730.227, 22731.135 (22793.734)	KLD: 26.493, 12.907, 3.486, 0.972, 0.660 (45.423)	Grad: 16.403, 8.984, 2.567, 1.477, 1.311
[Epoch  96 (37.59s)]	ELBO: 22146.750, 22704.584, 22707.320, 22707.092, 22706.488 (22827.986)	Log prob: 22173.500, 22744.479, 22750.627, 22751.484, 22751.588 (22873.227)	KLD: 26.747, 13.144, 3.418, 1.081, 0.707 (45.241)	Grad: 18.870, 9.146, 2.515, 1.463, 1.240
[Epoch  97 (37.90s)]	ELBO: 22133.414, 22701.973, 22704.266, 22704.027, 22704.236 (22739.659)	Log prob: 22160.084, 22741.873, 22747.643, 22748.473, 22749.346 (22787.088)	KLD: 26.669, 13.232, 3.478, 1.063, 0.665 (47.428)	Grad: 26.581, 9.881, 3.050, 1.907, 1.688
[Epoch  98 (40.62s)]	ELBO: 22135.924, 22696.402, 22700.611, 22699.521, 22699.445 (22846.914)	Log prob: 22162.420, 22736.123, 22743.816, 22743.779, 22744.371 (22890.652)	KLD: 26.492, 13.233, 3.483, 1.049, 0.665 (43.737)	Grad: 22.256, 11.934, 3.236, 1.869, 1.540
[Epoch  99 (38.11s)]	ELBO: 22156.635, 22733.703, 22738.730, 22739.305, 22740.172 (22746.254)	Log prob: 22183.254, 22773.682, 22782.205, 22783.926, 22785.385 (22799.189)	KLD: 26.618, 13.360, 3.495, 1.150, 0.592 (52.934)	Grad: 18.891, 9.115, 2.487, 1.701, 1.366
[Epoch 100 (34.51s)]	ELBO: 22168.805, 22746.441, 22752.488, 22755.951, 22755.549 (22801.778)	Log prob: 22195.658, 22786.752, 22796.389, 22801.256, 22801.551 (22847.488)	KLD: 26.852, 13.455, 3.591, 1.407, 0.698 (45.709)	Grad: 22.648, 11.005, 3.028, 1.889, 1.397
Best epoch(s): [98]	Training time(s): 3790.19s (3790.19s)	Best ELBO: 22755.549 (22846.914)	Best log prob: 22801.551 (22890.652)
Avg. mu: -1.094, 0.118, 2.332, 1.688, 0.168, -0.539, 1.127, -0.115, 0.040, 0.667, -0.284, -0.405, -0.101, -0.084, 0.069, -0.239, 0.253, -0.012, -0.247, -0.331
Avg. var: 0.006, 0.004, 0.003, 0.008, 0.029, 0.017, 0.023, 0.010, 0.244, 0.226, 0.165, 0.075, 0.568, 0.414, 0.578, 0.244, 0.577, 0.521, 0.577, 0.753
Max. mu: 11.727, 9.175, 10.104, 11.159, 4.207, 8.630, 5.920, 5.846, 2.683, 1.861, 1.060, 1.710, 0.743, 0.961, 1.099, 2.464, 1.275, 1.287, 1.054, 0.698
Max. var: 0.287, 0.195, 0.047, 0.257, 0.173, 0.072, 0.154, 0.108, 0.780, 1.045, 0.620, 0.300, 1.457, 0.948, 1.962, 0.814, 1.396, 1.132, 1.836, 1.286
Min. mu: -10.112, -7.889, -1.842, -6.432, -2.480, -8.640, -4.362, -6.932, -2.172, -2.363, -2.593, -3.709, -1.534, -1.213, -1.844, -1.635, -1.051, -1.181, -1.438, -1.403
Min. var: 0.000, 0.000, 0.000, 0.000, 0.006, 0.002, 0.005, 0.003, 0.078, 0.049, 0.031, 0.018, 0.172, 0.133, 0.207, 0.071, 0.173, 0.168, 0.149, 0.218
Cov. mu:
[[6.077 -0.887 -1.751 -1.070 0.209 -1.596 -0.968 -0.217 -0.907 -0.326
  -0.506 0.180 -0.177 0.083 -0.165 0.040 0.112 0.062 -0.097 0.016]
 [-0.887 4.470 0.845 1.534 0.382 1.781 0.717 0.015 -0.182 0.180 -0.120
  0.296 -0.219 0.181 -0.022 0.114 0.066 0.030 -0.068 0.093]
 [-1.751 0.845 2.866 -0.346 -0.033 0.939 0.409 0.344 0.403 0.085 0.299
  -0.386 -0.220 -0.219 -0.074 0.191 0.000 0.041 0.069 0.022]
 [-1.070 1.534 -0.346 5.980 -0.091 0.858 -0.301 -0.221 0.522 0.235 0.339
  0.051 -0.067 0.003 0.048 0.465 0.323 -0.285 -0.077 -0.096]
 [0.209 0.382 -0.033 -0.091 0.837 -1.575 0.578 -0.410 0.087 -0.181 -0.008
  -0.298 -0.019 -0.039 0.077 -0.084 0.128 -0.119 -0.023 0.010]
 [-1.596 1.781 0.939 0.858 -1.575 5.129 -0.923 0.557 -0.218 0.413 0.045
  0.720 -0.127 0.204 -0.114 0.204 -0.317 0.278 -0.009 0.084]
 [-0.968 0.717 0.409 -0.301 0.578 -0.923 1.092 -0.609 0.347 -0.130 0.178
  -0.380 0.024 -0.048 0.068 -0.009 0.035 -0.024 0.034 -0.034]
 [-0.217 0.015 0.344 -0.221 -0.410 0.557 -0.609 2.850 -0.512 0.360 -0.561
  0.680 -0.082 -0.100 -0.308 0.019 0.018 0.019 0.105 0.084]
 [-0.907 -0.182 0.403 0.522 0.087 -0.218 0.347 -0.512 0.516 -0.139 0.261
  -0.389 0.008 -0.086 0.009 0.184 0.020 -0.024 0.039 -0.091]
 [-0.326 0.180 0.085 0.235 -0.181 0.413 -0.130 0.360 -0.139 0.262 -0.012
  0.230 0.033 0.022 0.062 -0.112 0.026 -0.025 -0.021 0.050]
 [-0.506 -0.120 0.299 0.339 -0.008 0.045 0.178 -0.561 0.261 -0.012 0.264
  -0.205 0.030 -0.030 0.090 0.021 0.002 -0.026 -0.014 -0.023]
 [0.180 0.296 -0.386 0.051 -0.298 0.720 -0.380 0.680 -0.389 0.230 -0.205
  0.491 0.033 0.102 -0.007 -0.138 -0.041 0.023 -0.023 0.075]
 [-0.177 -0.219 -0.220 -0.067 -0.019 -0.127 0.024 -0.082 0.008 0.033
  0.030 0.033 0.077 0.021 0.052 -0.080 -0.015 -0.017 -0.010 -0.000]
 [0.083 0.181 -0.219 0.003 -0.039 0.204 -0.048 -0.100 -0.086 0.022 -0.030
  0.102 0.021 0.078 0.019 -0.041 -0.025 0.017 -0.000 0.023]
 [-0.165 -0.022 -0.074 0.048 0.077 -0.114 0.068 -0.308 0.009 0.062 0.090
  -0.007 0.052 0.019 0.131 -0.128 0.015 -0.043 -0.040 0.027]
 [0.040 0.114 0.191 0.465 -0.084 0.204 -0.009 0.019 0.184 -0.112 0.021
  -0.138 -0.080 -0.041 -0.128 0.260 0.020 0.040 0.051 -0.069]
 [0.112 0.066 0.000 0.323 0.128 -0.317 0.035 0.018 0.020 0.026 0.002
  -0.041 -0.015 -0.025 0.015 0.020 0.088 -0.045 -0.003 -0.008]
 [0.062 0.030 0.041 -0.285 -0.119 0.278 -0.024 0.019 -0.024 -0.025 -0.026
  0.023 -0.017 0.017 -0.043 0.040 -0.045 0.066 0.019 -0.008]
 [-0.097 -0.068 0.069 -0.077 -0.023 -0.009 0.034 0.105 0.039 -0.021
  -0.014 -0.023 -0.010 -0.000 -0.040 0.051 -0.003 0.019 0.051 -0.005]
 [0.016 0.093 0.022 -0.096 0.010 0.084 -0.034 0.084 -0.091 0.050 -0.023
  0.075 -0.000 0.023 0.027 -0.069 -0.008 -0.008 -0.005 0.050]]
Avg. mu: -1.094, 0.118, 2.332, 1.688, 0.168, -0.539, 1.127, -0.115, 0.040, 0.667, -0.284, -0.405, -0.101, -0.084, 0.069, -0.239, 0.253, -0.012, -0.247, -0.331
Avg. var: 0.006, 0.004, 0.003, 0.008, 0.029, 0.017, 0.023, 0.010, 0.244, 0.226, 0.165, 0.075, 0.568, 0.414, 0.578, 0.244, 0.577, 0.521, 0.577, 0.753
Max. mu: 11.727, 9.175, 10.104, 11.159, 4.207, 8.630, 5.920, 5.846, 2.683, 1.861, 1.060, 1.710, 0.743, 0.961, 1.099, 2.464, 1.275, 1.287, 1.054, 0.698
Max. var: 0.287, 0.195, 0.047, 0.257, 0.173, 0.072, 0.154, 0.108, 0.780, 1.045, 0.620, 0.300, 1.457, 0.948, 1.962, 0.814, 1.396, 1.132, 1.836, 1.286
Min. mu: -10.112, -7.889, -1.842, -6.432, -2.480, -8.640, -4.362, -6.932, -2.172, -2.363, -2.593, -3.709, -1.534, -1.213, -1.844, -1.635, -1.051, -1.181, -1.438, -1.403
Min. var: 0.000, 0.000, 0.000, 0.000, 0.006, 0.002, 0.005, 0.003, 0.078, 0.049, 0.031, 0.018, 0.172, 0.133, 0.207, 0.071, 0.173, 0.168, 0.149, 0.218
Cov. mu:
[[6.077 -0.887 -1.751 -1.070 0.209 -1.596 -0.968 -0.217 -0.907 -0.326
  -0.506 0.180 -0.177 0.083 -0.165 0.040 0.112 0.062 -0.097 0.016]
 [-0.887 4.470 0.845 1.534 0.382 1.781 0.717 0.015 -0.182 0.180 -0.120
  0.296 -0.219 0.181 -0.022 0.114 0.066 0.030 -0.068 0.093]
 [-1.751 0.845 2.866 -0.346 -0.033 0.939 0.409 0.344 0.403 0.085 0.299
  -0.386 -0.220 -0.219 -0.074 0.191 0.000 0.041 0.069 0.022]
 [-1.070 1.534 -0.346 5.980 -0.091 0.858 -0.301 -0.221 0.522 0.235 0.339
  0.051 -0.067 0.003 0.048 0.465 0.323 -0.285 -0.077 -0.096]
 [0.209 0.382 -0.033 -0.091 0.837 -1.575 0.578 -0.410 0.087 -0.181 -0.008
  -0.298 -0.019 -0.039 0.077 -0.084 0.128 -0.119 -0.023 0.010]
 [-1.596 1.781 0.939 0.858 -1.575 5.129 -0.923 0.557 -0.218 0.413 0.045
  0.720 -0.127 0.204 -0.114 0.204 -0.317 0.278 -0.009 0.084]
 [-0.968 0.717 0.409 -0.301 0.578 -0.923 1.092 -0.609 0.347 -0.130 0.178
  -0.380 0.024 -0.048 0.068 -0.009 0.035 -0.024 0.034 -0.034]
 [-0.217 0.015 0.344 -0.221 -0.410 0.557 -0.609 2.850 -0.512 0.360 -0.561
  0.680 -0.082 -0.100 -0.308 0.019 0.018 0.019 0.105 0.084]
 [-0.907 -0.182 0.403 0.522 0.087 -0.218 0.347 -0.512 0.516 -0.139 0.261
  -0.389 0.008 -0.086 0.009 0.184 0.020 -0.024 0.039 -0.091]
 [-0.326 0.180 0.085 0.235 -0.181 0.413 -0.130 0.360 -0.139 0.262 -0.012
  0.230 0.033 0.022 0.062 -0.112 0.026 -0.025 -0.021 0.050]
 [-0.506 -0.120 0.299 0.339 -0.008 0.045 0.178 -0.561 0.261 -0.012 0.264
  -0.205 0.030 -0.030 0.090 0.021 0.002 -0.026 -0.014 -0.023]
 [0.180 0.296 -0.386 0.051 -0.298 0.720 -0.380 0.680 -0.389 0.230 -0.205
  0.491 0.033 0.102 -0.007 -0.138 -0.041 0.023 -0.023 0.075]
 [-0.177 -0.219 -0.220 -0.067 -0.019 -0.127 0.024 -0.082 0.008 0.033
  0.030 0.033 0.077 0.021 0.052 -0.080 -0.015 -0.017 -0.010 -0.000]
 [0.083 0.181 -0.219 0.003 -0.039 0.204 -0.048 -0.100 -0.086 0.022 -0.030
  0.102 0.021 0.078 0.019 -0.041 -0.025 0.017 -0.000 0.023]
 [-0.165 -0.022 -0.074 0.048 0.077 -0.114 0.068 -0.308 0.009 0.062 0.090
  -0.007 0.052 0.019 0.131 -0.128 0.015 -0.043 -0.040 0.027]
 [0.040 0.114 0.191 0.465 -0.084 0.204 -0.009 0.019 0.184 -0.112 0.021
  -0.138 -0.080 -0.041 -0.128 0.260 0.020 0.040 0.051 -0.069]
 [0.112 0.066 0.000 0.323 0.128 -0.317 0.035 0.018 0.020 0.026 0.002
  -0.041 -0.015 -0.025 0.015 0.020 0.088 -0.045 -0.003 -0.008]
 [0.062 0.030 0.041 -0.285 -0.119 0.278 -0.024 0.019 -0.024 -0.025 -0.026
  0.023 -0.017 0.017 -0.043 0.040 -0.045 0.066 0.019 -0.008]
 [-0.097 -0.068 0.069 -0.077 -0.023 -0.009 0.034 0.105 0.039 -0.021
  -0.014 -0.023 -0.010 -0.000 -0.040 0.051 -0.003 0.019 0.051 -0.005]
 [0.016 0.093 0.022 -0.096 0.010 0.084 -0.034 0.084 -0.091 0.050 -0.023
  0.075 -0.000 0.023 0.027 -0.069 -0.008 -0.008 -0.005 0.050]]
Avg. mu: -1.094, 0.118, 2.332, 1.688, 0.168, -0.539, 1.127, -0.115, 0.040, 0.667, -0.284, -0.405, -0.101, -0.084, 0.069, -0.239, 0.253, -0.012, -0.247, -0.331
Avg. var: 0.006, 0.004, 0.003, 0.008, 0.029, 0.017, 0.023, 0.010, 0.244, 0.226, 0.165, 0.075, 0.568, 0.414, 0.578, 0.244, 0.577, 0.521, 0.577, 0.753
Max. mu: 11.727, 9.175, 10.104, 11.159, 4.207, 8.630, 5.920, 5.846, 2.683, 1.861, 1.060, 1.710, 0.743, 0.961, 1.099, 2.464, 1.275, 1.287, 1.054, 0.698
Max. var: 0.287, 0.195, 0.047, 0.257, 0.173, 0.072, 0.154, 0.108, 0.780, 1.045, 0.620, 0.300, 1.457, 0.948, 1.962, 0.814, 1.396, 1.132, 1.836, 1.286
Min. mu: -10.112, -7.889, -1.842, -6.432, -2.480, -8.640, -4.362, -6.932, -2.172, -2.363, -2.593, -3.709, -1.534, -1.213, -1.844, -1.635, -1.051, -1.181, -1.438, -1.403
Min. var: 0.000, 0.000, 0.000, 0.000, 0.006, 0.002, 0.005, 0.003, 0.078, 0.049, 0.031, 0.018, 0.172, 0.133, 0.207, 0.071, 0.173, 0.168, 0.149, 0.218
Cov. mu:
[[6.077 -0.887 -1.751 -1.070 0.209 -1.596 -0.968 -0.217 -0.907 -0.326
  -0.506 0.180 -0.177 0.083 -0.165 0.040 0.112 0.062 -0.097 0.016]
 [-0.887 4.470 0.845 1.534 0.382 1.781 0.717 0.015 -0.182 0.180 -0.120
  0.296 -0.219 0.181 -0.022 0.114 0.066 0.030 -0.068 0.093]
 [-1.751 0.845 2.866 -0.346 -0.033 0.939 0.409 0.344 0.403 0.085 0.299
  -0.386 -0.220 -0.219 -0.074 0.191 0.000 0.041 0.069 0.022]
 [-1.070 1.534 -0.346 5.980 -0.091 0.858 -0.301 -0.221 0.522 0.235 0.339
  0.051 -0.067 0.003 0.048 0.465 0.323 -0.285 -0.077 -0.096]
 [0.209 0.382 -0.033 -0.091 0.837 -1.575 0.578 -0.410 0.087 -0.181 -0.008
  -0.298 -0.019 -0.039 0.077 -0.084 0.128 -0.119 -0.023 0.010]
 [-1.596 1.781 0.939 0.858 -1.575 5.129 -0.923 0.557 -0.218 0.413 0.045
  0.720 -0.127 0.204 -0.114 0.204 -0.317 0.278 -0.009 0.084]
 [-0.968 0.717 0.409 -0.301 0.578 -0.923 1.092 -0.609 0.347 -0.130 0.178
  -0.380 0.024 -0.048 0.068 -0.009 0.035 -0.024 0.034 -0.034]
 [-0.217 0.015 0.344 -0.221 -0.410 0.557 -0.609 2.850 -0.512 0.360 -0.561
  0.680 -0.082 -0.100 -0.308 0.019 0.018 0.019 0.105 0.084]
 [-0.907 -0.182 0.403 0.522 0.087 -0.218 0.347 -0.512 0.516 -0.139 0.261
  -0.389 0.008 -0.086 0.009 0.184 0.020 -0.024 0.039 -0.091]
 [-0.326 0.180 0.085 0.235 -0.181 0.413 -0.130 0.360 -0.139 0.262 -0.012
  0.230 0.033 0.022 0.062 -0.112 0.026 -0.025 -0.021 0.050]
 [-0.506 -0.120 0.299 0.339 -0.008 0.045 0.178 -0.561 0.261 -0.012 0.264
  -0.205 0.030 -0.030 0.090 0.021 0.002 -0.026 -0.014 -0.023]
 [0.180 0.296 -0.386 0.051 -0.298 0.720 -0.380 0.680 -0.389 0.230 -0.205
  0.491 0.033 0.102 -0.007 -0.138 -0.041 0.023 -0.023 0.075]
 [-0.177 -0.219 -0.220 -0.067 -0.019 -0.127 0.024 -0.082 0.008 0.033
  0.030 0.033 0.077 0.021 0.052 -0.080 -0.015 -0.017 -0.010 -0.000]
 [0.083 0.181 -0.219 0.003 -0.039 0.204 -0.048 -0.100 -0.086 0.022 -0.030
  0.102 0.021 0.078 0.019 -0.041 -0.025 0.017 -0.000 0.023]
 [-0.165 -0.022 -0.074 0.048 0.077 -0.114 0.068 -0.308 0.009 0.062 0.090
  -0.007 0.052 0.019 0.131 -0.128 0.015 -0.043 -0.040 0.027]
 [0.040 0.114 0.191 0.465 -0.084 0.204 -0.009 0.019 0.184 -0.112 0.021
  -0.138 -0.080 -0.041 -0.128 0.260 0.020 0.040 0.051 -0.069]
 [0.112 0.066 0.000 0.323 0.128 -0.317 0.035 0.018 0.020 0.026 0.002
  -0.041 -0.015 -0.025 0.015 0.020 0.088 -0.045 -0.003 -0.008]
 [0.062 0.030 0.041 -0.285 -0.119 0.278 -0.024 0.019 -0.024 -0.025 -0.026
  0.023 -0.017 0.017 -0.043 0.040 -0.045 0.066 0.019 -0.008]
 [-0.097 -0.068 0.069 -0.077 -0.023 -0.009 0.034 0.105 0.039 -0.021
  -0.014 -0.023 -0.010 -0.000 -0.040 0.051 -0.003 0.019 0.051 -0.005]
 [0.016 0.093 0.022 -0.096 0.010 0.084 -0.034 0.084 -0.091 0.050 -0.023
  0.075 -0.000 0.023 0.027 -0.069 -0.008 -0.008 -0.005 0.050]]
