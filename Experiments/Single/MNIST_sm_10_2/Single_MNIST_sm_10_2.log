Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            76,930
├─Linear: 1-2                            4,851
=================================================================
Total params: 81,781
Trainable params: 81,781
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            100
├─Linear: 1-2                            100
=================================================================
Total params: 200
Trainable params: 200
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            100
├─Linear: 1-2                            100
=================================================================
Total params: 200
Trainable params: 200
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            100
├─Linear: 1-2                            100
=================================================================
Total params: 200
Trainable params: 200
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            100
├─Linear: 1-2                            100
=================================================================
Total params: 200
Trainable params: 200
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            100
├─Linear: 1-2                            100
=================================================================
Total params: 200
Trainable params: 200
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            147
=================================================================
Total params: 147
Trainable params: 147
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            147
=================================================================
Total params: 147
Trainable params: 147
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            147
=================================================================
Total params: 147
Trainable params: 147
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            147
=================================================================
Total params: 147
Trainable params: 147
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            147
=================================================================
Total params: 147
Trainable params: 147
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            4,900
├─Linear: 1-2                            77,616
=================================================================
Total params: 82,516
Trainable params: 82,516
Non-trainable params: 0
=================================================================
[Epoch   1 (34.77s)]	ELBO: 1346.077, 1424.384, 1446.130, 1452.152, 1453.725 (1593.912)	Log prob: 1369.730, 1429.113, 1448.540, 1453.487, 1455.078 (1616.439)	KLD: 23.654, 4.729, 2.409, 1.336, 1.353 (22.526)	Grad: 0.097, 0.056, 0.046, 0.048, 0.053
[Epoch   2 (37.82s)]	ELBO: 1479.266, 1564.331, 1621.337, 1634.817, 1639.214 (1638.561)	Log prob: 1486.516, 1569.574, 1625.757, 1637.275, 1641.065 (1659.710)	KLD: 7.251, 5.243, 4.420, 2.458, 1.852 (21.149)	Grad: 0.087, 0.066, 0.047, 0.034, 0.033
[Epoch   3 (38.77s)]	ELBO: 1502.810, 1586.206, 1642.426, 1664.133, 1672.893 (1664.358)	Log prob: 1509.866, 1591.452, 1646.896, 1667.302, 1675.311 (1687.389)	KLD: 7.056, 5.244, 4.471, 3.169, 2.420 (23.031)	Grad: 0.086, 0.062, 0.052, 0.037, 0.035
[Epoch   4 (40.01s)]	ELBO: 1516.001, 1597.803, 1653.983, 1679.086, 1691.114 (1678.686)	Log prob: 1523.270, 1603.064, 1658.584, 1682.481, 1693.825 (1702.262)	KLD: 7.270, 5.261, 4.601, 3.396, 2.711 (23.577)	Grad: 0.081, 0.057, 0.052, 0.040, 0.037
[Epoch   5 (39.73s)]	ELBO: 1525.824, 1604.964, 1659.866, 1686.558, 1700.033 (1682.683)	Log prob: 1533.229, 1610.276, 1664.515, 1690.087, 1702.923 (1706.786)	KLD: 7.404, 5.311, 4.649, 3.529, 2.891 (24.103)	Grad: 0.083, 0.057, 0.052, 0.041, 0.038
[Epoch   6 (39.70s)]	ELBO: 1533.096, 1610.874, 1663.133, 1690.608, 1704.527 (1684.414)	Log prob: 1540.518, 1616.177, 1667.832, 1694.226, 1707.551 (1708.306)	KLD: 7.422, 5.304, 4.699, 3.618, 3.024 (23.892)	Grad: 0.087, 0.057, 0.053, 0.043, 0.041
[Epoch   7 (39.99s)]	ELBO: 1539.160, 1615.650, 1665.859, 1693.450, 1707.483 (1684.776)	Log prob: 1546.570, 1620.955, 1670.545, 1697.104, 1710.517 (1708.481)	KLD: 7.408, 5.305, 4.687, 3.654, 3.034 (23.705)	Grad: 0.091, 0.057, 0.053, 0.045, 0.042
[Epoch   8 (40.65s)]	ELBO: 1543.649, 1619.591, 1668.342, 1696.036, 1710.681 (1687.392)	Log prob: 1551.119, 1624.877, 1673.044, 1699.745, 1713.715 (1712.079)	KLD: 7.470, 5.286, 4.701, 3.707, 3.034 (24.686)	Grad: 0.089, 0.057, 0.053, 0.045, 0.041
[Epoch   9 (41.57s)]	ELBO: 1547.452, 1622.669, 1670.231, 1697.503, 1712.744 (1687.695)	Log prob: 1554.978, 1627.932, 1674.905, 1701.245, 1715.741 (1712.316)	KLD: 7.527, 5.263, 4.673, 3.742, 2.997 (24.621)	Grad: 0.093, 0.057, 0.054, 0.047, 0.043
[Epoch  10 (41.12s)]	ELBO: 1551.080, 1624.968, 1671.681, 1699.021, 1714.329 (1688.609)	Log prob: 1558.672, 1630.260, 1676.319, 1702.778, 1717.317 (1712.404)	KLD: 7.593, 5.293, 4.640, 3.756, 2.989 (23.795)	Grad: 0.093, 0.058, 0.054, 0.046, 0.043
[Epoch  11 (40.54s)]	ELBO: 1554.258, 1627.281, 1673.156, 1700.454, 1716.255 (1686.820)	Log prob: 1561.886, 1632.566, 1677.814, 1704.214, 1719.228 (1711.206)	KLD: 7.629, 5.287, 4.657, 3.760, 2.972 (24.386)	Grad: 0.094, 0.058, 0.053, 0.048, 0.043
[Epoch  12 (39.91s)]	ELBO: 1557.343, 1629.104, 1674.503, 1701.833, 1717.580 (1697.636)	Log prob: 1565.058, 1634.370, 1679.148, 1705.580, 1720.534 (1721.910)	KLD: 7.713, 5.267, 4.646, 3.748, 2.953 (24.274)	Grad: 0.095, 0.059, 0.054, 0.047, 0.044
[Epoch  13 (44.84s)]	ELBO: 1559.897, 1630.663, 1675.894, 1702.988, 1718.908 (1694.886)	Log prob: 1567.640, 1635.985, 1680.519, 1706.715, 1721.851 (1720.019)	KLD: 7.743, 5.322, 4.625, 3.727, 2.944 (25.132)	Grad: 0.095, 0.059, 0.053, 0.047, 0.044
[Epoch  14 (44.61s)]	ELBO: 1562.748, 1632.512, 1677.123, 1703.941, 1719.688 (1694.718)	Log prob: 1570.528, 1637.822, 1681.754, 1707.648, 1722.629 (1718.619)	KLD: 7.778, 5.310, 4.632, 3.705, 2.942 (23.900)	Grad: 0.096, 0.059, 0.052, 0.048, 0.045
[Epoch  15 (41.80s)]	ELBO: 1564.798, 1633.880, 1678.130, 1704.721, 1720.505 (1692.312)	Log prob: 1572.585, 1639.176, 1682.740, 1708.432, 1723.410 (1715.565)	KLD: 7.787, 5.297, 4.609, 3.711, 2.906 (23.252)	Grad: 0.095, 0.060, 0.053, 0.048, 0.045
[Epoch  16 (41.93s)]	ELBO: 1565.933, 1634.565, 1678.246, 1704.569, 1720.183 (1694.637)	Log prob: 1573.719, 1639.923, 1682.851, 1708.276, 1723.080 (1718.967)	KLD: 7.787, 5.357, 4.606, 3.706, 2.897 (24.330)	Grad: 0.097, 0.060, 0.052, 0.049, 0.045
[Epoch  17 (40.28s)]	ELBO: 1567.862, 1635.935, 1679.244, 1705.537, 1720.942 (1695.648)	Log prob: 1575.650, 1641.292, 1683.839, 1709.233, 1723.824 (1720.462)	KLD: 7.788, 5.356, 4.595, 3.696, 2.882 (24.814)	Grad: 0.098, 0.060, 0.052, 0.049, 0.046
[Epoch  18 (41.13s)]	ELBO: 1568.991, 1636.696, 1679.801, 1705.798, 1721.213 (1692.153)	Log prob: 1576.760, 1642.042, 1684.373, 1709.502, 1724.068 (1715.762)	KLD: 7.769, 5.346, 4.571, 3.704, 2.854 (23.608)	Grad: 0.098, 0.060, 0.052, 0.050, 0.046
[Epoch  19 (40.25s)]	ELBO: 1570.938, 1637.876, 1680.541, 1706.394, 1721.699 (1693.531)	Log prob: 1578.745, 1643.219, 1685.112, 1710.090, 1724.544 (1717.159)	KLD: 7.807, 5.343, 4.570, 3.695, 2.845 (23.628)	Grad: 0.098, 0.061, 0.051, 0.049, 0.045
[Epoch  20 (41.93s)]	ELBO: 1572.511, 1638.929, 1680.951, 1706.943, 1722.128 (1695.926)	Log prob: 1580.335, 1644.264, 1685.490, 1710.622, 1724.940 (1719.759)	KLD: 7.824, 5.336, 4.539, 3.678, 2.811 (23.833)	Grad: 0.101, 0.061, 0.053, 0.050, 0.046
[Epoch  21 (40.48s)]	ELBO: 1573.757, 1639.785, 1681.465, 1707.464, 1722.397 (1696.618)	Log prob: 1581.580, 1645.067, 1685.999, 1711.133, 1725.205 (1720.002)	KLD: 7.824, 5.283, 4.533, 3.669, 2.808 (23.384)	Grad: 0.102, 0.061, 0.052, 0.050, 0.047
[Epoch  22 (40.74s)]	ELBO: 1574.949, 1640.768, 1682.109, 1708.130, 1722.870 (1696.288)	Log prob: 1582.788, 1646.063, 1686.638, 1711.806, 1725.662 (1720.387)	KLD: 7.838, 5.295, 4.529, 3.676, 2.792 (24.099)	Grad: 0.101, 0.059, 0.052, 0.049, 0.046
[Epoch  23 (40.33s)]	ELBO: 1576.013, 1641.653, 1682.799, 1708.642, 1723.482 (1694.299)	Log prob: 1583.873, 1646.940, 1687.307, 1712.328, 1726.260 (1717.721)	KLD: 7.859, 5.288, 4.508, 3.687, 2.779 (23.421)	Grad: 0.102, 0.060, 0.051, 0.050, 0.046
[Epoch  24 (40.40s)]	ELBO: 1576.587, 1642.318, 1683.098, 1708.876, 1723.515 (1698.523)	Log prob: 1584.445, 1647.601, 1687.548, 1712.539, 1726.294 (1721.521)	KLD: 7.859, 5.285, 4.449, 3.663, 2.780 (22.998)	Grad: 0.106, 0.060, 0.052, 0.050, 0.047
[Epoch  25 (39.29s)]	ELBO: 1577.885, 1643.277, 1683.472, 1709.137, 1723.729 (1696.588)	Log prob: 1585.764, 1648.558, 1687.920, 1712.790, 1726.484 (1720.154)	KLD: 7.880, 5.281, 4.447, 3.653, 2.754 (23.566)	Grad: 0.104, 0.060, 0.051, 0.050, 0.046
[Epoch  26 (39.35s)]	ELBO: 1578.562, 1644.141, 1683.885, 1709.519, 1724.026 (1699.766)	Log prob: 1586.446, 1649.395, 1688.284, 1713.159, 1726.764 (1724.159)	KLD: 7.885, 5.254, 4.399, 3.640, 2.737 (24.393)	Grad: 0.107, 0.060, 0.052, 0.050, 0.046
[Epoch  27 (43.22s)]	ELBO: 1579.627, 1645.006, 1684.208, 1709.491, 1723.728 (1693.934)	Log prob: 1587.553, 1650.248, 1688.568, 1713.112, 1726.435 (1716.952)	KLD: 7.925, 5.243, 4.359, 3.622, 2.707 (23.018)	Grad: 0.108, 0.060, 0.052, 0.050, 0.047
[Epoch  28 (40.51s)]	ELBO: 1581.219, 1646.066, 1684.565, 1709.659, 1723.908 (1695.822)	Log prob: 1589.137, 1651.300, 1688.866, 1713.271, 1726.610 (1719.709)	KLD: 7.919, 5.233, 4.302, 3.612, 2.703 (23.887)	Grad: 0.108, 0.060, 0.051, 0.051, 0.048
[Epoch  29 (39.74s)]	ELBO: 1581.618, 1646.606, 1684.562, 1709.790, 1723.936 (1695.955)	Log prob: 1589.556, 1651.822, 1688.817, 1713.385, 1726.637 (1719.420)	KLD: 7.937, 5.215, 4.255, 3.595, 2.701 (23.465)	Grad: 0.112, 0.061, 0.052, 0.051, 0.048
[Epoch  30 (39.56s)]	ELBO: 1582.468, 1647.505, 1685.314, 1710.599, 1724.585 (1697.747)	Log prob: 1590.408, 1652.729, 1689.535, 1714.224, 1727.249 (1721.289)	KLD: 7.941, 5.223, 4.220, 3.624, 2.662 (23.542)	Grad: 0.110, 0.061, 0.052, 0.050, 0.047
[Epoch  31 (40.41s)]	ELBO: 1583.166, 1648.349, 1685.780, 1711.020, 1725.005 (1699.728)	Log prob: 1591.130, 1653.562, 1689.995, 1714.633, 1727.662 (1723.234)	KLD: 7.963, 5.214, 4.216, 3.613, 2.656 (23.506)	Grad: 0.113, 0.060, 0.053, 0.050, 0.047
[Epoch  32 (44.68s)]	ELBO: 1583.815, 1648.740, 1685.717, 1710.856, 1724.825 (1693.585)	Log prob: 1591.760, 1653.963, 1689.892, 1714.478, 1727.478 (1717.159)	KLD: 7.943, 5.224, 4.176, 3.622, 2.653 (23.574)	Grad: 0.113, 0.061, 0.053, 0.050, 0.047
[Epoch  33 (40.80s)]	ELBO: 1584.835, 1649.419, 1685.919, 1710.929, 1724.857 (1693.189)	Log prob: 1592.816, 1654.626, 1690.060, 1714.540, 1727.451 (1716.228)	KLD: 7.980, 5.208, 4.140, 3.611, 2.594 (23.038)	Grad: 0.114, 0.060, 0.053, 0.050, 0.048
[Epoch  34 (39.87s)]	ELBO: 1585.632, 1650.255, 1686.393, 1711.220, 1725.027 (1694.051)	Log prob: 1593.619, 1655.459, 1690.483, 1714.804, 1727.635 (1717.005)	KLD: 7.986, 5.205, 4.089, 3.585, 2.607 (22.955)	Grad: 0.117, 0.061, 0.053, 0.050, 0.048
[Epoch  35 (40.12s)]	ELBO: 1586.485, 1650.800, 1686.639, 1711.392, 1725.103 (1695.399)	Log prob: 1594.503, 1655.999, 1690.708, 1714.988, 1727.708 (1718.598)	KLD: 8.018, 5.199, 4.069, 3.596, 2.604 (23.199)	Grad: 0.118, 0.061, 0.053, 0.049, 0.048
[Epoch  36 (39.81s)]	ELBO: 1586.363, 1651.062, 1686.749, 1711.298, 1724.911 (1694.883)	Log prob: 1594.368, 1656.273, 1690.781, 1714.878, 1727.509 (1717.592)	KLD: 8.006, 5.211, 4.033, 3.580, 2.599 (22.709)	Grad: 0.119, 0.061, 0.053, 0.050, 0.048
[Epoch  37 (39.73s)]	ELBO: 1586.405, 1651.626, 1687.126, 1711.589, 1725.224 (1694.580)	Log prob: 1594.410, 1656.835, 1691.145, 1715.188, 1727.811 (1717.511)	KLD: 8.004, 5.209, 4.019, 3.598, 2.587 (22.931)	Grad: 0.122, 0.061, 0.052, 0.049, 0.048
[Epoch  38 (40.02s)]	ELBO: 1586.845, 1652.061, 1687.409, 1711.579, 1725.136 (1696.337)	Log prob: 1594.853, 1657.265, 1691.394, 1715.163, 1727.706 (1719.483)	KLD: 8.007, 5.204, 3.985, 3.583, 2.571 (23.146)	Grad: 0.123, 0.061, 0.052, 0.050, 0.049
[Epoch  39 (39.07s)]	ELBO: 1588.536, 1653.201, 1688.251, 1712.152, 1725.534 (1695.055)	Log prob: 1596.561, 1658.433, 1692.214, 1715.710, 1728.122 (1718.509)	KLD: 8.026, 5.231, 3.964, 3.559, 2.589 (23.454)	Grad: 0.122, 0.061, 0.052, 0.050, 0.048
[Epoch  40 (40.44s)]	ELBO: 1588.334, 1653.298, 1688.113, 1711.942, 1725.316 (1698.502)	Log prob: 1596.380, 1658.525, 1692.046, 1715.477, 1727.891 (1721.089)	KLD: 8.045, 5.227, 3.935, 3.535, 2.575 (22.587)	Grad: 0.125, 0.061, 0.053, 0.050, 0.049
[Epoch  41 (41.38s)]	ELBO: 1588.929, 1653.851, 1688.450, 1712.206, 1725.431 (1697.185)	Log prob: 1596.981, 1659.031, 1692.380, 1715.751, 1728.016 (1720.851)	KLD: 8.053, 5.180, 3.930, 3.544, 2.585 (23.666)	Grad: 0.127, 0.061, 0.052, 0.050, 0.049
[Epoch  42 (40.20s)]	ELBO: 1589.141, 1654.484, 1688.751, 1712.274, 1725.505 (1698.724)	Log prob: 1597.200, 1659.681, 1692.647, 1715.820, 1728.067 (1721.581)	KLD: 8.060, 5.199, 3.898, 3.547, 2.562 (22.857)	Grad: 0.127, 0.061, 0.053, 0.051, 0.049
[Epoch  43 (40.65s)]	ELBO: 1589.410, 1654.432, 1688.609, 1711.962, 1725.267 (1695.521)	Log prob: 1597.471, 1659.663, 1692.475, 1715.499, 1727.834 (1719.158)	KLD: 8.062, 5.232, 3.865, 3.535, 2.566 (23.637)	Grad: 0.131, 0.063, 0.053, 0.051, 0.049
[Epoch  44 (40.34s)]	ELBO: 1590.301, 1655.271, 1689.183, 1712.456, 1725.689 (1695.436)	Log prob: 1598.393, 1660.497, 1693.037, 1716.042, 1728.231 (1718.769)	KLD: 8.093, 5.226, 3.854, 3.586, 2.542 (23.333)	Grad: 0.132, 0.061, 0.054, 0.051, 0.049
[Epoch  45 (40.84s)]	ELBO: 1590.968, 1655.856, 1689.711, 1712.788, 1725.837 (1694.055)	Log prob: 1599.069, 1661.083, 1693.559, 1716.339, 1728.365 (1717.017)	KLD: 8.102, 5.228, 3.847, 3.550, 2.527 (22.963)	Grad: 0.132, 0.062, 0.053, 0.050, 0.049
[Epoch  46 (41.75s)]	ELBO: 1591.223, 1656.385, 1689.931, 1712.808, 1725.848 (1693.753)	Log prob: 1599.326, 1661.594, 1693.748, 1716.343, 1728.390 (1716.989)	KLD: 8.105, 5.210, 3.819, 3.536, 2.541 (23.237)	Grad: 0.133, 0.061, 0.053, 0.051, 0.049
[Epoch  47 (39.49s)]	ELBO: 1591.176, 1656.500, 1690.154, 1712.872, 1725.758 (1693.722)	Log prob: 1599.304, 1661.718, 1693.971, 1716.432, 1728.282 (1716.981)	KLD: 8.127, 5.217, 3.817, 3.561, 2.523 (23.259)	Grad: 0.134, 0.062, 0.053, 0.051, 0.050
[Epoch  48 (39.66s)]	ELBO: 1591.924, 1657.102, 1690.587, 1713.202, 1726.174 (1696.958)	Log prob: 1600.051, 1662.318, 1694.395, 1716.712, 1728.672 (1720.001)	KLD: 8.127, 5.217, 3.808, 3.512, 2.498 (23.044)	Grad: 0.136, 0.062, 0.054, 0.051, 0.050
[Epoch  49 (39.13s)]	ELBO: 1592.284, 1657.760, 1691.182, 1713.689, 1726.391 (1696.497)	Log prob: 1600.461, 1662.991, 1694.977, 1717.192, 1728.904 (1719.350)	KLD: 8.178, 5.230, 3.795, 3.502, 2.514 (22.853)	Grad: 0.139, 0.062, 0.053, 0.051, 0.050
[Epoch  50 (45.54s)]	ELBO: 1592.874, 1658.427, 1691.677, 1713.920, 1726.683 (1695.500)	Log prob: 1601.024, 1663.651, 1695.468, 1717.432, 1729.178 (1718.176)	KLD: 8.149, 5.225, 3.791, 3.511, 2.495 (22.675)	Grad: 0.137, 0.061, 0.054, 0.051, 0.050
No improvement after 25 epochs...
Best epoch(s): [26]	Training time(s): 2028.89s (2028.89s)	Best ELBO: 1726.683 (1699.766)	Best log prob: 1729.178 (1724.159)
Avg. mu: 0.404, -0.238, 0.324, 0.004, 0.384, -0.113, 0.141, 0.074, 0.047, -0.005
Avg. var: 0.001, 0.001, 0.006, 0.005, 0.009, 0.009, 0.018, 0.023, 0.061, 0.033
Max. mu: 4.415, 3.513, 3.909, 3.671, 3.692, 4.017, 3.564, 3.530, 3.148, 3.384
Max. var: 0.019, 0.010, 0.070, 0.047, 0.058, 0.072, 0.089, 0.153, 0.251, 0.174
Min. mu: -3.507, -3.700, -3.185, -4.101, -1.440, -2.574, -2.961, -3.169, -2.952, -3.056
Min. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.002, 0.003, 0.016, 0.004
Cov. mu:
[[1.315 -0.079 -0.048 -0.038 0.058 -0.035 -0.018 0.041 -0.047 -0.029]
 [-0.079 1.558 -0.164 -0.273 0.024 -0.050 -0.052 0.072 0.066 0.047]
 [-0.048 -0.164 0.901 -0.039 0.013 -0.087 -0.054 0.028 -0.017 -0.038]
 [-0.038 -0.273 -0.039 0.809 -0.018 0.004 -0.051 -0.054 -0.026 -0.041]
 [0.058 0.024 0.013 -0.018 0.375 0.080 -0.022 0.043 -0.006 -0.007]
 [-0.035 -0.050 -0.087 0.004 0.080 0.517 -0.004 -0.020 -0.001 -0.023]
 [-0.018 -0.052 -0.054 -0.051 -0.022 -0.004 0.536 0.031 0.017 -0.022]
 [0.041 0.072 0.028 -0.054 0.043 -0.020 0.031 0.594 -0.015 -0.005]
 [-0.047 0.066 -0.017 -0.026 -0.006 -0.001 0.017 -0.015 0.613 0.006]
 [-0.029 0.047 -0.038 -0.041 -0.007 -0.023 -0.022 -0.005 0.006 0.610]]
Avg. mu: 0.404, -0.238, 0.324, 0.004, 0.384, -0.113, 0.141, 0.074, 0.047, -0.005
Avg. var: 0.001, 0.001, 0.006, 0.005, 0.009, 0.009, 0.018, 0.023, 0.061, 0.033
Max. mu: 4.415, 3.513, 3.909, 3.671, 3.692, 4.017, 3.564, 3.530, 3.148, 3.384
Max. var: 0.019, 0.010, 0.070, 0.047, 0.058, 0.072, 0.089, 0.153, 0.251, 0.174
Min. mu: -3.507, -3.700, -3.185, -4.101, -1.440, -2.574, -2.961, -3.169, -2.952, -3.056
Min. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.002, 0.003, 0.016, 0.004
Cov. mu:
[[1.315 -0.079 -0.048 -0.038 0.058 -0.035 -0.018 0.041 -0.047 -0.029]
 [-0.079 1.558 -0.164 -0.273 0.024 -0.050 -0.052 0.072 0.066 0.047]
 [-0.048 -0.164 0.901 -0.039 0.013 -0.087 -0.054 0.028 -0.017 -0.038]
 [-0.038 -0.273 -0.039 0.809 -0.018 0.004 -0.051 -0.054 -0.026 -0.041]
 [0.058 0.024 0.013 -0.018 0.375 0.080 -0.022 0.043 -0.006 -0.007]
 [-0.035 -0.050 -0.087 0.004 0.080 0.517 -0.004 -0.020 -0.001 -0.023]
 [-0.018 -0.052 -0.054 -0.051 -0.022 -0.004 0.536 0.031 0.017 -0.022]
 [0.041 0.072 0.028 -0.054 0.043 -0.020 0.031 0.594 -0.015 -0.005]
 [-0.047 0.066 -0.017 -0.026 -0.006 -0.001 0.017 -0.015 0.613 0.006]
 [-0.029 0.047 -0.038 -0.041 -0.007 -0.023 -0.022 -0.005 0.006 0.610]]
