Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            76,930
├─Linear: 1-2                            4,851
=================================================================
Total params: 81,781
Trainable params: 81,781
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            4,900
├─Linear: 1-2                            77,616
=================================================================
Total params: 82,516
Trainable params: 82,516
Non-trainable params: 0
=================================================================
[Epoch   1 (65.42s)]	ELBO: 1308.397, 1398.347, 1443.419, 1469.609, 1485.102, 1489.934, 1494.342, 1495.936, 1497.761, 1499.526 (1587.537)	Log prob: 1323.397, 1404.056, 1445.967, 1471.399, 1486.518, 1490.808, 1495.407, 1496.642, 1498.436, 1499.958 (1608.561)	KLD: 15.000, 5.709, 2.548, 1.790, 1.416, 0.875, 1.065, 0.707, 0.675, 0.431 (21.025)	Grad: 0.087, 0.079, 0.060, 0.050, 0.051, 0.052, 0.060, 0.061, 0.064, 0.063
[Epoch   2 (75.22s)]	ELBO: 1398.172, 1467.495, 1526.422, 1567.371, 1593.151, 1612.350, 1619.858, 1623.489, 1626.241, 1630.092 (1626.199)	Log prob: 1401.661, 1470.911, 1529.510, 1570.010, 1595.486, 1614.423, 1621.527, 1624.898, 1627.781, 1631.024 (1650.516)	KLD: 3.488, 3.416, 3.089, 2.639, 2.335, 2.074, 1.669, 1.408, 1.540, 0.931 (24.317)	Grad: 0.077, 0.058, 0.050, 0.047, 0.055, 0.053, 0.055, 0.061, 0.069, 0.055
[Epoch   3 (77.13s)]	ELBO: 1412.950, 1490.492, 1544.932, 1584.870, 1612.400, 1633.188, 1644.753, 1649.425, 1654.329, 1659.454 (1645.169)	Log prob: 1416.802, 1494.010, 1548.154, 1587.707, 1614.985, 1635.702, 1646.698, 1651.186, 1656.275, 1660.727 (1670.715)	KLD: 3.851, 3.520, 3.223, 2.838, 2.585, 2.515, 1.945, 1.762, 1.947, 1.273 (25.546)	Grad: 0.068, 0.053, 0.053, 0.049, 0.055, 0.063, 0.060, 0.064, 0.076, 0.056
[Epoch   4 (76.63s)]	ELBO: 1421.814, 1504.573, 1551.335, 1591.097, 1618.577, 1639.685, 1654.580, 1662.024, 1668.687, 1675.080 (1649.577)	Log prob: 1425.727, 1508.143, 1554.639, 1594.037, 1621.302, 1642.297, 1656.689, 1663.945, 1670.742, 1676.423 (1676.011)	KLD: 3.913, 3.569, 3.304, 2.940, 2.725, 2.613, 2.108, 1.920, 2.055, 1.343 (26.434)	Grad: 0.063, 0.049, 0.057, 0.056, 0.063, 0.070, 0.068, 0.070, 0.075, 0.056
[Epoch   5 (81.51s)]	ELBO: 1427.530, 1512.915, 1556.021, 1595.084, 1622.583, 1644.090, 1660.549, 1670.002, 1676.774, 1683.328 (1656.872)	Log prob: 1431.517, 1516.504, 1559.319, 1598.103, 1625.387, 1646.729, 1662.732, 1672.004, 1678.813, 1684.742 (1684.188)	KLD: 3.988, 3.588, 3.298, 3.019, 2.803, 2.638, 2.183, 2.002, 2.039, 1.415 (27.316)	Grad: 0.061, 0.054, 0.053, 0.056, 0.062, 0.070, 0.067, 0.068, 0.069, 0.057
[Epoch   6 (78.45s)]	ELBO: 1434.014, 1517.141, 1559.638, 1597.561, 1624.965, 1646.253, 1663.282, 1674.611, 1681.952, 1688.323 (1656.736)	Log prob: 1438.056, 1520.787, 1562.925, 1600.591, 1627.743, 1648.904, 1665.496, 1676.657, 1683.926, 1689.789 (1683.229)	KLD: 4.042, 3.646, 3.287, 3.031, 2.778, 2.651, 2.213, 2.046, 1.974, 1.466 (26.493)	Grad: 0.066, 0.057, 0.052, 0.058, 0.060, 0.073, 0.069, 0.071, 0.068, 0.059
[Epoch   7 (84.52s)]	ELBO: 1441.050, 1520.896, 1562.605, 1599.741, 1627.500, 1648.424, 1664.861, 1676.899, 1685.066, 1691.515 (1658.929)	Log prob: 1445.117, 1524.586, 1565.828, 1602.770, 1630.250, 1651.080, 1667.109, 1679.013, 1686.992, 1693.005 (1684.524)	KLD: 4.066, 3.690, 3.223, 3.031, 2.751, 2.657, 2.249, 2.115, 1.926, 1.491 (25.595)	Grad: 0.066, 0.060, 0.050, 0.055, 0.057, 0.069, 0.071, 0.072, 0.069, 0.061
[Epoch   8 (82.95s)]	ELBO: 1448.255, 1524.756, 1564.688, 1600.879, 1628.470, 1650.206, 1666.324, 1678.914, 1688.014, 1694.324 (1659.762)	Log prob: 1452.314, 1528.413, 1567.827, 1603.851, 1631.203, 1652.856, 1668.587, 1681.045, 1689.943, 1695.835 (1686.998)	KLD: 4.059, 3.657, 3.139, 2.973, 2.733, 2.649, 2.261, 2.131, 1.929, 1.512 (27.236)	Grad: 0.068, 0.061, 0.047, 0.053, 0.055, 0.066, 0.068, 0.068, 0.065, 0.060
[Epoch   9 (78.68s)]	ELBO: 1455.051, 1528.711, 1567.807, 1603.968, 1630.453, 1652.504, 1667.918, 1680.272, 1690.229, 1696.573 (1662.379)	Log prob: 1459.147, 1532.335, 1570.866, 1606.870, 1633.186, 1655.137, 1670.185, 1682.425, 1692.148, 1698.076 (1689.147)	KLD: 4.096, 3.624, 3.059, 2.902, 2.735, 2.632, 2.266, 2.153, 1.920, 1.501 (26.768)	Grad: 0.071, 0.060, 0.049, 0.052, 0.056, 0.066, 0.070, 0.069, 0.066, 0.060
[Epoch  10 (78.24s)]	ELBO: 1459.720, 1531.155, 1570.324, 1606.514, 1631.785, 1653.978, 1669.307, 1681.363, 1692.120, 1698.277 (1662.370)	Log prob: 1463.819, 1534.741, 1573.320, 1609.336, 1634.516, 1656.587, 1671.551, 1683.550, 1694.022, 1699.790 (1687.838)	KLD: 4.100, 3.587, 2.995, 2.822, 2.730, 2.610, 2.245, 2.187, 1.904, 1.513 (25.469)	Grad: 0.075, 0.059, 0.048, 0.051, 0.056, 0.064, 0.068, 0.069, 0.065, 0.060
[Epoch  11 (79.14s)]	ELBO: 1463.564, 1534.309, 1573.774, 1608.698, 1632.965, 1655.061, 1670.822, 1683.035, 1693.882, 1700.173 (1668.547)	Log prob: 1467.742, 1537.855, 1576.761, 1611.492, 1635.695, 1657.705, 1673.098, 1685.239, 1695.808, 1701.694 (1695.488)	KLD: 4.176, 3.546, 2.986, 2.794, 2.730, 2.644, 2.275, 2.203, 1.927, 1.521 (26.941)	Grad: 0.078, 0.057, 0.049, 0.050, 0.058, 0.067, 0.068, 0.069, 0.064, 0.058
[Epoch  12 (77.61s)]	ELBO: 1466.269, 1536.862, 1577.141, 1611.075, 1634.469, 1656.350, 1671.988, 1683.936, 1694.834, 1701.311 (1666.679)	Log prob: 1470.508, 1540.412, 1580.159, 1613.825, 1637.216, 1659.000, 1674.249, 1686.132, 1696.750, 1702.840 (1694.275)	KLD: 4.239, 3.551, 3.017, 2.751, 2.747, 2.650, 2.260, 2.196, 1.915, 1.528 (27.596)	Grad: 0.081, 0.057, 0.051, 0.050, 0.060, 0.068, 0.070, 0.070, 0.066, 0.060
[Epoch  13 (77.29s)]	ELBO: 1468.481, 1538.582, 1579.098, 1612.389, 1635.450, 1657.105, 1673.252, 1685.322, 1696.156, 1702.750 (1657.171)	Log prob: 1472.770, 1542.119, 1582.094, 1615.116, 1638.208, 1659.779, 1675.512, 1687.533, 1698.069, 1704.304 (1683.311)	KLD: 4.288, 3.538, 2.996, 2.727, 2.757, 2.673, 2.261, 2.212, 1.913, 1.553 (26.140)	Grad: 0.082, 0.059, 0.052, 0.052, 0.060, 0.068, 0.070, 0.070, 0.065, 0.061
[Epoch  14 (79.36s)]	ELBO: 1470.756, 1540.082, 1581.330, 1614.593, 1637.507, 1659.170, 1675.369, 1687.379, 1697.717, 1704.383 (1664.912)	Log prob: 1475.081, 1543.610, 1584.342, 1617.289, 1640.280, 1661.850, 1677.623, 1689.563, 1699.622, 1705.939 (1690.573)	KLD: 4.326, 3.527, 3.011, 2.697, 2.773, 2.680, 2.253, 2.183, 1.905, 1.557 (25.661)	Grad: 0.086, 0.059, 0.052, 0.051, 0.060, 0.067, 0.069, 0.070, 0.065, 0.061
[Epoch  15 (77.71s)]	ELBO: 1472.896, 1541.309, 1582.670, 1616.101, 1638.516, 1659.927, 1676.032, 1687.994, 1698.535, 1705.382 (1658.280)	Log prob: 1477.289, 1544.840, 1585.675, 1618.791, 1641.295, 1662.613, 1678.267, 1690.163, 1700.421, 1706.960 (1684.230)	KLD: 4.393, 3.530, 3.004, 2.690, 2.780, 2.687, 2.235, 2.169, 1.885, 1.579 (25.950)	Grad: 0.088, 0.060, 0.052, 0.051, 0.062, 0.067, 0.070, 0.070, 0.066, 0.062
[Epoch  16 (75.83s)]	ELBO: 1474.756, 1543.331, 1584.480, 1617.659, 1639.603, 1660.411, 1676.591, 1688.469, 1698.945, 1706.027 (1669.964)	Log prob: 1479.210, 1546.845, 1587.475, 1620.334, 1642.372, 1663.117, 1678.811, 1690.650, 1700.855, 1707.621 (1697.116)	KLD: 4.454, 3.514, 2.995, 2.674, 2.771, 2.706, 2.220, 2.181, 1.909, 1.592 (27.153)	Grad: 0.091, 0.058, 0.052, 0.050, 0.061, 0.067, 0.070, 0.070, 0.066, 0.062
[Epoch  17 (83.42s)]	ELBO: 1475.198, 1543.996, 1585.798, 1618.501, 1640.041, 1660.738, 1676.960, 1688.778, 1699.208, 1706.334 (1671.309)	Log prob: 1479.706, 1547.522, 1588.779, 1621.158, 1642.833, 1663.445, 1679.188, 1690.939, 1701.076, 1707.912 (1698.233)	KLD: 4.508, 3.526, 2.981, 2.658, 2.793, 2.706, 2.227, 2.160, 1.868, 1.578 (26.924)	Grad: 0.100, 0.060, 0.054, 0.053, 0.063, 0.070, 0.072, 0.072, 0.067, 0.063
[Epoch  18 (77.60s)]	ELBO: 1477.585, 1545.497, 1587.425, 1620.422, 1641.741, 1662.278, 1678.185, 1690.305, 1700.272, 1707.371 (1673.795)	Log prob: 1482.157, 1549.047, 1590.395, 1623.063, 1644.562, 1664.961, 1680.432, 1692.483, 1702.124, 1708.958 (1700.739)	KLD: 4.572, 3.550, 2.969, 2.641, 2.821, 2.684, 2.246, 2.178, 1.851, 1.587 (26.943)	Grad: 0.098, 0.060, 0.053, 0.050, 0.060, 0.066, 0.070, 0.069, 0.066, 0.062
[Epoch  19 (78.34s)]	ELBO: 1479.348, 1546.274, 1588.777, 1621.594, 1642.916, 1663.014, 1678.501, 1690.442, 1700.381, 1707.371 (1674.847)	Log prob: 1483.979, 1549.822, 1591.744, 1624.208, 1645.709, 1665.686, 1680.716, 1692.607, 1702.208, 1708.927 (1702.337)	KLD: 4.632, 3.549, 2.967, 2.613, 2.793, 2.670, 2.217, 2.165, 1.825, 1.556 (27.491)	Grad: 0.104, 0.061, 0.053, 0.051, 0.061, 0.066, 0.069, 0.072, 0.066, 0.064
[Epoch  20 (77.34s)]	ELBO: 1479.181, 1545.693, 1588.043, 1620.613, 1642.443, 1662.118, 1677.361, 1689.208, 1699.393, 1706.532 (1662.288)	Log prob: 1483.887, 1549.251, 1590.995, 1623.214, 1645.243, 1664.759, 1679.575, 1691.367, 1701.188, 1708.082 (1689.089)	KLD: 4.706, 3.558, 2.951, 2.602, 2.800, 2.641, 2.212, 2.160, 1.793, 1.550 (26.801)	Grad: 0.111, 0.061, 0.053, 0.052, 0.059, 0.067, 0.071, 0.073, 0.067, 0.065
[Epoch  21 (77.51s)]	ELBO: 1478.747, 1544.759, 1587.858, 1619.937, 1641.371, 1661.252, 1676.770, 1689.251, 1699.010, 1706.337 (1658.677)	Log prob: 1483.460, 1548.349, 1590.819, 1622.513, 1644.167, 1663.866, 1678.998, 1691.432, 1700.812, 1707.889 (1683.967)	KLD: 4.713, 3.591, 2.960, 2.575, 2.795, 2.612, 2.229, 2.179, 1.801, 1.553 (25.290)	Grad: 0.117, 0.062, 0.054, 0.052, 0.061, 0.067, 0.071, 0.071, 0.066, 0.064
[Epoch  22 (77.49s)]	ELBO: 1478.908, 1544.035, 1586.634, 1619.428, 1642.189, 1661.523, 1676.707, 1688.476, 1698.449, 1705.811 (1662.935)	Log prob: 1483.677, 1547.603, 1589.589, 1622.002, 1644.986, 1664.115, 1678.929, 1690.639, 1700.217, 1707.354 (1690.083)	KLD: 4.770, 3.569, 2.953, 2.574, 2.797, 2.591, 2.221, 2.164, 1.769, 1.543 (27.148)	Grad: 0.128, 0.060, 0.055, 0.052, 0.060, 0.067, 0.069, 0.071, 0.066, 0.065
[Epoch  23 (76.46s)]	ELBO: 1481.559, 1546.567, 1588.993, 1621.164, 1643.068, 1662.177, 1677.140, 1689.344, 1699.442, 1706.881 (1668.407)	Log prob: 1486.396, 1550.155, 1591.942, 1623.707, 1645.882, 1664.740, 1679.377, 1691.516, 1701.202, 1708.441 (1695.433)	KLD: 4.835, 3.588, 2.949, 2.542, 2.812, 2.562, 2.238, 2.171, 1.759, 1.559 (27.027)	Grad: 0.128, 0.060, 0.054, 0.053, 0.062, 0.067, 0.072, 0.074, 0.068, 0.065
[Epoch  24 (80.41s)]	ELBO: 1482.652, 1548.001, 1590.672, 1622.834, 1644.363, 1663.253, 1678.285, 1690.653, 1700.445, 1707.628 (1668.058)	Log prob: 1487.488, 1551.591, 1593.625, 1625.372, 1647.151, 1665.803, 1680.506, 1692.837, 1702.170, 1709.172 (1696.026)	KLD: 4.835, 3.590, 2.953, 2.539, 2.789, 2.552, 2.221, 2.185, 1.727, 1.544 (27.968)	Grad: 0.133, 0.061, 0.055, 0.052, 0.061, 0.066, 0.070, 0.071, 0.066, 0.065
[Epoch  25 (76.49s)]	ELBO: 1483.451, 1549.357, 1591.357, 1623.852, 1645.378, 1663.582, 1678.573, 1690.968, 1700.966, 1708.101 (1669.584)	Log prob: 1488.347, 1552.975, 1594.330, 1626.371, 1648.175, 1666.121, 1680.824, 1693.135, 1702.682, 1709.659 (1696.126)	KLD: 4.894, 3.617, 2.973, 2.520, 2.797, 2.539, 2.252, 2.167, 1.716, 1.558 (26.542)	Grad: 0.140, 0.060, 0.056, 0.054, 0.061, 0.069, 0.072, 0.076, 0.068, 0.067
[Epoch  26 (80.89s)]	ELBO: 1485.431, 1551.018, 1592.935, 1624.871, 1645.648, 1663.166, 1678.502, 1690.985, 1701.127, 1708.195 (1668.154)	Log prob: 1490.349, 1554.644, 1595.877, 1627.380, 1648.416, 1665.706, 1680.758, 1693.164, 1702.860, 1709.714 (1694.726)	KLD: 4.918, 3.626, 2.943, 2.508, 2.766, 2.540, 2.255, 2.178, 1.732, 1.518 (26.572)	Grad: 0.134, 0.061, 0.056, 0.054, 0.062, 0.069, 0.072, 0.074, 0.068, 0.067
[Epoch  27 (66.86s)]	ELBO: 1486.275, 1551.418, 1593.537, 1625.961, 1646.134, 1663.453, 1678.388, 1691.036, 1701.001, 1707.859 (1670.966)	Log prob: 1491.243, 1555.096, 1596.503, 1628.464, 1648.908, 1665.989, 1680.616, 1693.212, 1702.686, 1709.383 (1697.974)	KLD: 4.968, 3.677, 2.967, 2.504, 2.774, 2.537, 2.228, 2.177, 1.685, 1.524 (27.008)	Grad: 0.136, 0.064, 0.057, 0.053, 0.061, 0.067, 0.072, 0.074, 0.068, 0.068
[Epoch  28 (61.64s)]	ELBO: 1485.658, 1550.009, 1591.957, 1624.376, 1644.389, 1661.593, 1677.011, 1690.329, 1700.108, 1706.822 (1663.204)	Log prob: 1490.641, 1553.683, 1594.971, 1626.870, 1647.126, 1664.123, 1679.266, 1692.495, 1701.800, 1708.327 (1690.709)	KLD: 4.982, 3.675, 3.014, 2.494, 2.738, 2.532, 2.255, 2.166, 1.692, 1.505 (27.506)	Grad: 0.136, 0.066, 0.059, 0.055, 0.061, 0.067, 0.069, 0.071, 0.067, 0.065
[Epoch  29 (60.78s)]	ELBO: 1488.885, 1552.777, 1595.060, 1627.240, 1647.685, 1664.138, 1679.004, 1691.349, 1701.321, 1707.908 (1666.781)	Log prob: 1493.938, 1556.473, 1598.073, 1629.739, 1650.439, 1666.683, 1681.247, 1693.467, 1702.982, 1709.407 (1694.355)	KLD: 5.052, 3.696, 3.013, 2.498, 2.754, 2.544, 2.243, 2.117, 1.662, 1.499 (27.574)	Grad: 0.143, 0.066, 0.059, 0.055, 0.061, 0.068, 0.072, 0.074, 0.068, 0.068
[Epoch  30 (59.95s)]	ELBO: 1487.094, 1551.271, 1593.711, 1626.639, 1647.295, 1663.534, 1678.061, 1690.559, 1700.611, 1707.361 (1670.932)	Log prob: 1492.118, 1554.960, 1596.750, 1629.146, 1650.030, 1666.099, 1680.297, 1692.681, 1702.295, 1708.878 (1697.573)	KLD: 5.024, 3.689, 3.037, 2.506, 2.735, 2.565, 2.236, 2.122, 1.684, 1.518 (26.641)	Grad: 0.155, 0.067, 0.060, 0.056, 0.060, 0.068, 0.071, 0.073, 0.069, 0.068
[Epoch  31 (60.65s)]	ELBO: 1489.584, 1553.735, 1596.012, 1628.862, 1649.665, 1665.393, 1679.842, 1692.128, 1702.100, 1708.550 (1676.320)	Log prob: 1494.642, 1557.468, 1599.065, 1631.378, 1652.366, 1667.963, 1682.069, 1694.219, 1703.778, 1710.062 (1703.974)	KLD: 5.058, 3.734, 3.052, 2.515, 2.702, 2.570, 2.229, 2.091, 1.677, 1.511 (27.654)	Grad: 0.144, 0.066, 0.060, 0.055, 0.060, 0.066, 0.069, 0.071, 0.067, 0.066
[Epoch  32 (61.21s)]	ELBO: 1491.005, 1554.177, 1596.533, 1628.995, 1649.606, 1664.945, 1679.682, 1691.568, 1701.564, 1707.759 (1663.924)	Log prob: 1496.081, 1557.910, 1599.581, 1631.505, 1652.311, 1667.502, 1681.895, 1693.636, 1703.243, 1709.259 (1690.961)	KLD: 5.076, 3.733, 3.049, 2.509, 2.705, 2.558, 2.212, 2.067, 1.678, 1.500 (27.036)	Grad: 0.150, 0.069, 0.060, 0.056, 0.061, 0.068, 0.071, 0.073, 0.068, 0.068
[Epoch  33 (60.92s)]	ELBO: 1490.847, 1554.469, 1596.444, 1628.991, 1650.138, 1665.401, 1679.753, 1691.904, 1702.036, 1708.542 (1663.254)	Log prob: 1495.956, 1558.217, 1599.490, 1631.512, 1652.836, 1667.972, 1681.959, 1693.990, 1703.702, 1710.023 (1691.066)	KLD: 5.109, 3.748, 3.047, 2.521, 2.698, 2.569, 2.207, 2.086, 1.666, 1.481 (27.812)	Grad: 0.154, 0.069, 0.061, 0.055, 0.060, 0.067, 0.070, 0.072, 0.067, 0.066
[Epoch  34 (61.09s)]	ELBO: 1492.815, 1556.786, 1598.716, 1630.896, 1652.264, 1667.257, 1681.417, 1693.212, 1703.202, 1709.454 (1668.866)	Log prob: 1497.938, 1560.530, 1601.775, 1633.417, 1654.932, 1669.820, 1683.589, 1695.293, 1704.848, 1710.940 (1697.416)	KLD: 5.123, 3.744, 3.059, 2.522, 2.668, 2.563, 2.172, 2.080, 1.645, 1.486 (28.551)	Grad: 0.157, 0.068, 0.060, 0.055, 0.060, 0.067, 0.071, 0.073, 0.067, 0.067
[Epoch  35 (61.89s)]	ELBO: 1492.949, 1557.745, 1600.206, 1631.631, 1653.160, 1668.445, 1682.447, 1693.543, 1703.387, 1709.666 (1673.025)	Log prob: 1498.125, 1561.493, 1603.289, 1634.146, 1655.823, 1670.993, 1684.621, 1695.629, 1705.003, 1711.148 (1700.166)	KLD: 5.176, 3.748, 3.082, 2.512, 2.665, 2.548, 2.174, 2.087, 1.614, 1.481 (27.141)	Grad: 0.166, 0.068, 0.060, 0.055, 0.059, 0.064, 0.066, 0.069, 0.065, 0.064
[Epoch  36 (57.68s)]	ELBO: 1493.897, 1558.785, 1601.490, 1633.151, 1654.554, 1669.813, 1683.837, 1694.751, 1704.431, 1710.589 (1660.646)	Log prob: 1499.047, 1562.535, 1604.574, 1635.684, 1657.192, 1672.362, 1685.989, 1696.828, 1706.036, 1712.080 (1686.864)	KLD: 5.148, 3.750, 3.084, 2.533, 2.639, 2.549, 2.152, 2.077, 1.606, 1.490 (26.218)	Grad: 0.162, 0.070, 0.061, 0.054, 0.059, 0.063, 0.067, 0.070, 0.065, 0.065
[Epoch  37 (44.91s)]	ELBO: 1488.402, 1553.051, 1595.381, 1628.055, 1650.142, 1665.315, 1679.703, 1690.670, 1700.491, 1706.447 (1667.048)	Log prob: 1493.492, 1556.842, 1598.490, 1630.605, 1652.758, 1667.844, 1681.892, 1692.746, 1702.069, 1707.910 (1693.743)	KLD: 5.091, 3.789, 3.110, 2.549, 2.617, 2.529, 2.189, 2.076, 1.578, 1.463 (26.695)	Grad: 0.156, 0.075, 0.062, 0.055, 0.059, 0.064, 0.069, 0.072, 0.066, 0.066
[Epoch  38 (53.70s)]	ELBO: 1491.856, 1556.406, 1598.474, 1630.661, 1652.484, 1667.458, 1681.703, 1692.484, 1702.202, 1707.965 (1670.454)	Log prob: 1496.943, 1560.221, 1601.603, 1633.210, 1655.107, 1669.976, 1683.870, 1694.566, 1703.781, 1709.430 (1696.914)	KLD: 5.088, 3.814, 3.129, 2.549, 2.624, 2.517, 2.168, 2.081, 1.580, 1.464 (26.460)	Grad: 0.158, 0.076, 0.062, 0.055, 0.060, 0.065, 0.070, 0.071, 0.066, 0.066
[Epoch  39 (45.37s)]	ELBO: 1490.526, 1556.910, 1598.274, 1629.736, 1651.893, 1666.619, 1681.082, 1691.831, 1701.556, 1707.379 (1662.480)	Log prob: 1495.634, 1560.735, 1601.362, 1632.298, 1654.530, 1669.116, 1683.250, 1693.929, 1703.132, 1708.825 (1689.067)	KLD: 5.108, 3.825, 3.090, 2.560, 2.637, 2.497, 2.168, 2.098, 1.575, 1.446 (26.587)	Grad: 0.166, 0.073, 0.063, 0.057, 0.060, 0.066, 0.071, 0.073, 0.066, 0.067
[Epoch  40 (37.70s)]	ELBO: 1492.003, 1558.004, 1601.006, 1632.041, 1653.964, 1668.737, 1683.154, 1693.757, 1703.387, 1709.054 (1668.172)	Log prob: 1497.121, 1561.840, 1604.130, 1634.614, 1656.599, 1671.237, 1685.340, 1695.848, 1704.966, 1710.452 (1694.958)	KLD: 5.118, 3.836, 3.124, 2.573, 2.635, 2.501, 2.184, 2.091, 1.579, 1.397 (26.786)	Grad: 0.172, 0.075, 0.062, 0.056, 0.059, 0.066, 0.070, 0.072, 0.066, 0.067
[Epoch  41 (32.10s)]	ELBO: 1489.645, 1556.838, 1599.460, 1630.679, 1653.010, 1667.690, 1681.630, 1692.147, 1701.726, 1707.268 (1670.365)	Log prob: 1494.769, 1560.671, 1602.587, 1633.255, 1655.627, 1670.152, 1683.792, 1694.257, 1703.285, 1708.693 (1697.334)	KLD: 5.123, 3.834, 3.129, 2.576, 2.618, 2.464, 2.164, 2.110, 1.561, 1.424 (26.970)	Grad: 0.167, 0.078, 0.062, 0.055, 0.060, 0.065, 0.068, 0.071, 0.066, 0.067
[Epoch  42 (34.04s)]	ELBO: 1488.042, 1555.581, 1598.206, 1629.114, 1651.450, 1666.573, 1680.653, 1691.692, 1701.372, 1707.218 (1665.976)	Log prob: 1493.114, 1559.410, 1601.331, 1631.687, 1654.098, 1669.058, 1682.829, 1693.807, 1702.922, 1708.612 (1692.206)	KLD: 5.071, 3.830, 3.124, 2.571, 2.647, 2.485, 2.177, 2.115, 1.549, 1.393 (26.230)	Grad: 0.166, 0.076, 0.063, 0.057, 0.060, 0.064, 0.067, 0.070, 0.064, 0.063
[Epoch  43 (33.17s)]	ELBO: 1491.471, 1558.454, 1600.477, 1630.897, 1653.398, 1668.420, 1682.249, 1692.811, 1702.492, 1708.386 (1668.775)	Log prob: 1496.553, 1562.273, 1603.618, 1633.482, 1656.023, 1670.882, 1684.423, 1694.917, 1704.059, 1709.715 (1696.819)	KLD: 5.082, 3.821, 3.140, 2.584, 2.625, 2.462, 2.174, 2.107, 1.568, 1.328 (28.045)	Grad: 0.176, 0.074, 0.062, 0.055, 0.059, 0.063, 0.066, 0.068, 0.063, 0.062
[Epoch  44 (34.00s)]	ELBO: 1493.150, 1559.501, 1601.316, 1631.140, 1653.560, 1668.564, 1682.503, 1693.235, 1702.976, 1708.793 (1674.654)	Log prob: 1498.272, 1563.321, 1604.432, 1633.726, 1656.201, 1671.059, 1684.698, 1695.325, 1704.552, 1710.121 (1701.671)	KLD: 5.122, 3.820, 3.117, 2.587, 2.640, 2.494, 2.195, 2.090, 1.576, 1.329 (27.017)	Grad: 0.189, 0.074, 0.062, 0.055, 0.058, 0.062, 0.066, 0.067, 0.063, 0.061
[Epoch  45 (29.81s)]	ELBO: 1495.193, 1560.797, 1602.557, 1632.277, 1654.377, 1669.013, 1683.145, 1693.517, 1703.637, 1709.132 (1671.322)	Log prob: 1500.318, 1564.599, 1605.678, 1634.868, 1656.993, 1671.502, 1685.333, 1695.582, 1705.220, 1710.433 (1698.175)	KLD: 5.126, 3.804, 3.121, 2.591, 2.615, 2.488, 2.189, 2.065, 1.583, 1.302 (26.853)	Grad: 0.190, 0.072, 0.062, 0.055, 0.058, 0.063, 0.066, 0.068, 0.064, 0.063
[Epoch  46 (34.26s)]	ELBO: 1496.944, 1562.955, 1604.688, 1633.924, 1656.101, 1670.630, 1684.366, 1694.517, 1704.193, 1709.867 (1671.997)	Log prob: 1502.099, 1566.766, 1607.793, 1636.544, 1658.708, 1673.122, 1686.529, 1696.571, 1705.784, 1711.185 (1698.693)	KLD: 5.156, 3.812, 3.105, 2.621, 2.607, 2.492, 2.163, 2.054, 1.591, 1.319 (26.696)	Grad: 0.196, 0.072, 0.063, 0.054, 0.057, 0.063, 0.067, 0.068, 0.063, 0.062
[Epoch  47 (34.90s)]	ELBO: 1494.215, 1561.764, 1602.846, 1631.543, 1654.607, 1669.176, 1682.693, 1693.134, 1702.500, 1708.160 (1675.851)	Log prob: 1499.396, 1565.580, 1605.894, 1634.184, 1657.224, 1671.654, 1684.847, 1695.165, 1704.084, 1709.466 (1702.046)	KLD: 5.181, 3.816, 3.050, 2.640, 2.617, 2.479, 2.153, 2.030, 1.584, 1.306 (26.195)	Grad: 0.206, 0.073, 0.061, 0.053, 0.057, 0.061, 0.065, 0.066, 0.062, 0.060
[Epoch  48 (32.67s)]	ELBO: 1495.510, 1562.997, 1603.638, 1632.533, 1655.439, 1670.194, 1683.768, 1694.110, 1703.667, 1709.517 (1664.236)	Log prob: 1500.712, 1566.819, 1606.738, 1635.192, 1658.046, 1672.667, 1685.907, 1696.138, 1705.258, 1710.828 (1690.530)	KLD: 5.202, 3.821, 3.102, 2.660, 2.606, 2.473, 2.139, 2.026, 1.591, 1.311 (26.294)	Grad: 0.199, 0.073, 0.062, 0.054, 0.056, 0.062, 0.064, 0.065, 0.062, 0.059
[Epoch  49 (32.78s)]	ELBO: 1494.146, 1562.078, 1602.185, 1630.390, 1653.698, 1668.635, 1682.312, 1692.906, 1702.310, 1708.005 (1670.346)	Log prob: 1499.385, 1565.877, 1605.276, 1633.076, 1656.302, 1671.123, 1684.449, 1694.907, 1703.901, 1709.318 (1697.728)	KLD: 5.239, 3.800, 3.092, 2.687, 2.605, 2.488, 2.136, 2.001, 1.591, 1.314 (27.382)	Grad: 0.206, 0.072, 0.063, 0.056, 0.058, 0.065, 0.066, 0.066, 0.063, 0.061
[Epoch  50 (34.23s)]	ELBO: 1498.650, 1565.062, 1606.027, 1634.931, 1657.536, 1672.509, 1686.240, 1696.910, 1706.107, 1711.587 (1673.806)	Log prob: 1503.864, 1568.883, 1609.136, 1637.628, 1660.148, 1674.969, 1688.367, 1698.922, 1707.662, 1712.872 (1699.990)	KLD: 5.214, 3.820, 3.109, 2.697, 2.614, 2.458, 2.129, 2.013, 1.553, 1.284 (26.184)	Grad: 0.192, 0.076, 0.062, 0.053, 0.055, 0.060, 0.063, 0.063, 0.061, 0.059
[Epoch  51 (32.97s)]	ELBO: 1496.806, 1563.689, 1604.681, 1633.828, 1656.211, 1671.216, 1685.354, 1696.064, 1704.935, 1710.576 (1671.014)	Log prob: 1501.984, 1567.549, 1607.787, 1636.550, 1658.790, 1673.657, 1687.479, 1698.055, 1706.457, 1711.859 (1698.334)	KLD: 5.177, 3.860, 3.106, 2.721, 2.580, 2.440, 2.125, 1.990, 1.523, 1.283 (27.320)	Grad: 0.190, 0.078, 0.063, 0.054, 0.056, 0.063, 0.065, 0.065, 0.062, 0.059
[Epoch  52 (33.02s)]	ELBO: 1498.430, 1566.053, 1607.413, 1636.083, 1658.980, 1674.164, 1687.544, 1698.154, 1707.062, 1712.742 (1673.780)	Log prob: 1503.620, 1569.901, 1610.531, 1638.830, 1661.557, 1676.616, 1689.636, 1700.126, 1708.574, 1713.986 (1699.849)	KLD: 5.190, 3.848, 3.119, 2.747, 2.578, 2.453, 2.094, 1.972, 1.512, 1.243 (26.069)	Grad: 0.197, 0.075, 0.061, 0.052, 0.054, 0.059, 0.062, 0.063, 0.060, 0.057
[Epoch  53 (31.37s)]	ELBO: 1498.319, 1566.717, 1606.822, 1634.698, 1657.735, 1672.734, 1686.210, 1696.387, 1705.583, 1711.537 (1672.401)	Log prob: 1503.573, 1570.550, 1609.842, 1637.448, 1660.305, 1675.189, 1688.305, 1698.349, 1707.103, 1712.823 (1699.605)	KLD: 5.253, 3.831, 3.020, 2.750, 2.571, 2.455, 2.094, 1.962, 1.521, 1.285 (27.204)	Grad: 0.208, 0.075, 0.061, 0.053, 0.054, 0.060, 0.063, 0.064, 0.060, 0.057
[Epoch  54 (36.41s)]	ELBO: 1498.019, 1566.556, 1607.774, 1636.234, 1659.147, 1674.467, 1688.036, 1698.296, 1707.270, 1712.677 (1667.867)	Log prob: 1503.224, 1570.393, 1610.919, 1639.012, 1661.685, 1676.924, 1690.131, 1700.229, 1708.777, 1713.906 (1694.136)	KLD: 5.205, 3.837, 3.144, 2.779, 2.539, 2.457, 2.095, 1.933, 1.508, 1.228 (26.269)	Grad: 0.205, 0.076, 0.062, 0.053, 0.053, 0.058, 0.063, 0.062, 0.059, 0.056
[Epoch  55 (32.57s)]	ELBO: 1497.356, 1565.632, 1605.956, 1634.596, 1657.392, 1672.800, 1686.363, 1696.925, 1706.005, 1711.553 (1668.988)	Log prob: 1502.558, 1569.499, 1609.059, 1637.394, 1659.946, 1675.263, 1688.467, 1698.844, 1707.511, 1712.780 (1695.325)	KLD: 5.202, 3.866, 3.103, 2.798, 2.552, 2.462, 2.104, 1.918, 1.506, 1.226 (26.337)	Grad: 0.201, 0.078, 0.064, 0.054, 0.054, 0.059, 0.063, 0.063, 0.059, 0.056
No improvement after 25 epochs...
Best epoch(s): [31]	Training time(s): 3324.29s (3324.29s)	Best ELBO: 1712.742 (1676.320)	Best log prob: 1713.986 (1703.974)
Avg. mu: 0.676, -0.025, 0.558, 0.315, 0.358, 0.429, -0.157, -0.361, -0.211, -0.118
Avg. var: 0.000, 0.000, 0.002, 0.007, 0.002, 0.003, 0.009, 0.010, 0.026, 0.029
Max. mu: 6.768, 1.942, 2.823, 2.944, 2.012, 1.707, 1.630, 1.527, 3.129, 1.942
Max. var: 0.190, 0.008, 0.046, 0.098, 0.016, 0.022, 0.081, 0.068, 0.221, 0.205
Min. mu: -0.213, -2.207, -4.735, -2.571, -0.768, -0.902, -2.035, -1.932, -2.433, -1.800
Min. var: 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.001, 0.001, 0.003, 0.004
Cov. mu:
[[1.081 0.075 -0.071 0.128 0.020 0.048 0.047 -0.035 -0.030 -0.018]
 [0.075 0.299 0.030 -0.011 -0.008 0.023 0.029 -0.009 0.019 -0.026]
 [-0.071 0.030 0.478 -0.062 0.001 -0.003 0.019 0.004 0.032 -0.043]
 [0.128 -0.011 -0.062 0.679 -0.014 0.003 0.022 0.020 -0.008 0.011]
 [0.020 -0.008 0.001 -0.014 0.122 -0.001 0.015 0.001 -0.019 0.005]
 [0.048 0.023 -0.003 0.003 -0.001 0.116 0.007 -0.006 0.010 -0.015]
 [0.047 0.029 0.019 0.022 0.015 0.007 0.225 0.009 -0.033 0.017]
 [-0.035 -0.009 0.004 0.020 0.001 -0.006 0.009 0.204 -0.002 -0.006]
 [-0.030 0.019 0.032 -0.008 -0.019 0.010 -0.033 -0.002 0.468 0.002]
 [-0.018 -0.026 -0.043 0.011 0.005 -0.015 0.017 -0.006 0.002 0.267]]
Avg. mu: 0.676, -0.025, 0.558, 0.315, 0.358, 0.429, -0.157, -0.361, -0.211, -0.118
Avg. var: 0.000, 0.000, 0.002, 0.007, 0.002, 0.003, 0.009, 0.010, 0.026, 0.029
Max. mu: 6.768, 1.942, 2.823, 2.944, 2.012, 1.707, 1.630, 1.527, 3.129, 1.942
Max. var: 0.190, 0.008, 0.046, 0.098, 0.016, 0.022, 0.081, 0.068, 0.221, 0.205
Min. mu: -0.213, -2.207, -4.735, -2.571, -0.768, -0.902, -2.035, -1.932, -2.433, -1.800
Min. var: 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.001, 0.001, 0.003, 0.004
Cov. mu:
[[1.081 0.075 -0.071 0.128 0.020 0.048 0.047 -0.035 -0.030 -0.018]
 [0.075 0.299 0.030 -0.011 -0.008 0.023 0.029 -0.009 0.019 -0.026]
 [-0.071 0.030 0.478 -0.062 0.001 -0.003 0.019 0.004 0.032 -0.043]
 [0.128 -0.011 -0.062 0.679 -0.014 0.003 0.022 0.020 -0.008 0.011]
 [0.020 -0.008 0.001 -0.014 0.122 -0.001 0.015 0.001 -0.019 0.005]
 [0.048 0.023 -0.003 0.003 -0.001 0.116 0.007 -0.006 0.010 -0.015]
 [0.047 0.029 0.019 0.022 0.015 0.007 0.225 0.009 -0.033 0.017]
 [-0.035 -0.009 0.004 0.020 0.001 -0.006 0.009 0.204 -0.002 -0.006]
 [-0.030 0.019 0.032 -0.008 -0.019 0.010 -0.033 -0.002 0.468 0.002]
 [-0.018 -0.026 -0.043 0.011 0.005 -0.015 0.017 -0.006 0.002 0.267]]
