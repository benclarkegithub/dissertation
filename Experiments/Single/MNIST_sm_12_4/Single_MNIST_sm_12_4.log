Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            76,930
├─Linear: 1-2                            4,851
=================================================================
Total params: 81,781
Trainable params: 81,781
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            200
├─Linear: 1-2                            200
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            200
├─Linear: 1-2                            200
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            200
├─Linear: 1-2                            200
=================================================================
Total params: 400
Trainable params: 400
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            245
=================================================================
Total params: 245
Trainable params: 245
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            245
=================================================================
Total params: 245
Trainable params: 245
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            245
=================================================================
Total params: 245
Trainable params: 245
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            4,900
├─Linear: 1-2                            77,616
=================================================================
Total params: 82,516
Trainable params: 82,516
Non-trainable params: 0
=================================================================
[Epoch   1 (23.54s)]	ELBO: 1354.258, 1390.444, 1395.578 (1518.735)	Log prob: 1377.788, 1393.160, 1397.388 (1542.093)	KLD: 23.530, 2.715, 1.810 (23.358)	Grad: 0.082, 0.047, 0.050
[Epoch   2 (22.81s)]	ELBO: 1548.555, 1602.629, 1608.820 (1632.763)	Log prob: 1565.265, 1607.662, 1610.566 (1656.627)	KLD: 16.710, 5.032, 1.746 (23.863)	Grad: 0.095, 0.052, 0.036
[Epoch   3 (23.95s)]	ELBO: 1584.283, 1654.599, 1667.507 (1663.214)	Log prob: 1598.160, 1661.266, 1670.328 (1686.584)	KLD: 13.876, 6.667, 2.822 (23.369)	Grad: 0.099, 0.061, 0.037
[Epoch   4 (23.48s)]	ELBO: 1599.334, 1675.704, 1693.159 (1688.018)	Log prob: 1612.189, 1682.887, 1696.734 (1711.782)	KLD: 12.856, 7.183, 3.573 (23.764)	Grad: 0.101, 0.058, 0.038
[Epoch   5 (24.26s)]	ELBO: 1609.277, 1690.122, 1713.193 (1699.582)	Log prob: 1621.793, 1697.690, 1717.465 (1723.761)	KLD: 12.516, 7.568, 4.271 (24.179)	Grad: 0.101, 0.061, 0.041
[Epoch   6 (24.49s)]	ELBO: 1617.778, 1699.316, 1725.232 (1709.298)	Log prob: 1630.291, 1707.084, 1729.922 (1734.643)	KLD: 12.515, 7.769, 4.688 (25.344)	Grad: 0.099, 0.063, 0.042
[Epoch   7 (24.37s)]	ELBO: 1625.082, 1706.313, 1733.066 (1714.992)	Log prob: 1637.707, 1714.198, 1738.015 (1740.519)	KLD: 12.625, 7.886, 4.949 (25.527)	Grad: 0.100, 0.064, 0.044
[Epoch   8 (24.45s)]	ELBO: 1630.737, 1711.426, 1738.919 (1715.773)	Log prob: 1643.513, 1719.373, 1744.027 (1741.192)	KLD: 12.776, 7.948, 5.108 (25.419)	Grad: 0.104, 0.064, 0.044
[Epoch   9 (24.16s)]	ELBO: 1635.385, 1715.339, 1743.433 (1724.027)	Log prob: 1648.301, 1723.326, 1748.737 (1750.056)	KLD: 12.917, 7.988, 5.303 (26.029)	Grad: 0.099, 0.063, 0.045
[Epoch  10 (24.51s)]	ELBO: 1638.655, 1717.908, 1746.584 (1725.222)	Log prob: 1651.630, 1725.913, 1751.963 (1751.590)	KLD: 12.975, 8.004, 5.380 (26.368)	Grad: 0.103, 0.062, 0.045
[Epoch  11 (24.92s)]	ELBO: 1641.528, 1720.229, 1749.213 (1729.468)	Log prob: 1654.593, 1728.195, 1754.611 (1755.750)	KLD: 13.065, 7.968, 5.397 (26.283)	Grad: 0.101, 0.062, 0.045
[Epoch  12 (25.17s)]	ELBO: 1643.790, 1722.105, 1751.297 (1728.117)	Log prob: 1656.932, 1730.096, 1756.725 (1754.789)	KLD: 13.142, 7.992, 5.428 (26.673)	Grad: 0.103, 0.062, 0.045
[Epoch  13 (27.24s)]	ELBO: 1645.640, 1723.505, 1752.694 (1730.772)	Log prob: 1658.824, 1731.458, 1758.140 (1756.889)	KLD: 13.184, 7.952, 5.447 (26.117)	Grad: 0.103, 0.061, 0.046
[Epoch  14 (25.61s)]	ELBO: 1647.383, 1724.694, 1753.957 (1732.375)	Log prob: 1660.589, 1732.614, 1759.401 (1758.806)	KLD: 13.205, 7.920, 5.443 (26.432)	Grad: 0.104, 0.060, 0.045
[Epoch  15 (25.42s)]	ELBO: 1648.749, 1725.922, 1755.282 (1731.042)	Log prob: 1662.033, 1733.833, 1760.718 (1758.064)	KLD: 13.284, 7.912, 5.435 (27.021)	Grad: 0.104, 0.060, 0.046
[Epoch  16 (25.26s)]	ELBO: 1650.181, 1726.900, 1756.148 (1734.447)	Log prob: 1663.484, 1734.775, 1761.575 (1760.966)	KLD: 13.304, 7.875, 5.427 (26.519)	Grad: 0.107, 0.060, 0.045
[Epoch  17 (24.89s)]	ELBO: 1651.515, 1727.999, 1757.224 (1733.875)	Log prob: 1664.864, 1735.861, 1762.653 (1761.222)	KLD: 13.350, 7.861, 5.428 (27.347)	Grad: 0.107, 0.060, 0.045
[Epoch  18 (25.23s)]	ELBO: 1652.681, 1729.003, 1758.261 (1734.288)	Log prob: 1666.054, 1736.875, 1763.688 (1760.554)	KLD: 13.373, 7.873, 5.426 (26.266)	Grad: 0.108, 0.059, 0.045
[Epoch  19 (25.49s)]	ELBO: 1653.871, 1729.878, 1759.146 (1737.372)	Log prob: 1667.282, 1737.716, 1764.554 (1764.138)	KLD: 13.412, 7.839, 5.407 (26.766)	Grad: 0.106, 0.059, 0.045
[Epoch  20 (28.51s)]	ELBO: 1654.698, 1730.647, 1759.978 (1737.562)	Log prob: 1668.168, 1738.489, 1765.355 (1764.827)	KLD: 13.471, 7.842, 5.378 (27.264)	Grad: 0.107, 0.060, 0.045
[Epoch  21 (28.41s)]	ELBO: 1655.611, 1731.402, 1760.589 (1738.466)	Log prob: 1669.116, 1739.208, 1765.949 (1765.410)	KLD: 13.505, 7.807, 5.361 (26.944)	Grad: 0.108, 0.060, 0.044
[Epoch  22 (26.54s)]	ELBO: 1656.436, 1732.147, 1761.376 (1737.367)	Log prob: 1669.963, 1739.956, 1766.711 (1763.935)	KLD: 13.526, 7.809, 5.333 (26.568)	Grad: 0.109, 0.059, 0.045
[Epoch  23 (26.58s)]	ELBO: 1657.308, 1732.891, 1761.954 (1736.701)	Log prob: 1670.817, 1740.700, 1767.301 (1763.535)	KLD: 13.509, 7.808, 5.347 (26.834)	Grad: 0.110, 0.059, 0.044
[Epoch  24 (25.90s)]	ELBO: 1658.130, 1733.383, 1762.515 (1731.842)	Log prob: 1671.703, 1741.202, 1767.839 (1758.996)	KLD: 13.573, 7.819, 5.325 (27.155)	Grad: 0.110, 0.058, 0.045
[Epoch  25 (25.88s)]	ELBO: 1658.771, 1733.712, 1762.907 (1737.815)	Log prob: 1672.344, 1741.514, 1768.224 (1763.927)	KLD: 13.572, 7.801, 5.317 (26.112)	Grad: 0.111, 0.058, 0.045
[Epoch  26 (25.34s)]	ELBO: 1659.421, 1734.381, 1763.471 (1738.323)	Log prob: 1673.023, 1742.147, 1768.772 (1764.711)	KLD: 13.601, 7.766, 5.301 (26.388)	Grad: 0.113, 0.057, 0.044
[Epoch  27 (24.93s)]	ELBO: 1660.153, 1734.973, 1763.978 (1740.099)	Log prob: 1673.781, 1742.757, 1769.263 (1766.225)	KLD: 13.630, 7.783, 5.285 (26.126)	Grad: 0.113, 0.057, 0.044
[Epoch  28 (25.73s)]	ELBO: 1660.889, 1735.390, 1764.586 (1739.076)	Log prob: 1674.531, 1743.153, 1769.834 (1765.152)	KLD: 13.642, 7.763, 5.247 (26.076)	Grad: 0.111, 0.057, 0.044
[Epoch  29 (25.42s)]	ELBO: 1661.275, 1735.855, 1764.838 (1738.946)	Log prob: 1674.954, 1743.638, 1770.108 (1765.881)	KLD: 13.678, 7.783, 5.271 (26.936)	Grad: 0.115, 0.056, 0.044
[Epoch  30 (25.83s)]	ELBO: 1661.993, 1736.141, 1765.017 (1740.149)	Log prob: 1675.661, 1743.891, 1770.281 (1766.847)	KLD: 13.668, 7.749, 5.265 (26.698)	Grad: 0.111, 0.057, 0.045
[Epoch  31 (25.09s)]	ELBO: 1662.430, 1736.471, 1765.480 (1740.389)	Log prob: 1676.099, 1744.189, 1770.732 (1766.865)	KLD: 13.670, 7.718, 5.251 (26.476)	Grad: 0.112, 0.056, 0.044
[Epoch  32 (24.79s)]	ELBO: 1662.652, 1736.830, 1765.756 (1740.441)	Log prob: 1676.352, 1744.560, 1770.981 (1767.204)	KLD: 13.701, 7.729, 5.226 (26.763)	Grad: 0.116, 0.056, 0.044
[Epoch  33 (25.29s)]	ELBO: 1663.127, 1737.094, 1765.900 (1742.307)	Log prob: 1676.859, 1744.793, 1771.161 (1768.817)	KLD: 13.733, 7.699, 5.260 (26.511)	Grad: 0.115, 0.056, 0.045
[Epoch  34 (25.03s)]	ELBO: 1663.783, 1737.441, 1766.217 (1740.621)	Log prob: 1677.484, 1745.123, 1771.452 (1767.339)	KLD: 13.701, 7.681, 5.237 (26.718)	Grad: 0.113, 0.056, 0.045
[Epoch  35 (25.44s)]	ELBO: 1664.147, 1737.664, 1766.466 (1740.672)	Log prob: 1677.880, 1745.372, 1771.689 (1767.170)	KLD: 13.734, 7.709, 5.224 (26.498)	Grad: 0.116, 0.056, 0.045
[Epoch  36 (25.15s)]	ELBO: 1664.376, 1737.768, 1766.564 (1738.037)	Log prob: 1678.084, 1745.433, 1771.778 (1763.108)	KLD: 13.708, 7.665, 5.214 (25.071)	Grad: 0.114, 0.056, 0.045
[Epoch  37 (25.39s)]	ELBO: 1664.773, 1738.053, 1766.826 (1741.248)	Log prob: 1678.489, 1745.695, 1772.036 (1767.877)	KLD: 13.717, 7.642, 5.209 (26.629)	Grad: 0.115, 0.056, 0.046
[Epoch  38 (25.65s)]	ELBO: 1665.148, 1738.309, 1766.904 (1740.115)	Log prob: 1678.892, 1745.967, 1772.100 (1766.242)	KLD: 13.743, 7.658, 5.195 (26.127)	Grad: 0.119, 0.057, 0.045
[Epoch  39 (24.94s)]	ELBO: 1665.559, 1738.569, 1767.281 (1738.782)	Log prob: 1679.321, 1746.208, 1772.494 (1765.538)	KLD: 13.762, 7.639, 5.213 (26.756)	Grad: 0.116, 0.056, 0.045
[Epoch  40 (24.70s)]	ELBO: 1665.849, 1738.769, 1767.234 (1737.104)	Log prob: 1679.591, 1746.403, 1772.417 (1763.441)	KLD: 13.743, 7.635, 5.183 (26.338)	Grad: 0.119, 0.056, 0.045
[Epoch  41 (24.91s)]	ELBO: 1666.367, 1739.165, 1767.696 (1740.183)	Log prob: 1680.108, 1746.786, 1772.874 (1767.418)	KLD: 13.741, 7.622, 5.178 (27.235)	Grad: 0.118, 0.056, 0.046
[Epoch  42 (26.14s)]	ELBO: 1666.661, 1739.557, 1767.928 (1741.204)	Log prob: 1680.409, 1747.172, 1773.101 (1767.416)	KLD: 13.748, 7.616, 5.173 (26.212)	Grad: 0.119, 0.056, 0.045
[Epoch  43 (27.26s)]	ELBO: 1667.007, 1739.770, 1768.142 (1743.122)	Log prob: 1680.773, 1747.386, 1773.311 (1769.020)	KLD: 13.764, 7.616, 5.168 (25.898)	Grad: 0.119, 0.056, 0.045
[Epoch  44 (24.91s)]	ELBO: 1667.053, 1739.845, 1768.061 (1742.261)	Log prob: 1680.801, 1747.440, 1773.220 (1768.871)	KLD: 13.749, 7.594, 5.159 (26.610)	Grad: 0.121, 0.057, 0.046
[Epoch  45 (24.24s)]	ELBO: 1667.433, 1740.013, 1768.224 (1739.051)	Log prob: 1681.156, 1747.590, 1773.399 (1764.877)	KLD: 13.724, 7.579, 5.176 (25.826)	Grad: 0.122, 0.056, 0.046
[Epoch  46 (24.37s)]	ELBO: 1667.777, 1740.283, 1768.397 (1742.207)	Log prob: 1681.512, 1747.859, 1773.554 (1768.232)	KLD: 13.734, 7.576, 5.157 (26.025)	Grad: 0.123, 0.057, 0.046
[Epoch  47 (24.16s)]	ELBO: 1668.215, 1740.591, 1768.743 (1741.591)	Log prob: 1681.956, 1748.151, 1773.907 (1767.225)	KLD: 13.741, 7.561, 5.164 (25.634)	Grad: 0.121, 0.056, 0.046
[Epoch  48 (24.10s)]	ELBO: 1668.322, 1740.670, 1768.732 (1743.065)	Log prob: 1682.080, 1748.199, 1773.887 (1769.658)	KLD: 13.758, 7.531, 5.155 (26.593)	Grad: 0.124, 0.057, 0.047
[Epoch  49 (25.10s)]	ELBO: 1668.593, 1740.748, 1768.755 (1742.366)	Log prob: 1682.320, 1748.291, 1773.893 (1768.241)	KLD: 13.726, 7.543, 5.137 (25.876)	Grad: 0.125, 0.057, 0.047
[Epoch  50 (27.77s)]	ELBO: 1669.174, 1741.238, 1769.141 (1739.565)	Log prob: 1682.916, 1748.751, 1774.281 (1766.098)	KLD: 13.742, 7.513, 5.139 (26.533)	Grad: 0.122, 0.056, 0.047
[Epoch  51 (28.17s)]	ELBO: 1669.512, 1741.374, 1769.111 (1742.955)	Log prob: 1683.236, 1748.880, 1774.242 (1769.309)	KLD: 13.726, 7.507, 5.132 (26.354)	Grad: 0.122, 0.057, 0.047
[Epoch  52 (25.82s)]	ELBO: 1669.737, 1741.615, 1769.453 (1742.357)	Log prob: 1683.489, 1749.113, 1774.574 (1768.478)	KLD: 13.752, 7.498, 5.122 (26.121)	Grad: 0.123, 0.057, 0.047
[Epoch  53 (25.23s)]	ELBO: 1670.010, 1741.723, 1769.600 (1741.894)	Log prob: 1683.729, 1749.219, 1774.712 (1767.423)	KLD: 13.720, 7.497, 5.113 (25.529)	Grad: 0.125, 0.057, 0.048
[Epoch  54 (24.74s)]	ELBO: 1670.230, 1741.940, 1769.670 (1742.956)	Log prob: 1683.955, 1749.406, 1774.812 (1769.095)	KLD: 13.724, 7.466, 5.140 (26.139)	Grad: 0.125, 0.057, 0.048
[Epoch  55 (24.45s)]	ELBO: 1670.608, 1742.344, 1770.037 (1743.412)	Log prob: 1684.351, 1749.820, 1775.162 (1769.075)	KLD: 13.744, 7.477, 5.125 (25.662)	Grad: 0.122, 0.057, 0.047
[Epoch  56 (25.08s)]	ELBO: 1670.969, 1742.398, 1770.034 (1742.761)	Log prob: 1684.714, 1749.859, 1775.173 (1769.247)	KLD: 13.744, 7.462, 5.139 (26.486)	Grad: 0.123, 0.057, 0.048
[Epoch  57 (24.52s)]	ELBO: 1670.918, 1742.346, 1769.932 (1744.260)	Log prob: 1684.651, 1749.808, 1775.056 (1770.472)	KLD: 13.733, 7.463, 5.125 (26.212)	Grad: 0.126, 0.058, 0.049
[Epoch  58 (25.64s)]	ELBO: 1671.353, 1742.850, 1770.470 (1744.452)	Log prob: 1685.091, 1750.318, 1775.601 (1771.265)	KLD: 13.739, 7.469, 5.131 (26.813)	Grad: 0.123, 0.057, 0.049
[Epoch  59 (25.03s)]	ELBO: 1671.450, 1742.953, 1770.598 (1743.402)	Log prob: 1685.172, 1750.406, 1775.724 (1769.773)	KLD: 13.721, 7.453, 5.125 (26.371)	Grad: 0.125, 0.057, 0.049
[Epoch  60 (24.95s)]	ELBO: 1671.912, 1743.236, 1770.750 (1743.895)	Log prob: 1685.636, 1750.714, 1775.880 (1770.033)	KLD: 13.725, 7.478, 5.129 (26.138)	Grad: 0.125, 0.058, 0.049
[Epoch  61 (24.97s)]	ELBO: 1671.479, 1743.015, 1770.531 (1744.719)	Log prob: 1685.241, 1750.460, 1775.683 (1770.725)	KLD: 13.762, 7.447, 5.152 (26.006)	Grad: 0.125, 0.058, 0.050
[Epoch  62 (24.95s)]	ELBO: 1672.184, 1743.549, 1771.059 (1743.753)	Log prob: 1685.946, 1750.984, 1776.183 (1769.886)	KLD: 13.762, 7.435, 5.124 (26.133)	Grad: 0.125, 0.058, 0.049
[Epoch  63 (25.31s)]	ELBO: 1671.534, 1743.116, 1770.710 (1743.077)	Log prob: 1685.279, 1750.545, 1775.842 (1769.128)	KLD: 13.745, 7.429, 5.132 (26.052)	Grad: 0.127, 0.058, 0.051
[Epoch  64 (25.01s)]	ELBO: 1672.578, 1743.878, 1771.340 (1745.564)	Log prob: 1686.322, 1751.309, 1776.463 (1772.059)	KLD: 13.743, 7.432, 5.122 (26.494)	Grad: 0.124, 0.058, 0.050
[Epoch  65 (24.55s)]	ELBO: 1672.040, 1743.572, 1771.131 (1744.575)	Log prob: 1685.775, 1751.001, 1776.252 (1770.780)	KLD: 13.735, 7.427, 5.121 (26.204)	Grad: 0.130, 0.058, 0.051
[Epoch  66 (24.88s)]	ELBO: 1672.879, 1744.052, 1771.495 (1741.286)	Log prob: 1686.607, 1751.494, 1776.642 (1768.308)	KLD: 13.728, 7.443, 5.147 (27.022)	Grad: 0.124, 0.058, 0.050
[Epoch  67 (25.94s)]	ELBO: 1673.088, 1744.212, 1771.537 (1744.815)	Log prob: 1686.806, 1751.644, 1776.648 (1771.229)	KLD: 13.719, 7.431, 5.111 (26.414)	Grad: 0.126, 0.059, 0.050
[Epoch  68 (25.02s)]	ELBO: 1673.170, 1744.207, 1771.563 (1745.746)	Log prob: 1686.895, 1751.635, 1776.696 (1771.873)	KLD: 13.726, 7.427, 5.134 (26.127)	Grad: 0.129, 0.060, 0.052
[Epoch  69 (24.75s)]	ELBO: 1673.266, 1744.390, 1771.830 (1743.998)	Log prob: 1686.997, 1751.834, 1776.950 (1770.722)	KLD: 13.731, 7.445, 5.120 (26.725)	Grad: 0.127, 0.059, 0.051
[Epoch  70 (25.13s)]	ELBO: 1673.624, 1744.828, 1772.024 (1744.963)	Log prob: 1687.383, 1752.240, 1777.136 (1771.111)	KLD: 13.758, 7.410, 5.113 (26.148)	Grad: 0.127, 0.060, 0.051
[Epoch  71 (24.49s)]	ELBO: 1673.629, 1744.730, 1771.968 (1745.483)	Log prob: 1687.370, 1752.153, 1777.076 (1771.173)	KLD: 13.741, 7.422, 5.109 (25.690)	Grad: 0.128, 0.060, 0.052
[Epoch  72 (26.89s)]	ELBO: 1673.839, 1744.918, 1772.094 (1745.168)	Log prob: 1687.573, 1752.321, 1777.200 (1771.459)	KLD: 13.734, 7.404, 5.108 (26.291)	Grad: 0.128, 0.060, 0.052
[Epoch  73 (25.24s)]	ELBO: 1674.053, 1745.102, 1772.170 (1745.518)	Log prob: 1687.766, 1752.499, 1777.290 (1771.365)	KLD: 13.712, 7.398, 5.118 (25.847)	Grad: 0.129, 0.059, 0.052
[Epoch  74 (24.84s)]	ELBO: 1673.852, 1744.834, 1772.091 (1745.141)	Log prob: 1687.577, 1752.250, 1777.199 (1770.637)	KLD: 13.724, 7.417, 5.108 (25.497)	Grad: 0.130, 0.061, 0.052
[Epoch  75 (24.18s)]	ELBO: 1673.965, 1745.110, 1772.161 (1746.149)	Log prob: 1687.693, 1752.524, 1777.291 (1772.331)	KLD: 13.729, 7.414, 5.131 (26.182)	Grad: 0.130, 0.060, 0.053
[Epoch  76 (24.40s)]	ELBO: 1674.301, 1745.263, 1772.272 (1744.409)	Log prob: 1687.997, 1752.660, 1777.397 (1770.176)	KLD: 13.697, 7.397, 5.125 (25.768)	Grad: 0.132, 0.061, 0.053
[Epoch  77 (24.41s)]	ELBO: 1674.443, 1745.151, 1772.220 (1743.018)	Log prob: 1688.150, 1752.555, 1777.337 (1768.781)	KLD: 13.707, 7.404, 5.116 (25.763)	Grad: 0.129, 0.062, 0.053
[Epoch  78 (24.93s)]	ELBO: 1674.237, 1745.134, 1772.231 (1744.736)	Log prob: 1687.950, 1752.520, 1777.368 (1770.691)	KLD: 13.714, 7.386, 5.137 (25.955)	Grad: 0.133, 0.061, 0.053
[Epoch  79 (27.69s)]	ELBO: 1674.472, 1745.369, 1772.469 (1742.782)	Log prob: 1688.196, 1752.759, 1777.570 (1768.711)	KLD: 13.724, 7.391, 5.102 (25.929)	Grad: 0.131, 0.062, 0.054
[Epoch  80 (26.84s)]	ELBO: 1674.740, 1745.449, 1772.465 (1746.019)	Log prob: 1688.458, 1752.857, 1777.569 (1772.156)	KLD: 13.717, 7.408, 5.106 (26.137)	Grad: 0.130, 0.061, 0.053
[Epoch  81 (19.94s)]	ELBO: 1674.385, 1745.417, 1772.365 (1745.105)	Log prob: 1688.125, 1752.839, 1777.489 (1771.078)	KLD: 13.740, 7.421, 5.124 (25.973)	Grad: 0.134, 0.063, 0.054
[Epoch  82 (19.73s)]	ELBO: 1674.373, 1745.378, 1772.349 (1746.224)	Log prob: 1688.112, 1752.783, 1777.458 (1771.852)	KLD: 13.738, 7.405, 5.111 (25.627)	Grad: 0.139, 0.063, 0.055
[Epoch  83 (20.30s)]	ELBO: 1674.509, 1745.216, 1772.206 (1742.426)	Log prob: 1688.230, 1752.624, 1777.341 (1768.780)	KLD: 13.720, 7.407, 5.136 (26.355)	Grad: 0.133, 0.063, 0.055
[Epoch  84 (19.73s)]	ELBO: 1674.783, 1745.547, 1772.308 (1746.075)	Log prob: 1688.518, 1752.929, 1777.425 (1772.299)	KLD: 13.735, 7.382, 5.116 (26.224)	Grad: 0.135, 0.063, 0.055
[Epoch  85 (19.60s)]	ELBO: 1674.959, 1745.624, 1772.496 (1742.142)	Log prob: 1688.690, 1753.035, 1777.621 (1767.781)	KLD: 13.731, 7.411, 5.125 (25.640)	Grad: 0.134, 0.063, 0.055
[Epoch  86 (19.53s)]	ELBO: 1674.904, 1745.690, 1772.543 (1747.492)	Log prob: 1688.626, 1753.073, 1777.645 (1774.181)	KLD: 13.724, 7.384, 5.101 (26.689)	Grad: 0.135, 0.063, 0.055
[Epoch  87 (19.23s)]	ELBO: 1675.212, 1745.910, 1772.737 (1741.402)	Log prob: 1688.966, 1753.287, 1777.837 (1767.497)	KLD: 13.754, 7.376, 5.100 (26.095)	Grad: 0.134, 0.063, 0.055
[Epoch  88 (19.07s)]	ELBO: 1675.375, 1746.039, 1772.852 (1743.617)	Log prob: 1689.107, 1753.450, 1777.953 (1769.848)	KLD: 13.732, 7.411, 5.101 (26.231)	Grad: 0.137, 0.063, 0.055
[Epoch  89 (19.62s)]	ELBO: 1675.456, 1746.318, 1773.084 (1746.308)	Log prob: 1689.228, 1753.728, 1778.197 (1772.692)	KLD: 13.771, 7.408, 5.113 (26.384)	Grad: 0.137, 0.062, 0.055
[Epoch  90 (19.14s)]	ELBO: 1675.599, 1746.367, 1773.182 (1745.882)	Log prob: 1689.348, 1753.754, 1778.282 (1772.430)	KLD: 13.749, 7.387, 5.099 (26.547)	Grad: 0.133, 0.063, 0.055
[Epoch  91 (19.38s)]	ELBO: 1675.732, 1746.532, 1773.228 (1746.027)	Log prob: 1689.493, 1753.916, 1778.323 (1771.838)	KLD: 13.761, 7.382, 5.096 (25.811)	Grad: 0.137, 0.063, 0.055
[Epoch  92 (19.35s)]	ELBO: 1675.703, 1746.312, 1773.116 (1745.298)	Log prob: 1689.445, 1753.692, 1778.215 (1771.409)	KLD: 13.742, 7.381, 5.099 (26.111)	Grad: 0.140, 0.065, 0.057
[Epoch  93 (19.24s)]	ELBO: 1675.678, 1746.240, 1773.155 (1744.425)	Log prob: 1689.447, 1753.652, 1778.276 (1770.838)	KLD: 13.768, 7.412, 5.121 (26.413)	Grad: 0.136, 0.065, 0.056
[Epoch  94 (19.45s)]	ELBO: 1675.917, 1746.572, 1773.109 (1746.485)	Log prob: 1689.668, 1753.939, 1778.219 (1772.208)	KLD: 13.752, 7.368, 5.111 (25.723)	Grad: 0.141, 0.064, 0.057
[Epoch  95 (19.82s)]	ELBO: 1676.261, 1746.879, 1773.447 (1746.411)	Log prob: 1690.023, 1754.270, 1778.563 (1772.681)	KLD: 13.762, 7.390, 5.115 (26.271)	Grad: 0.137, 0.063, 0.056
[Epoch  96 (19.61s)]	ELBO: 1675.912, 1746.635, 1773.297 (1744.999)	Log prob: 1689.665, 1754.029, 1778.418 (1771.241)	KLD: 13.752, 7.394, 5.122 (26.241)	Grad: 0.141, 0.065, 0.056
[Epoch  97 (20.04s)]	ELBO: 1676.044, 1746.794, 1773.395 (1744.258)	Log prob: 1689.797, 1754.196, 1778.506 (1770.038)	KLD: 13.753, 7.403, 5.112 (25.780)	Grad: 0.142, 0.064, 0.057
[Epoch  98 (19.89s)]	ELBO: 1676.382, 1747.018, 1773.717 (1743.512)	Log prob: 1690.150, 1754.410, 1778.829 (1769.678)	KLD: 13.768, 7.390, 5.111 (26.166)	Grad: 0.136, 0.064, 0.057
[Epoch  99 (20.09s)]	ELBO: 1676.808, 1747.296, 1773.909 (1744.611)	Log prob: 1690.570, 1754.682, 1779.024 (1770.488)	KLD: 13.762, 7.386, 5.115 (25.876)	Grad: 0.138, 0.064, 0.057
[Epoch 100 (19.31s)]	ELBO: 1676.126, 1746.821, 1773.393 (1744.958)	Log prob: 1689.902, 1754.204, 1778.504 (1771.044)	KLD: 13.777, 7.383, 5.111 (26.086)	Grad: 0.141, 0.065, 0.058
[Epoch 101 (19.96s)]	ELBO: 1676.598, 1747.287, 1773.876 (1745.174)	Log prob: 1690.377, 1754.685, 1778.972 (1771.283)	KLD: 13.780, 7.396, 5.096 (26.109)	Grad: 0.141, 0.065, 0.057
[Epoch 102 (19.67s)]	ELBO: 1676.531, 1747.160, 1773.756 (1742.937)	Log prob: 1690.296, 1754.533, 1778.866 (1768.931)	KLD: 13.766, 7.374, 5.111 (25.995)	Grad: 0.144, 0.065, 0.057
[Epoch 103 (19.65s)]	ELBO: 1676.179, 1746.899, 1773.347 (1744.319)	Log prob: 1689.942, 1754.275, 1778.459 (1770.308)	KLD: 13.763, 7.378, 5.113 (25.989)	Grad: 0.145, 0.065, 0.058
[Epoch 104 (19.79s)]	ELBO: 1676.823, 1747.332, 1773.867 (1744.755)	Log prob: 1690.606, 1754.730, 1778.972 (1770.414)	KLD: 13.782, 7.399, 5.105 (25.659)	Grad: 0.142, 0.065, 0.058
[Epoch 105 (22.37s)]	ELBO: 1676.893, 1747.317, 1773.878 (1744.274)	Log prob: 1690.669, 1754.713, 1778.993 (1770.003)	KLD: 13.775, 7.397, 5.115 (25.729)	Grad: 0.143, 0.065, 0.058
[Epoch 106 (18.82s)]	ELBO: 1676.900, 1747.334, 1773.867 (1744.203)	Log prob: 1690.667, 1754.743, 1778.986 (1770.167)	KLD: 13.768, 7.411, 5.120 (25.963)	Grad: 0.138, 0.064, 0.057
[Epoch 107 (18.56s)]	ELBO: 1676.529, 1747.215, 1773.757 (1741.049)	Log prob: 1690.307, 1754.610, 1778.866 (1767.386)	KLD: 13.778, 7.394, 5.109 (26.337)	Grad: 0.148, 0.066, 0.058
[Epoch 108 (18.88s)]	ELBO: 1676.976, 1747.511, 1774.039 (1746.997)	Log prob: 1690.785, 1754.896, 1779.136 (1773.343)	KLD: 13.809, 7.385, 5.097 (26.347)	Grad: 0.144, 0.065, 0.058
[Epoch 109 (18.87s)]	ELBO: 1677.208, 1747.653, 1774.050 (1746.807)	Log prob: 1691.007, 1755.014, 1779.170 (1772.924)	KLD: 13.799, 7.361, 5.121 (26.117)	Grad: 0.147, 0.064, 0.058
[Epoch 110 (18.68s)]	ELBO: 1677.090, 1747.654, 1774.088 (1746.042)	Log prob: 1690.894, 1755.009, 1779.207 (1772.657)	KLD: 13.804, 7.355, 5.119 (26.614)	Grad: 0.145, 0.065, 0.058
No improvement after 25 epochs...
Best epoch(s): [86]	Training time(s): 2608.14s (2608.14s)	Best ELBO: 1774.088 (1747.492)	Best log prob: 1779.207 (1774.181)
Avg. mu: -0.039, 0.152, -0.151, -0.152, -0.056, 0.133, -0.026, -0.048, 0.016, -0.041, -0.029, 0.120
Avg. var: 0.001, 0.001, 0.001, 0.001, 0.020, 0.027, 0.029, 0.021, 0.070, 0.060, 0.080, 0.083
Max. mu: 3.147, 4.847, 1.755, 3.393, 3.964, 3.273, 4.054, 4.116, 4.280, 4.301, 4.089, 3.868
Max. var: 0.007, 0.012, 0.005, 0.012, 0.108, 0.168, 0.253, 0.109, 0.382, 0.239, 0.409, 0.326
Min. mu: -2.471, -2.497, -2.053, -3.069, -3.354, -4.440, -3.170, -3.941, -3.986, -4.280, -4.157, -3.487
Min. var: 0.000, 0.000, 0.000, 0.000, 0.004, 0.004, 0.005, 0.003, 0.017, 0.018, 0.027, 0.022
Cov. mu:
[[0.721 0.039 0.010 0.010 0.085 -0.066 0.078 0.040 0.016 -0.016 0.005
  -0.001]
 [0.039 0.758 0.051 0.078 -0.021 0.120 -0.083 0.052 0.002 -0.045 0.006
  -0.032]
 [0.010 0.051 0.550 0.130 0.056 -0.071 -0.015 -0.035 0.020 -0.030 -0.026
  0.020]
 [0.010 0.078 0.130 0.786 -0.026 0.030 -0.012 -0.074 -0.017 -0.014 0.002
  0.084]
 [0.085 -0.021 0.056 -0.026 0.783 -0.046 0.127 -0.010 -0.014 0.008 0.017
  -0.040]
 [-0.066 0.120 -0.071 0.030 -0.046 0.773 -0.039 0.064 0.011 0.001 0.022
  -0.015]
 [0.078 -0.083 -0.015 -0.012 0.127 -0.039 0.727 0.017 0.014 0.007 -0.002
  -0.022]
 [0.040 0.052 -0.035 -0.074 -0.010 0.064 0.017 0.813 -0.012 0.005 -0.034
  0.008]
 [0.016 0.002 0.020 -0.017 -0.014 0.011 0.014 -0.012 0.835 0.002 0.014
  -0.013]
 [-0.016 -0.045 -0.030 -0.014 0.008 0.001 0.007 0.005 0.002 0.802 -0.005
  -0.023]
 [0.005 0.006 -0.026 0.002 0.017 0.022 -0.002 -0.034 0.014 -0.005 0.815
  -0.000]
 [-0.001 -0.032 0.020 0.084 -0.040 -0.015 -0.022 0.008 -0.013 -0.023
  -0.000 0.733]]
Avg. mu: -0.039, 0.152, -0.151, -0.152, -0.056, 0.133, -0.026, -0.048, 0.016, -0.041, -0.029, 0.120
Avg. var: 0.001, 0.001, 0.001, 0.001, 0.020, 0.027, 0.029, 0.021, 0.070, 0.060, 0.080, 0.083
Max. mu: 3.147, 4.847, 1.755, 3.393, 3.964, 3.273, 4.054, 4.116, 4.280, 4.301, 4.089, 3.868
Max. var: 0.007, 0.012, 0.005, 0.012, 0.108, 0.168, 0.253, 0.109, 0.382, 0.239, 0.409, 0.326
Min. mu: -2.471, -2.497, -2.053, -3.069, -3.354, -4.440, -3.170, -3.941, -3.986, -4.280, -4.157, -3.487
Min. var: 0.000, 0.000, 0.000, 0.000, 0.004, 0.004, 0.005, 0.003, 0.017, 0.018, 0.027, 0.022
Cov. mu:
[[0.721 0.039 0.010 0.010 0.085 -0.066 0.078 0.040 0.016 -0.016 0.005
  -0.001]
 [0.039 0.758 0.051 0.078 -0.021 0.120 -0.083 0.052 0.002 -0.045 0.006
  -0.032]
 [0.010 0.051 0.550 0.130 0.056 -0.071 -0.015 -0.035 0.020 -0.030 -0.026
  0.020]
 [0.010 0.078 0.130 0.786 -0.026 0.030 -0.012 -0.074 -0.017 -0.014 0.002
  0.084]
 [0.085 -0.021 0.056 -0.026 0.783 -0.046 0.127 -0.010 -0.014 0.008 0.017
  -0.040]
 [-0.066 0.120 -0.071 0.030 -0.046 0.773 -0.039 0.064 0.011 0.001 0.022
  -0.015]
 [0.078 -0.083 -0.015 -0.012 0.127 -0.039 0.727 0.017 0.014 0.007 -0.002
  -0.022]
 [0.040 0.052 -0.035 -0.074 -0.010 0.064 0.017 0.813 -0.012 0.005 -0.034
  0.008]
 [0.016 0.002 0.020 -0.017 -0.014 0.011 0.014 -0.012 0.835 0.002 0.014
  -0.013]
 [-0.016 -0.045 -0.030 -0.014 0.008 0.001 0.007 0.005 0.002 0.802 -0.005
  -0.023]
 [0.005 0.006 -0.026 0.002 0.017 0.022 -0.002 -0.034 0.014 -0.005 0.815
  -0.000]
 [-0.001 -0.032 0.020 0.084 -0.040 -0.015 -0.022 0.008 -0.013 -0.023
  -0.000 0.733]]
