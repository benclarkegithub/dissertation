Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            76,930
├─Linear: 1-2                            4,851
=================================================================
Total params: 81,781
Trainable params: 81,781
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            4,900
├─Linear: 1-2                            77,616
=================================================================
Total params: 82,516
Trainable params: 82,516
Non-trainable params: 0
=================================================================
[Epoch   1 (148.13s)]	ELBO: 1297.925, 1394.655, 1420.898, 1433.701, 1437.240, 1439.137 (1535.952)	Log prob: 1310.294, 1399.143, 1423.482, 1435.054, 1438.208, 1439.655 (1548.708)	KLD: 12.369, 4.488, 2.585, 1.353, 0.968, 0.517 (12.756)	Grad: 0.079, 0.064, 0.049, 0.045, 0.044, 0.048
[Epoch   2 (121.29s)]	ELBO: 1421.748, 1489.335, 1537.614, 1565.743, 1581.647, 1589.370 (1601.106)	Log prob: 1425.311, 1492.651, 1540.237, 1567.913, 1583.432, 1590.737 (1616.411)	KLD: 3.562, 3.317, 2.624, 2.171, 1.785, 1.367 (15.304)	Grad: 0.067, 0.070, 0.036, 0.033, 0.035, 0.036
[Epoch   3 (133.18s)]	ELBO: 1432.026, 1503.612, 1555.944, 1588.452, 1610.134, 1625.804 (1608.979)	Log prob: 1435.540, 1506.974, 1558.706, 1590.816, 1612.377, 1627.812 (1625.650)	KLD: 3.514, 3.362, 2.762, 2.364, 2.243, 2.008 (16.672)	Grad: 0.072, 0.061, 0.040, 0.038, 0.041, 0.040
[Epoch   4 (170.47s)]	ELBO: 1443.507, 1515.721, 1564.673, 1598.113, 1619.276, 1634.801 (1620.649)	Log prob: 1447.143, 1519.060, 1567.473, 1600.543, 1621.560, 1636.806 (1637.385)	KLD: 3.636, 3.340, 2.800, 2.431, 2.285, 2.006 (16.736)	Grad: 0.082, 0.054, 0.042, 0.041, 0.043, 0.043
[Epoch   5 (172.59s)]	ELBO: 1452.449, 1525.480, 1571.413, 1603.532, 1624.162, 1639.996 (1618.287)	Log prob: 1456.223, 1528.813, 1574.267, 1605.994, 1626.417, 1642.030 (1635.748)	KLD: 3.775, 3.335, 2.853, 2.460, 2.255, 2.033 (17.461)	Grad: 0.084, 0.051, 0.044, 0.042, 0.041, 0.043
[Epoch   6 (188.92s)]	ELBO: 1458.928, 1531.699, 1577.207, 1607.973, 1628.246, 1644.106 (1624.227)	Log prob: 1462.839, 1535.019, 1580.113, 1610.445, 1630.465, 1646.164 (1640.449)	KLD: 3.911, 3.319, 2.905, 2.472, 2.219, 2.058 (16.222)	Grad: 0.084, 0.051, 0.048, 0.043, 0.042, 0.045
[Epoch   7 (188.79s)]	ELBO: 1464.437, 1535.624, 1581.885, 1611.908, 1631.849, 1647.764 (1624.437)	Log prob: 1468.449, 1538.916, 1584.849, 1614.404, 1634.085, 1649.832 (1641.416)	KLD: 4.012, 3.293, 2.964, 2.496, 2.235, 2.068 (16.979)	Grad: 0.084, 0.049, 0.049, 0.043, 0.042, 0.045
[Epoch   8 (166.45s)]	ELBO: 1468.777, 1538.975, 1585.581, 1614.911, 1634.939, 1650.729 (1627.097)	Log prob: 1472.879, 1542.281, 1588.552, 1617.405, 1637.186, 1652.836 (1644.933)	KLD: 4.102, 3.306, 2.972, 2.493, 2.247, 2.106 (17.836)	Grad: 0.081, 0.051, 0.051, 0.043, 0.043, 0.047
[Epoch   9 (178.52s)]	ELBO: 1472.362, 1541.289, 1588.060, 1616.763, 1636.787, 1652.157 (1625.296)	Log prob: 1476.530, 1544.598, 1591.065, 1619.287, 1639.027, 1654.268 (1643.282)	KLD: 4.167, 3.311, 3.004, 2.522, 2.240, 2.111 (17.986)	Grad: 0.087, 0.051, 0.053, 0.044, 0.044, 0.048
[Epoch  10 (170.90s)]	ELBO: 1476.795, 1543.111, 1589.823, 1618.109, 1637.984, 1652.897 (1630.037)	Log prob: 1481.043, 1546.427, 1592.862, 1620.625, 1640.226, 1655.045 (1647.614)	KLD: 4.247, 3.318, 3.040, 2.517, 2.240, 2.149 (17.577)	Grad: 0.083, 0.053, 0.054, 0.044, 0.045, 0.049
[Epoch  11 (171.37s)]	ELBO: 1481.331, 1545.151, 1591.879, 1620.249, 1640.264, 1654.363 (1630.920)	Log prob: 1485.591, 1548.492, 1594.950, 1622.758, 1642.488, 1656.516 (1648.046)	KLD: 4.260, 3.341, 3.071, 2.510, 2.226, 2.153 (17.126)	Grad: 0.088, 0.054, 0.056, 0.045, 0.046, 0.050
[Epoch  12 (177.79s)]	ELBO: 1484.882, 1548.233, 1594.538, 1622.337, 1642.038, 1655.615 (1632.358)	Log prob: 1489.235, 1551.607, 1597.609, 1624.836, 1644.277, 1657.776 (1649.906)	KLD: 4.354, 3.374, 3.071, 2.500, 2.239, 2.159 (17.548)	Grad: 0.086, 0.056, 0.057, 0.046, 0.047, 0.052
[Epoch  13 (170.94s)]	ELBO: 1487.598, 1549.401, 1595.767, 1623.936, 1643.536, 1656.497 (1631.930)	Log prob: 1492.001, 1552.810, 1598.887, 1626.457, 1645.759, 1658.629 (1649.348)	KLD: 4.403, 3.409, 3.119, 2.522, 2.223, 2.131 (17.418)	Grad: 0.088, 0.056, 0.057, 0.046, 0.047, 0.052
[Epoch  14 (178.14s)]	ELBO: 1490.245, 1551.131, 1597.604, 1625.935, 1645.373, 1658.125 (1633.285)	Log prob: 1494.658, 1554.578, 1600.734, 1628.438, 1647.635, 1660.261 (1651.390)	KLD: 4.413, 3.447, 3.129, 2.503, 2.261, 2.135 (18.104)	Grad: 0.087, 0.057, 0.058, 0.046, 0.048, 0.052
[Epoch  15 (163.00s)]	ELBO: 1492.612, 1553.233, 1599.739, 1627.799, 1647.136, 1659.390 (1633.066)	Log prob: 1497.054, 1556.699, 1602.891, 1630.294, 1649.416, 1661.535 (1651.077)	KLD: 4.441, 3.468, 3.152, 2.494, 2.282, 2.145 (18.011)	Grad: 0.087, 0.059, 0.058, 0.045, 0.047, 0.052
[Epoch  16 (169.44s)]	ELBO: 1494.180, 1554.812, 1601.554, 1629.434, 1648.186, 1659.969 (1637.684)	Log prob: 1498.636, 1558.297, 1604.725, 1631.917, 1650.440, 1662.109 (1655.901)	KLD: 4.456, 3.486, 3.171, 2.482, 2.254, 2.138 (18.218)	Grad: 0.086, 0.058, 0.060, 0.047, 0.048, 0.053
[Epoch  17 (172.01s)]	ELBO: 1495.928, 1557.041, 1603.694, 1631.624, 1650.183, 1661.646 (1633.059)	Log prob: 1500.442, 1560.567, 1606.881, 1634.095, 1652.458, 1663.782 (1650.914)	KLD: 4.514, 3.526, 3.186, 2.471, 2.275, 2.136 (17.855)	Grad: 0.089, 0.059, 0.059, 0.048, 0.049, 0.055
[Epoch  18 (167.38s)]	ELBO: 1497.432, 1558.456, 1604.827, 1632.790, 1651.091, 1662.485 (1634.759)	Log prob: 1501.951, 1562.007, 1607.993, 1635.290, 1653.349, 1664.584 (1652.541)	KLD: 4.519, 3.550, 3.167, 2.502, 2.258, 2.099 (17.783)	Grad: 0.090, 0.058, 0.060, 0.048, 0.049, 0.055
[Epoch  19 (170.05s)]	ELBO: 1498.550, 1559.234, 1605.798, 1633.713, 1651.579, 1662.589 (1637.563)	Log prob: 1503.115, 1562.789, 1609.007, 1636.186, 1653.831, 1664.701 (1655.147)	KLD: 4.564, 3.556, 3.209, 2.473, 2.251, 2.111 (17.583)	Grad: 0.095, 0.059, 0.059, 0.047, 0.050, 0.056
[Epoch  20 (170.65s)]	ELBO: 1499.795, 1560.839, 1606.981, 1634.841, 1652.528, 1663.635 (1630.125)	Log prob: 1504.363, 1564.399, 1610.179, 1637.323, 1654.783, 1665.717 (1647.505)	KLD: 4.569, 3.560, 3.197, 2.480, 2.255, 2.082 (17.380)	Grad: 0.092, 0.058, 0.060, 0.048, 0.050, 0.055
[Epoch  21 (162.38s)]	ELBO: 1500.744, 1562.251, 1608.546, 1636.285, 1653.805, 1664.716 (1639.520)	Log prob: 1505.323, 1565.853, 1611.733, 1638.754, 1656.056, 1666.795 (1657.437)	KLD: 4.579, 3.603, 3.186, 2.469, 2.252, 2.079 (17.916)	Grad: 0.094, 0.059, 0.060, 0.048, 0.050, 0.056
[Epoch  22 (174.83s)]	ELBO: 1502.586, 1563.719, 1609.661, 1637.681, 1655.101, 1665.922 (1640.753)	Log prob: 1507.194, 1567.311, 1612.891, 1640.167, 1657.332, 1668.001 (1658.817)	KLD: 4.608, 3.592, 3.231, 2.486, 2.231, 2.078 (18.064)	Grad: 0.096, 0.060, 0.060, 0.048, 0.050, 0.056
[Epoch  23 (165.19s)]	ELBO: 1503.563, 1564.367, 1610.272, 1638.469, 1655.833, 1666.707 (1639.763)	Log prob: 1508.190, 1567.991, 1613.516, 1640.927, 1658.080, 1668.767 (1658.154)	KLD: 4.627, 3.623, 3.244, 2.457, 2.248, 2.062 (18.391)	Grad: 0.096, 0.059, 0.060, 0.048, 0.050, 0.057
[Epoch  24 (164.25s)]	ELBO: 1503.816, 1564.567, 1610.140, 1638.294, 1655.547, 1666.380 (1639.048)	Log prob: 1508.465, 1568.219, 1613.374, 1640.768, 1657.806, 1668.449 (1657.448)	KLD: 4.649, 3.652, 3.234, 2.472, 2.259, 2.069 (18.400)	Grad: 0.098, 0.059, 0.060, 0.049, 0.051, 0.058
[Epoch  25 (167.38s)]	ELBO: 1503.298, 1564.322, 1609.842, 1637.911, 1654.999, 1666.178 (1639.282)	Log prob: 1507.971, 1567.976, 1613.090, 1640.385, 1657.223, 1668.245 (1657.290)	KLD: 4.674, 3.654, 3.249, 2.473, 2.223, 2.067 (18.008)	Grad: 0.102, 0.060, 0.060, 0.049, 0.051, 0.057
[Epoch  26 (164.29s)]	ELBO: 1506.097, 1565.858, 1611.346, 1639.450, 1656.500, 1667.786 (1638.061)	Log prob: 1510.778, 1569.524, 1614.579, 1641.931, 1658.724, 1669.842 (1656.286)	KLD: 4.681, 3.665, 3.233, 2.480, 2.223, 2.055 (18.225)	Grad: 0.100, 0.060, 0.061, 0.050, 0.052, 0.059
[Epoch  27 (163.81s)]	ELBO: 1506.611, 1566.849, 1611.902, 1639.739, 1656.741, 1667.976 (1640.432)	Log prob: 1511.313, 1570.523, 1615.146, 1642.217, 1658.965, 1670.022 (1659.102)	KLD: 4.702, 3.672, 3.244, 2.480, 2.224, 2.046 (18.670)	Grad: 0.104, 0.060, 0.060, 0.051, 0.053, 0.060
[Epoch  28 (169.06s)]	ELBO: 1507.973, 1567.908, 1612.870, 1640.981, 1658.046, 1669.296 (1642.061)	Log prob: 1512.704, 1571.581, 1616.125, 1643.453, 1660.268, 1671.321 (1660.905)	KLD: 4.732, 3.674, 3.254, 2.472, 2.222, 2.025 (18.844)	Grad: 0.102, 0.060, 0.060, 0.049, 0.051, 0.058
[Epoch  29 (156.00s)]	ELBO: 1508.574, 1568.805, 1613.581, 1641.782, 1658.668, 1669.676 (1639.968)	Log prob: 1513.315, 1572.498, 1616.835, 1644.227, 1660.885, 1671.696 (1658.907)	KLD: 4.742, 3.693, 3.253, 2.445, 2.218, 2.020 (18.939)	Grad: 0.105, 0.063, 0.060, 0.050, 0.052, 0.059
[Epoch  30 (147.77s)]	ELBO: 1508.169, 1568.909, 1613.597, 1641.605, 1658.375, 1669.788 (1644.200)	Log prob: 1512.930, 1572.622, 1616.819, 1644.054, 1660.607, 1671.790 (1662.330)	KLD: 4.761, 3.713, 3.221, 2.450, 2.231, 2.001 (18.130)	Grad: 0.108, 0.061, 0.060, 0.050, 0.053, 0.060
[Epoch  31 (159.27s)]	ELBO: 1508.974, 1569.781, 1614.296, 1642.583, 1659.476, 1670.965 (1642.668)	Log prob: 1513.740, 1573.512, 1617.538, 1645.028, 1661.699, 1672.958 (1661.116)	KLD: 4.766, 3.730, 3.243, 2.446, 2.224, 1.993 (18.449)	Grad: 0.106, 0.061, 0.060, 0.050, 0.052, 0.058
[Epoch  32 (151.84s)]	ELBO: 1509.836, 1570.761, 1615.449, 1643.198, 1659.672, 1670.993 (1645.468)	Log prob: 1514.657, 1574.505, 1618.687, 1645.647, 1661.888, 1672.986 (1664.223)	KLD: 4.821, 3.743, 3.238, 2.450, 2.217, 1.992 (18.755)	Grad: 0.114, 0.063, 0.060, 0.051, 0.053, 0.060
[Epoch  33 (154.62s)]	ELBO: 1511.082, 1571.557, 1616.079, 1643.884, 1660.239, 1671.604 (1644.532)	Log prob: 1515.916, 1575.308, 1619.312, 1646.319, 1662.465, 1673.563 (1663.109)	KLD: 4.835, 3.751, 3.233, 2.435, 2.227, 1.959 (18.576)	Grad: 0.113, 0.062, 0.062, 0.051, 0.053, 0.059
[Epoch  34 (153.08s)]	ELBO: 1510.966, 1571.962, 1616.860, 1644.432, 1661.099, 1672.409 (1636.829)	Log prob: 1515.844, 1575.734, 1620.093, 1646.878, 1663.337, 1674.369 (1655.499)	KLD: 4.878, 3.771, 3.232, 2.446, 2.238, 1.961 (18.671)	Grad: 0.113, 0.063, 0.060, 0.051, 0.052, 0.059
[Epoch  35 (159.53s)]	ELBO: 1512.039, 1573.448, 1618.232, 1646.183, 1662.321, 1673.763 (1639.960)	Log prob: 1516.961, 1577.232, 1621.489, 1648.618, 1664.539, 1675.700 (1659.145)	KLD: 4.922, 3.784, 3.257, 2.436, 2.219, 1.936 (19.185)	Grad: 0.113, 0.064, 0.059, 0.050, 0.052, 0.060
[Epoch  36 (127.59s)]	ELBO: 1512.029, 1573.170, 1618.236, 1645.839, 1662.094, 1673.516 (1639.320)	Log prob: 1516.971, 1576.948, 1621.496, 1648.261, 1664.345, 1675.444 (1657.806)	KLD: 4.941, 3.777, 3.260, 2.422, 2.252, 1.927 (18.485)	Grad: 0.113, 0.064, 0.060, 0.051, 0.052, 0.059
[Epoch  37 (124.44s)]	ELBO: 1512.833, 1574.365, 1619.351, 1647.074, 1663.016, 1674.723 (1649.403)	Log prob: 1517.737, 1578.152, 1622.583, 1649.511, 1665.270, 1676.649 (1668.047)	KLD: 4.905, 3.788, 3.232, 2.437, 2.254, 1.925 (18.644)	Grad: 0.119, 0.065, 0.060, 0.051, 0.053, 0.060
[Epoch  38 (136.01s)]	ELBO: 1514.083, 1575.623, 1620.555, 1648.371, 1664.200, 1675.699 (1643.597)	Log prob: 1519.006, 1579.437, 1623.794, 1650.795, 1666.440, 1677.626 (1661.756)	KLD: 4.923, 3.813, 3.240, 2.424, 2.240, 1.926 (18.159)	Grad: 0.114, 0.065, 0.060, 0.051, 0.053, 0.060
[Epoch  39 (116.51s)]	ELBO: 1514.261, 1575.147, 1620.259, 1647.953, 1663.849, 1675.485 (1646.126)	Log prob: 1519.198, 1578.967, 1623.500, 1650.387, 1666.082, 1677.382 (1665.603)	KLD: 4.936, 3.818, 3.242, 2.434, 2.235, 1.899 (19.477)	Grad: 0.116, 0.066, 0.060, 0.051, 0.053, 0.060
[Epoch  40 (115.64s)]	ELBO: 1514.276, 1576.216, 1621.072, 1648.728, 1664.480, 1675.962 (1641.832)	Log prob: 1519.250, 1580.049, 1624.310, 1651.164, 1666.716, 1677.846 (1660.569)	KLD: 4.974, 3.833, 3.238, 2.436, 2.238, 1.884 (18.737)	Grad: 0.117, 0.066, 0.060, 0.051, 0.053, 0.059
[Epoch  41 (124.99s)]	ELBO: 1514.650, 1575.539, 1619.795, 1647.933, 1663.739, 1675.666 (1647.074)	Log prob: 1519.662, 1579.345, 1623.016, 1650.373, 1665.973, 1677.576 (1666.158)	KLD: 5.012, 3.805, 3.221, 2.440, 2.234, 1.909 (19.084)	Grad: 0.119, 0.068, 0.060, 0.051, 0.053, 0.060
[Epoch  42 (106.75s)]	ELBO: 1515.185, 1576.692, 1620.891, 1649.131, 1664.817, 1676.606 (1639.794)	Log prob: 1520.211, 1580.525, 1624.130, 1651.594, 1667.057, 1678.485 (1658.691)	KLD: 5.027, 3.832, 3.239, 2.463, 2.241, 1.878 (18.897)	Grad: 0.122, 0.069, 0.060, 0.051, 0.053, 0.059
[Epoch  43 (107.39s)]	ELBO: 1515.851, 1577.766, 1621.852, 1649.824, 1664.882, 1676.568 (1647.872)	Log prob: 1520.890, 1581.605, 1625.098, 1652.281, 1667.116, 1678.441 (1666.813)	KLD: 5.038, 3.838, 3.247, 2.457, 2.233, 1.874 (18.941)	Grad: 0.123, 0.069, 0.060, 0.051, 0.054, 0.060
[Epoch  44 (107.40s)]	ELBO: 1514.698, 1575.431, 1619.783, 1647.753, 1663.189, 1674.692 (1643.656)	Log prob: 1519.703, 1579.269, 1623.031, 1650.212, 1665.420, 1676.547 (1662.369)	KLD: 5.005, 3.837, 3.249, 2.459, 2.229, 1.856 (18.713)	Grad: 0.130, 0.071, 0.062, 0.052, 0.054, 0.061
[Epoch  45 (116.93s)]	ELBO: 1514.017, 1574.865, 1618.889, 1647.437, 1662.926, 1674.688 (1640.937)	Log prob: 1519.042, 1578.672, 1622.156, 1649.899, 1665.158, 1676.554 (1660.369)	KLD: 5.026, 3.808, 3.267, 2.463, 2.231, 1.868 (19.432)	Grad: 0.130, 0.071, 0.062, 0.053, 0.054, 0.061
[Epoch  46 (108.70s)]	ELBO: 1515.514, 1575.372, 1618.951, 1646.861, 1662.309, 1673.739 (1634.832)	Log prob: 1520.551, 1579.195, 1622.214, 1649.310, 1664.530, 1675.590 (1653.086)	KLD: 5.037, 3.823, 3.263, 2.448, 2.222, 1.849 (18.254)	Grad: 0.128, 0.071, 0.062, 0.053, 0.054, 0.061
No improvement after 10 epochs...
Best epoch(s): [37]	Training time(s): 7055.70s (7055.70s)	Best ELBO: 1676.606 (1649.403)	Best log prob: 1678.485 (1668.047)
Avg. mu: -0.454, -0.332, -0.130, -0.240, -0.217, -0.154
Avg. var: 0.000, 0.001, 0.002, 0.005, 0.007, 0.013
Max. mu: 3.230, 1.171, 2.174, 2.209, 2.472, 1.930
Max. var: 0.013, 0.049, 0.045, 0.039, 0.110, 0.068
Min. mu: -5.650, -5.454, -4.245, -3.065, -1.754, -2.641
Min. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001
Cov. mu:
[[1.422 0.039 0.063 0.121 0.101 -0.130]
 [0.039 0.617 0.033 0.020 0.019 -0.056]
 [0.063 0.033 0.531 -0.028 0.014 0.028]
 [0.121 0.020 -0.028 0.448 0.001 -0.022]
 [0.101 0.019 0.014 0.001 0.267 -0.035]
 [-0.130 -0.056 0.028 -0.022 -0.035 0.357]]
