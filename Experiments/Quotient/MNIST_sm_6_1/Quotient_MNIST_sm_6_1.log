Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            76,930
├─Linear: 1-2                            4,851
=================================================================
Total params: 81,781
Trainable params: 81,781
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            50
├─Linear: 1-2                            50
=================================================================
Total params: 100
Trainable params: 100
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            98
=================================================================
Total params: 98
Trainable params: 98
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            4,900
├─Linear: 1-2                            77,616
=================================================================
Total params: 82,516
Trainable params: 82,516
Non-trainable params: 0
=================================================================
[Epoch   1 (84.58s)]	ELBO: 1156.371, 1277.409, 1297.173, 1303.497, 1305.048, 1302.510 (1418.709)	Log prob: 1162.686, 1288.180, 1308.695, 1315.473, 1317.525, 1315.599 (1429.821)	KLD: 6.315, 10.770, 11.523, 11.977, 12.476, 13.089 (11.111)	Grad: 0.229, 0.350, 0.422, 0.456, 0.510, 0.592
[Epoch   2 (85.28s)]	ELBO: 1352.010, 1423.323, 1446.611, 1447.155, 1447.768, 1446.709 (1484.541)	Log prob: 1356.200, 1431.274, 1456.858, 1457.993, 1459.031, 1458.623 (1497.294)	KLD: 4.191, 7.950, 10.246, 10.838, 11.263, 11.914 (12.752)	Grad: 0.136, 0.161, 0.201, 0.231, 0.282, 0.338
[Epoch   3 (84.62s)]	ELBO: 1381.086, 1450.104, 1495.016, 1510.334, 1515.302, 1514.931 (1551.874)	Log prob: 1384.924, 1457.449, 1505.127, 1522.174, 1528.083, 1528.266 (1566.110)	KLD: 3.838, 7.345, 10.112, 11.840, 12.781, 13.336 (14.236)	Grad: 0.080, 0.126, 0.175, 0.232, 0.289, 0.354
[Epoch   4 (82.85s)]	ELBO: 1387.569, 1466.571, 1520.609, 1549.065, 1567.104, 1566.963 (1581.326)	Log prob: 1391.058, 1473.676, 1530.575, 1561.405, 1581.312, 1581.589 (1595.764)	KLD: 3.489, 7.105, 9.967, 12.341, 14.208, 14.625 (14.438)	Grad: 0.074, 0.123, 0.174, 0.232, 0.291, 0.359
[Epoch   5 (85.21s)]	ELBO: 1396.563, 1477.941, 1535.263, 1568.783, 1587.866, 1589.962 (1605.794)	Log prob: 1400.087, 1485.069, 1545.298, 1581.305, 1602.283, 1604.833 (1621.168)	KLD: 3.526, 7.128, 10.035, 12.522, 14.417, 14.869 (15.374)	Grad: 0.076, 0.126, 0.173, 0.231, 0.292, 0.362
[Epoch   6 (87.02s)]	ELBO: 1401.601, 1484.718, 1546.321, 1582.559, 1602.310, 1609.978 (1622.368)	Log prob: 1405.234, 1491.857, 1556.355, 1595.201, 1616.891, 1625.664 (1638.016)	KLD: 3.633, 7.139, 10.035, 12.641, 14.582, 15.686 (15.648)	Grad: 0.076, 0.132, 0.184, 0.246, 0.312, 0.386
[Epoch   7 (89.44s)]	ELBO: 1404.809, 1488.477, 1553.576, 1592.315, 1612.525, 1625.007 (1635.347)	Log prob: 1408.445, 1495.542, 1563.519, 1604.882, 1627.047, 1641.091 (1651.662)	KLD: 3.637, 7.066, 9.945, 12.567, 14.521, 16.085 (16.316)	Grad: 0.074, 0.132, 0.187, 0.249, 0.318, 0.394
[Epoch   8 (89.63s)]	ELBO: 1408.041, 1492.377, 1559.841, 1600.442, 1620.737, 1636.182 (1643.919)	Log prob: 1411.724, 1499.447, 1569.815, 1612.984, 1635.219, 1652.446 (1660.524)	KLD: 3.684, 7.069, 9.974, 12.542, 14.484, 16.265 (16.605)	Grad: 0.072, 0.137, 0.194, 0.257, 0.326, 0.404
[Epoch   9 (87.33s)]	ELBO: 1411.186, 1496.179, 1565.551, 1606.453, 1626.603, 1642.952 (1648.187)	Log prob: 1414.901, 1503.263, 1575.552, 1618.984, 1641.054, 1659.182 (1664.450)	KLD: 3.715, 7.085, 10.002, 12.531, 14.451, 16.231 (16.263)	Grad: 0.071, 0.141, 0.204, 0.270, 0.343, 0.426
[Epoch  10 (88.38s)]	ELBO: 1413.808, 1499.911, 1570.835, 1611.241, 1631.137, 1647.692 (1652.647)	Log prob: 1417.541, 1507.020, 1580.878, 1623.787, 1645.594, 1663.907 (1669.026)	KLD: 3.733, 7.109, 10.043, 12.548, 14.457, 16.215 (16.379)	Grad: 0.069, 0.141, 0.202, 0.267, 0.339, 0.419
[Epoch  11 (90.15s)]	ELBO: 1416.083, 1503.052, 1575.305, 1614.930, 1634.813, 1651.349 (1656.754)	Log prob: 1419.847, 1510.200, 1585.392, 1627.484, 1649.259, 1667.539 (1672.973)	KLD: 3.765, 7.147, 10.086, 12.554, 14.446, 16.189 (16.218)	Grad: 0.069, 0.143, 0.207, 0.272, 0.345, 0.426
[Epoch  12 (94.25s)]	ELBO: 1418.386, 1506.037, 1579.135, 1617.935, 1638.021, 1654.439 (1659.116)	Log prob: 1422.181, 1513.226, 1589.273, 1630.506, 1652.482, 1670.622 (1675.444)	KLD: 3.794, 7.188, 10.137, 12.570, 14.460, 16.184 (16.328)	Grad: 0.069, 0.142, 0.207, 0.272, 0.344, 0.425
[Epoch  13 (95.88s)]	ELBO: 1420.373, 1508.652, 1582.645, 1620.594, 1640.734, 1656.900 (1661.292)	Log prob: 1424.183, 1515.857, 1592.797, 1633.175, 1655.203, 1673.081 (1677.749)	KLD: 3.810, 7.206, 10.152, 12.582, 14.471, 16.180 (16.458)	Grad: 0.070, 0.144, 0.211, 0.278, 0.354, 0.437
[Epoch  14 (94.06s)]	ELBO: 1422.479, 1511.269, 1585.688, 1622.922, 1643.342, 1659.307 (1662.791)	Log prob: 1426.309, 1518.521, 1595.881, 1635.536, 1657.845, 1675.509 (1679.305)	KLD: 3.829, 7.250, 10.192, 12.614, 14.502, 16.202 (16.514)	Grad: 0.071, 0.145, 0.211, 0.277, 0.351, 0.432
[Epoch  15 (92.32s)]	ELBO: 1424.130, 1513.466, 1588.441, 1624.993, 1645.473, 1661.148 (1664.280)	Log prob: 1428.002, 1520.776, 1598.700, 1637.660, 1660.023, 1677.382 (1680.641)	KLD: 3.872, 7.311, 10.259, 12.667, 14.550, 16.235 (16.361)	Grad: 0.072, 0.146, 0.210, 0.276, 0.350, 0.430
[Epoch  16 (94.09s)]	ELBO: 1425.926, 1515.614, 1591.022, 1627.090, 1647.659, 1663.217 (1666.424)	Log prob: 1429.786, 1522.919, 1601.282, 1639.750, 1662.208, 1679.446 (1682.648)	KLD: 3.861, 7.305, 10.258, 12.660, 14.549, 16.228 (16.224)	Grad: 0.074, 0.147, 0.214, 0.281, 0.357, 0.439
[Epoch  17 (92.27s)]	ELBO: 1427.740, 1517.524, 1593.197, 1628.985, 1649.665, 1664.881 (1666.549)	Log prob: 1431.629, 1524.880, 1603.502, 1641.695, 1664.283, 1681.167 (1682.867)	KLD: 3.890, 7.356, 10.305, 12.710, 14.617, 16.283 (16.318)	Grad: 0.075, 0.150, 0.219, 0.288, 0.367, 0.451
[Epoch  18 (90.13s)]	ELBO: 1429.094, 1519.501, 1595.141, 1630.709, 1651.517, 1666.532 (1667.830)	Log prob: 1433.001, 1526.888, 1605.490, 1643.449, 1666.158, 1682.819 (1684.183)	KLD: 3.906, 7.387, 10.348, 12.740, 14.641, 16.286 (16.354)	Grad: 0.076, 0.150, 0.217, 0.284, 0.359, 0.440
[Epoch  19 (87.72s)]	ELBO: 1430.950, 1521.418, 1597.213, 1632.515, 1653.346, 1668.109 (1670.832)	Log prob: 1434.889, 1528.828, 1607.592, 1645.275, 1668.024, 1684.421 (1687.103)	KLD: 3.939, 7.409, 10.379, 12.760, 14.678, 16.313 (16.270)	Grad: 0.076, 0.150, 0.216, 0.282, 0.357, 0.438
[Epoch  20 (85.45s)]	ELBO: 1432.315, 1522.747, 1598.593, 1633.799, 1654.820, 1669.394 (1671.659)	Log prob: 1436.262, 1530.191, 1609.005, 1646.575, 1669.515, 1685.706 (1687.518)	KLD: 3.948, 7.444, 10.413, 12.777, 14.695, 16.313 (15.859)	Grad: 0.077, 0.153, 0.222, 0.293, 0.372, 0.456
[Epoch  21 (84.83s)]	ELBO: 1433.898, 1524.547, 1600.224, 1635.225, 1656.320, 1670.796 (1671.853)	Log prob: 1437.864, 1532.018, 1610.678, 1648.063, 1671.078, 1687.171 (1688.324)	KLD: 3.965, 7.471, 10.453, 12.840, 14.759, 16.375 (16.470)	Grad: 0.078, 0.153, 0.222, 0.291, 0.367, 0.448
[Epoch  22 (85.84s)]	ELBO: 1435.356, 1526.078, 1601.612, 1636.498, 1657.726, 1671.864 (1673.092)	Log prob: 1439.345, 1533.579, 1612.096, 1649.361, 1672.507, 1688.245 (1689.138)	KLD: 3.988, 7.502, 10.484, 12.862, 14.781, 16.381 (16.046)	Grad: 0.079, 0.154, 0.223, 0.293, 0.371, 0.454
[Epoch  23 (90.12s)]	ELBO: 1436.666, 1527.378, 1602.932, 1637.639, 1658.947, 1672.931 (1674.140)	Log prob: 1440.673, 1534.906, 1613.442, 1650.537, 1673.762, 1689.340 (1690.552)	KLD: 4.008, 7.528, 10.509, 12.899, 14.815, 16.409 (16.413)	Grad: 0.080, 0.155, 0.225, 0.297, 0.376, 0.461
[Epoch  24 (86.23s)]	ELBO: 1437.894, 1528.820, 1604.094, 1638.826, 1660.127, 1673.981 (1674.866)	Log prob: 1441.904, 1536.366, 1614.629, 1651.742, 1674.969, 1690.408 (1691.564)	KLD: 4.010, 7.546, 10.535, 12.916, 14.842, 16.428 (16.698)	Grad: 0.080, 0.155, 0.224, 0.293, 0.370, 0.452
[Epoch  25 (85.05s)]	ELBO: 1439.101, 1530.306, 1605.246, 1639.754, 1661.188, 1674.853 (1676.521)	Log prob: 1443.129, 1537.876, 1615.800, 1652.692, 1676.053, 1691.297 (1693.045)	KLD: 4.027, 7.568, 10.553, 12.939, 14.866, 16.444 (16.524)	Grad: 0.080, 0.156, 0.228, 0.301, 0.383, 0.469
[Epoch  26 (82.22s)]	ELBO: 1440.267, 1531.557, 1606.498, 1640.918, 1662.428, 1675.919 (1677.078)	Log prob: 1444.305, 1539.132, 1617.060, 1653.857, 1677.297, 1692.379 (1693.280)	KLD: 4.039, 7.577, 10.561, 12.941, 14.869, 16.458 (16.202)	Grad: 0.081, 0.155, 0.224, 0.293, 0.370, 0.453
[Epoch  27 (87.55s)]	ELBO: 1441.372, 1532.507, 1607.214, 1641.592, 1663.179, 1676.554 (1676.913)	Log prob: 1445.427, 1540.117, 1617.818, 1654.586, 1678.101, 1693.056 (1692.826)	KLD: 4.055, 7.611, 10.603, 12.994, 14.922, 16.502 (15.912)	Grad: 0.082, 0.159, 0.231, 0.304, 0.385, 0.471
[Epoch  28 (91.09s)]	ELBO: 1442.329, 1533.833, 1608.351, 1642.761, 1664.380, 1677.661 (1678.617)	Log prob: 1446.385, 1541.458, 1618.953, 1655.760, 1679.305, 1694.158 (1695.242)	KLD: 4.056, 7.624, 10.603, 12.999, 14.924, 16.496 (16.625)	Grad: 0.081, 0.157, 0.228, 0.300, 0.379, 0.463
[Epoch  29 (87.21s)]	ELBO: 1443.332, 1534.857, 1609.298, 1643.557, 1665.276, 1678.347 (1678.656)	Log prob: 1447.414, 1542.530, 1619.977, 1656.624, 1680.268, 1694.922 (1694.931)	KLD: 4.082, 7.674, 10.679, 13.067, 14.991, 16.576 (16.275)	Grad: 0.081, 0.157, 0.229, 0.301, 0.381, 0.465
[Epoch  30 (82.92s)]	ELBO: 1444.162, 1535.662, 1609.867, 1644.199, 1665.995, 1678.989 (1679.393)	Log prob: 1448.243, 1543.346, 1620.548, 1657.276, 1681.011, 1695.564 (1696.136)	KLD: 4.082, 7.682, 10.681, 13.078, 15.014, 16.576 (16.744)	Grad: 0.082, 0.158, 0.230, 0.303, 0.383, 0.468
[Epoch  31 (82.67s)]	ELBO: 1445.019, 1536.747, 1610.812, 1644.910, 1666.756, 1679.552 (1680.367)	Log prob: 1449.104, 1544.438, 1621.488, 1657.973, 1681.748, 1696.112 (1697.061)	KLD: 4.087, 7.692, 10.677, 13.061, 14.991, 16.559 (16.694)	Grad: 0.082, 0.159, 0.230, 0.302, 0.381, 0.466
[Epoch  32 (81.89s)]	ELBO: 1445.901, 1537.603, 1611.424, 1645.630, 1667.573, 1680.285 (1680.175)	Log prob: 1449.992, 1545.313, 1622.131, 1658.722, 1682.589, 1696.884 (1697.102)	KLD: 4.091, 7.711, 10.709, 13.092, 15.017, 16.597 (16.927)	Grad: 0.083, 0.160, 0.233, 0.308, 0.390, 0.476
[Epoch  33 (85.08s)]	ELBO: 1446.715, 1538.751, 1612.358, 1646.308, 1668.180, 1680.939 (1680.454)	Log prob: 1450.812, 1546.477, 1623.082, 1659.409, 1683.207, 1697.554 (1696.988)	KLD: 4.097, 7.726, 10.723, 13.100, 15.026, 16.614 (16.534)	Grad: 0.084, 0.161, 0.234, 0.308, 0.389, 0.475
[Epoch  34 (85.66s)]	ELBO: 1447.472, 1539.311, 1612.745, 1646.770, 1668.676, 1681.364 (1681.361)	Log prob: 1451.570, 1547.024, 1623.459, 1659.869, 1683.699, 1697.963 (1697.937)	KLD: 4.098, 7.714, 10.715, 13.099, 15.023, 16.599 (16.576)	Grad: 0.084, 0.162, 0.236, 0.311, 0.392, 0.478
[Epoch  35 (83.73s)]	ELBO: 1448.064, 1539.877, 1613.319, 1647.333, 1669.258, 1681.881 (1681.034)	Log prob: 1452.174, 1547.635, 1624.073, 1660.477, 1684.328, 1698.542 (1697.473)	KLD: 4.110, 7.759, 10.754, 13.144, 15.070, 16.661 (16.439)	Grad: 0.086, 0.165, 0.241, 0.318, 0.402, 0.491
[Epoch  36 (81.35s)]	ELBO: 1448.950, 1540.915, 1614.142, 1648.036, 1670.082, 1682.640 (1682.522)	Log prob: 1453.062, 1548.669, 1624.897, 1661.167, 1685.138, 1699.283 (1699.444)	KLD: 4.113, 7.754, 10.755, 13.129, 15.056, 16.644 (16.922)	Grad: 0.085, 0.162, 0.235, 0.309, 0.389, 0.475
[Epoch  37 (81.05s)]	ELBO: 1449.647, 1541.835, 1614.787, 1648.677, 1670.680, 1683.196 (1682.568)	Log prob: 1453.769, 1549.616, 1625.561, 1661.835, 1685.765, 1699.875 (1698.878)	KLD: 4.122, 7.783, 10.775, 13.158, 15.085, 16.680 (16.310)	Grad: 0.085, 0.163, 0.237, 0.313, 0.395, 0.483
[Epoch  38 (81.47s)]	ELBO: 1450.468, 1542.304, 1615.360, 1649.154, 1671.223, 1683.711 (1683.173)	Log prob: 1454.600, 1550.090, 1626.149, 1662.325, 1686.328, 1700.418 (1699.965)	KLD: 4.132, 7.786, 10.789, 13.171, 15.104, 16.705 (16.792)	Grad: 0.086, 0.164, 0.238, 0.314, 0.396, 0.483
[Epoch  39 (84.09s)]	ELBO: 1450.936, 1543.162, 1615.937, 1649.708, 1671.781, 1684.192 (1683.972)	Log prob: 1455.073, 1550.976, 1626.755, 1662.903, 1686.904, 1700.917 (1700.496)	KLD: 4.136, 7.813, 10.817, 13.197, 15.123, 16.725 (16.524)	Grad: 0.087, 0.165, 0.239, 0.316, 0.399, 0.487
[Epoch  40 (74.58s)]	ELBO: 1451.697, 1543.870, 1616.452, 1650.266, 1672.347, 1684.677 (1683.374)	Log prob: 1455.843, 1551.704, 1627.279, 1663.476, 1687.489, 1701.419 (1699.898)	KLD: 4.147, 7.833, 10.826, 13.211, 15.143, 16.741 (16.525)	Grad: 0.087, 0.166, 0.242, 0.319, 0.403, 0.492
[Epoch  41 (73.97s)]	ELBO: 1452.245, 1544.190, 1616.912, 1650.718, 1672.838, 1685.160 (1684.458)	Log prob: 1456.385, 1552.011, 1627.728, 1663.917, 1687.955, 1701.872 (1701.207)	KLD: 4.139, 7.822, 10.816, 13.199, 15.118, 16.712 (16.749)	Grad: 0.088, 0.167, 0.243, 0.320, 0.403, 0.491
[Epoch  42 (72.96s)]	ELBO: 1452.865, 1545.219, 1617.602, 1651.237, 1673.405, 1685.663 (1684.222)	Log prob: 1457.014, 1553.066, 1628.457, 1664.468, 1688.568, 1702.417 (1700.682)	KLD: 4.150, 7.845, 10.855, 13.231, 15.163, 16.757 (16.460)	Grad: 0.089, 0.168, 0.245, 0.323, 0.407, 0.496
[Epoch  43 (71.56s)]	ELBO: 1453.488, 1545.765, 1618.128, 1651.767, 1673.904, 1686.029 (1685.833)	Log prob: 1457.652, 1553.623, 1628.987, 1665.005, 1689.068, 1702.801 (1702.392)	KLD: 4.163, 7.857, 10.859, 13.238, 15.165, 16.771 (16.559)	Grad: 0.089, 0.168, 0.243, 0.320, 0.403, 0.491
[Epoch  44 (74.86s)]	ELBO: 1454.074, 1545.984, 1618.061, 1651.672, 1673.898, 1686.172 (1684.795)	Log prob: 1458.225, 1553.834, 1628.903, 1664.897, 1689.051, 1702.922 (1701.391)	KLD: 4.151, 7.848, 10.842, 13.225, 15.152, 16.751 (16.596)	Grad: 0.089, 0.169, 0.247, 0.326, 0.412, 0.502
[Epoch  45 (69.22s)]	ELBO: 1454.740, 1547.022, 1619.098, 1652.610, 1674.782, 1686.804 (1684.864)	Log prob: 1458.897, 1554.890, 1629.982, 1665.882, 1689.981, 1703.604 (1701.260)	KLD: 4.157, 7.868, 10.884, 13.271, 15.200, 16.799 (16.396)	Grad: 0.089, 0.168, 0.243, 0.320, 0.403, 0.490
[Epoch  46 (62.82s)]	ELBO: 1455.144, 1547.466, 1619.490, 1652.926, 1675.176, 1687.149 (1686.380)	Log prob: 1459.314, 1555.363, 1630.385, 1666.206, 1690.374, 1703.943 (1703.309)	KLD: 4.172, 7.895, 10.896, 13.281, 15.198, 16.794 (16.930)	Grad: 0.091, 0.171, 0.249, 0.329, 0.415, 0.507
[Epoch  47 (63.56s)]	ELBO: 1455.147, 1547.429, 1619.499, 1652.891, 1675.142, 1687.170 (1686.336)	Log prob: 1459.324, 1555.321, 1630.395, 1666.172, 1690.340, 1703.970 (1703.094)	KLD: 4.177, 7.892, 10.896, 13.281, 15.199, 16.799 (16.758)	Grad: 0.093, 0.176, 0.256, 0.338, 0.427, 0.519
[Epoch  48 (69.68s)]	ELBO: 1456.251, 1548.463, 1620.378, 1653.740, 1675.933, 1687.902 (1686.335)	Log prob: 1460.446, 1556.386, 1631.293, 1667.038, 1691.147, 1704.715 (1703.367)	KLD: 4.195, 7.923, 10.916, 13.297, 15.215, 16.813 (17.033)	Grad: 0.091, 0.171, 0.249, 0.328, 0.413, 0.503
[Epoch  49 (66.04s)]	ELBO: 1456.807, 1549.289, 1620.947, 1654.164, 1676.396, 1688.253 (1686.978)	Log prob: 1461.007, 1557.220, 1631.884, 1667.481, 1691.632, 1705.100 (1703.626)	KLD: 4.199, 7.931, 10.938, 13.318, 15.236, 16.846 (16.648)	Grad: 0.091, 0.171, 0.247, 0.327, 0.412, 0.502
[Epoch  50 (53.74s)]	ELBO: 1457.123, 1549.556, 1621.176, 1654.394, 1676.655, 1688.433 (1685.806)	Log prob: 1461.309, 1557.480, 1632.104, 1667.702, 1691.889, 1705.253 (1702.622)	KLD: 4.186, 7.923, 10.929, 13.308, 15.234, 16.821 (16.816)	Grad: 0.092, 0.172, 0.249, 0.330, 0.416, 0.507
[Epoch  51 (51.99s)]	ELBO: 1457.693, 1550.067, 1621.588, 1654.781, 1676.995, 1688.695 (1687.113)	Log prob: 1461.882, 1558.000, 1632.522, 1668.094, 1692.220, 1705.501 (1703.956)	KLD: 4.189, 7.931, 10.935, 13.315, 15.224, 16.805 (16.843)	Grad: 0.092, 0.173, 0.252, 0.333, 0.422, 0.516
[Epoch  52 (53.31s)]	ELBO: 1458.224, 1550.484, 1621.974, 1655.089, 1677.397, 1689.078 (1687.076)	Log prob: 1462.428, 1558.430, 1632.923, 1668.414, 1692.629, 1705.911 (1703.676)	KLD: 4.203, 7.946, 10.948, 13.324, 15.232, 16.834 (16.600)	Grad: 0.093, 0.175, 0.254, 0.335, 0.423, 0.515
[Epoch  53 (53.25s)]	ELBO: 1458.588, 1551.072, 1622.405, 1655.433, 1677.760, 1689.345 (1687.255)	Log prob: 1462.797, 1559.040, 1633.382, 1668.786, 1693.025, 1706.193 (1704.093)	KLD: 4.209, 7.968, 10.977, 13.352, 15.265, 16.848 (16.838)	Grad: 0.092, 0.173, 0.251, 0.332, 0.418, 0.510
[Epoch  54 (53.13s)]	ELBO: 1458.877, 1551.498, 1622.880, 1655.733, 1678.109, 1689.658 (1686.866)	Log prob: 1463.091, 1559.476, 1633.865, 1669.095, 1693.384, 1706.526 (1703.755)	KLD: 4.213, 7.977, 10.985, 13.361, 15.275, 16.868 (16.889)	Grad: 0.093, 0.176, 0.255, 0.338, 0.426, 0.519
[Epoch  55 (54.23s)]	ELBO: 1459.465, 1551.907, 1623.223, 1656.057, 1678.424, 1689.960 (1686.989)	Log prob: 1463.678, 1559.870, 1634.195, 1669.400, 1693.682, 1706.804 (1703.764)	KLD: 4.213, 7.965, 10.972, 13.342, 15.257, 16.843 (16.775)	Grad: 0.093, 0.176, 0.255, 0.337, 0.425, 0.518
[Epoch  56 (53.91s)]	ELBO: 1459.609, 1552.251, 1623.459, 1656.310, 1678.723, 1690.310 (1686.701)	Log prob: 1463.823, 1560.225, 1634.448, 1669.677, 1694.004, 1707.173 (1703.699)	KLD: 4.214, 7.975, 10.989, 13.367, 15.282, 16.863 (16.999)	Grad: 0.095, 0.177, 0.256, 0.338, 0.426, 0.518
[Epoch  57 (53.59s)]	ELBO: 1459.940, 1552.553, 1623.669, 1656.428, 1678.836, 1690.250 (1687.367)	Log prob: 1464.167, 1560.556, 1634.681, 1669.816, 1694.140, 1707.145 (1704.117)	KLD: 4.229, 8.003, 11.012, 13.389, 15.304, 16.895 (16.750)	Grad: 0.095, 0.180, 0.261, 0.347, 0.437, 0.533
[Epoch  58 (53.12s)]	ELBO: 1460.220, 1552.662, 1623.884, 1656.647, 1679.098, 1690.564 (1687.602)	Log prob: 1464.446, 1560.647, 1634.879, 1670.017, 1694.379, 1707.431 (1704.346)	KLD: 4.226, 7.987, 10.996, 13.370, 15.279, 16.868 (16.744)	Grad: 0.096, 0.180, 0.260, 0.344, 0.432, 0.525
[Epoch  59 (53.35s)]	ELBO: 1460.756, 1553.486, 1624.448, 1657.227, 1679.684, 1690.904 (1687.495)	Log prob: 1464.984, 1561.471, 1635.441, 1670.597, 1694.957, 1707.759 (1704.414)	KLD: 4.226, 7.985, 10.993, 13.370, 15.274, 16.854 (16.919)	Grad: 0.095, 0.178, 0.259, 0.343, 0.432, 0.527
[Epoch  60 (53.11s)]	ELBO: 1461.258, 1554.096, 1624.909, 1657.473, 1679.938, 1691.241 (1689.055)	Log prob: 1465.485, 1562.099, 1635.938, 1670.875, 1695.248, 1708.135 (1705.823)	KLD: 4.228, 8.004, 11.028, 13.402, 15.309, 16.894 (16.768)	Grad: 0.095, 0.177, 0.258, 0.342, 0.431, 0.524
[Epoch  61 (47.33s)]	ELBO: 1461.410, 1553.940, 1624.946, 1657.612, 1680.040, 1691.183 (1689.011)	Log prob: 1465.631, 1561.933, 1635.962, 1670.990, 1695.323, 1708.055 (1706.110)	KLD: 4.220, 7.993, 11.016, 13.378, 15.284, 16.872 (17.099)	Grad: 0.095, 0.179, 0.259, 0.343, 0.431, 0.525
[Epoch  62 (40.98s)]	ELBO: 1461.973, 1554.571, 1625.374, 1657.923, 1680.403, 1691.495 (1688.867)	Log prob: 1466.208, 1562.573, 1636.388, 1671.316, 1695.707, 1708.385 (1705.480)	KLD: 4.235, 8.002, 11.015, 13.393, 15.304, 16.888 (16.613)	Grad: 0.096, 0.181, 0.262, 0.346, 0.436, 0.530
[Epoch  63 (41.40s)]	ELBO: 1462.193, 1554.772, 1625.527, 1658.093, 1680.615, 1691.753 (1688.846)	Log prob: 1466.431, 1562.790, 1636.562, 1671.507, 1695.937, 1708.654 (1705.724)	KLD: 4.239, 8.019, 11.037, 13.415, 15.323, 16.901 (16.878)	Grad: 0.097, 0.181, 0.263, 0.348, 0.438, 0.533
[Epoch  64 (40.29s)]	ELBO: 1462.445, 1555.127, 1625.810, 1658.401, 1680.862, 1691.917 (1688.446)	Log prob: 1466.685, 1563.151, 1636.852, 1671.822, 1696.188, 1708.817 (1705.373)	KLD: 4.240, 8.024, 11.041, 13.420, 15.325, 16.901 (16.927)	Grad: 0.097, 0.182, 0.264, 0.349, 0.438, 0.533
[Epoch  65 (44.23s)]	ELBO: 1462.649, 1555.146, 1626.014, 1658.380, 1680.898, 1691.887 (1688.736)	Log prob: 1466.891, 1563.179, 1637.073, 1671.814, 1696.240, 1708.803 (1705.498)	KLD: 4.242, 8.032, 11.060, 13.436, 15.343, 16.915 (16.762)	Grad: 0.098, 0.183, 0.267, 0.354, 0.445, 0.541
[Epoch  66 (46.37s)]	ELBO: 1462.811, 1555.363, 1626.379, 1658.722, 1681.277, 1692.297 (1690.001)	Log prob: 1467.062, 1563.409, 1637.453, 1672.166, 1696.628, 1709.229 (1707.103)	KLD: 4.251, 8.047, 11.074, 13.444, 15.350, 16.933 (17.103)	Grad: 0.099, 0.184, 0.266, 0.352, 0.443, 0.539
[Epoch  67 (41.96s)]	ELBO: 1463.074, 1555.289, 1626.221, 1658.536, 1681.091, 1692.031 (1688.351)	Log prob: 1467.327, 1563.350, 1637.308, 1671.993, 1696.447, 1708.945 (1705.030)	KLD: 4.253, 8.060, 11.087, 13.457, 15.358, 16.914 (16.678)	Grad: 0.100, 0.189, 0.274, 0.364, 0.459, 0.559
[Epoch  68 (41.30s)]	ELBO: 1463.220, 1556.108, 1626.668, 1659.076, 1681.612, 1692.534 (1689.070)	Log prob: 1467.483, 1564.154, 1637.737, 1672.512, 1696.949, 1709.443 (1706.258)	KLD: 4.263, 8.046, 11.070, 13.437, 15.336, 16.909 (17.188)	Grad: 0.099, 0.185, 0.269, 0.356, 0.449, 0.547
[Epoch  69 (42.13s)]	ELBO: 1463.809, 1556.427, 1626.902, 1659.259, 1681.786, 1692.606 (1689.465)	Log prob: 1468.072, 1564.484, 1637.978, 1672.712, 1697.139, 1709.531 (1706.523)	KLD: 4.263, 8.055, 11.076, 13.451, 15.353, 16.925 (17.058)	Grad: 0.100, 0.185, 0.268, 0.355, 0.448, 0.546
[Epoch  70 (41.33s)]	ELBO: 1464.295, 1556.854, 1627.343, 1659.584, 1682.107, 1692.807 (1688.433)	Log prob: 1468.558, 1564.912, 1638.422, 1673.036, 1697.466, 1709.726 (1705.176)	KLD: 4.263, 8.058, 11.078, 13.452, 15.359, 16.920 (16.743)	Grad: 0.099, 0.184, 0.268, 0.354, 0.446, 0.543
[Epoch  71 (41.19s)]	ELBO: 1464.217, 1556.594, 1627.304, 1659.543, 1682.048, 1692.677 (1689.420)	Log prob: 1468.480, 1564.649, 1638.387, 1672.987, 1697.387, 1709.576 (1706.615)	KLD: 4.263, 8.055, 11.082, 13.445, 15.340, 16.899 (17.195)	Grad: 0.100, 0.187, 0.271, 0.358, 0.450, 0.548
[Epoch  72 (42.60s)]	ELBO: 1464.678, 1557.473, 1627.972, 1660.147, 1682.751, 1693.419 (1689.224)	Log prob: 1468.946, 1565.520, 1639.036, 1673.586, 1698.088, 1710.310 (1706.255)	KLD: 4.268, 8.048, 11.064, 13.438, 15.338, 16.891 (17.030)	Grad: 0.100, 0.186, 0.268, 0.354, 0.445, 0.541
[Epoch  73 (42.32s)]	ELBO: 1465.128, 1557.792, 1628.055, 1660.175, 1682.761, 1693.425 (1688.749)	Log prob: 1469.406, 1565.859, 1639.139, 1673.627, 1698.111, 1710.332 (1705.550)	KLD: 4.279, 8.067, 11.085, 13.452, 15.350, 16.908 (16.801)	Grad: 0.100, 0.185, 0.270, 0.357, 0.450, 0.548
[Epoch  74 (42.23s)]	ELBO: 1464.917, 1557.419, 1627.875, 1659.924, 1682.540, 1693.260 (1689.116)	Log prob: 1469.184, 1565.484, 1638.969, 1673.395, 1697.913, 1710.193 (1706.080)	KLD: 4.265, 8.065, 11.095, 13.470, 15.372, 16.934 (16.964)	Grad: 0.102, 0.189, 0.273, 0.362, 0.455, 0.553
[Epoch  75 (42.88s)]	ELBO: 1465.522, 1558.035, 1628.426, 1660.583, 1683.205, 1693.920 (1689.499)	Log prob: 1469.788, 1566.107, 1639.523, 1674.045, 1698.562, 1710.822 (1706.255)	KLD: 4.265, 8.073, 11.097, 13.461, 15.357, 16.901 (16.756)	Grad: 0.101, 0.187, 0.271, 0.359, 0.451, 0.547
No improvement after 10 epochs...
Best epoch(s): [66]	Training time(s): 5315.25s (5315.25s)	Best ELBO: 1693.920 (1690.001)	Best log prob: 1710.822 (1707.103)
Avg. mu: -0.136, -0.046, 0.012, -0.067, 0.101, 0.048
Avg. var: 0.002, 0.002, 0.004, 0.010, 0.024, 0.071
Max. mu: 6.020, 4.922, 5.367, 4.205, 4.514, 4.763
Max. var: 0.059, 0.035, 0.150, 0.104, 0.178, 0.585
Min. mu: -5.073, -4.737, -4.837, -6.288, -4.171, -6.070
Min. var: 0.000, 0.000, 0.000, 0.002, 0.006, 0.011
Cov. mu:
[[2.213 -0.631 0.006 -0.042 0.020 -0.053]
 [-0.631 2.094 -0.083 0.028 0.027 0.015]
 [0.006 -0.083 1.149 -0.001 0.015 -0.016]
 [-0.042 0.028 -0.001 1.006 0.030 -0.003]
 [0.020 0.027 0.015 0.030 1.115 -0.017]
 [-0.053 0.015 -0.016 -0.003 -0.017 1.163]]
