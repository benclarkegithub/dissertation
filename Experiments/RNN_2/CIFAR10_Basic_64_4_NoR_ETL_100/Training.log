Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            786,688
├─Linear: 1-2                            65,792
=================================================================
Total params: 852,480
Trainable params: 852,480
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            65,792
├─Linear: 1-2                            789,504
=================================================================
Total params: 855,296
Trainable params: 855,296
Non-trainable params: 0
=================================================================
Training model 1/16...
[Epoch   1 (12.50s)]	ELBO: -14828.756 (-12397.405)	Log prob: -14804.262 (-12370.781)	KLD: 24.494 (26.624)	Grad: 254.617
[Epoch   2 (13.61s)]	ELBO: -12203.479 (-11943.901)	Log prob: -12177.462 (-11918.560)	KLD: 26.017 (25.342)	Grad: 203.296
[Epoch   3 (16.61s)]	ELBO: -11863.460 (-11682.287)	Log prob: -11837.962 (-11656.840)	KLD: 25.498 (25.446)	Grad: 198.526
[Epoch   4 (14.52s)]	ELBO: -11669.579 (-11568.814)	Log prob: -11644.019 (-11542.207)	KLD: 25.559 (26.607)	Grad: 192.604
[Epoch   5 (14.52s)]	ELBO: -11537.930 (-11733.072)	Log prob: -11512.363 (-11707.376)	KLD: 25.568 (25.696)	Grad: 162.494
[Epoch   6 (14.56s)]	ELBO: -11508.897 (-11341.582)	Log prob: -11483.442 (-11317.358)	KLD: 25.456 (24.224)	Grad: 176.423
[Epoch   7 (15.52s)]	ELBO: -11383.122 (-11385.544)	Log prob: -11357.500 (-11359.840)	KLD: 25.623 (25.705)	Grad: 155.676
[Epoch   8 (15.30s)]	ELBO: -11314.206 (-11396.053)	Log prob: -11288.432 (-11370.143)	KLD: 25.775 (25.910)	Grad: 169.679
[Epoch   9 (15.47s)]	ELBO: -11314.846 (-11189.023)	Log prob: -11289.528 (-11163.693)	KLD: 25.316 (25.329)	Grad: 167.717
[Epoch  10 (17.04s)]	ELBO: -11262.988 (-11137.028)	Log prob: -11237.481 (-11111.806)	KLD: 25.507 (25.222)	Grad: 147.024
Training model 2/16...
[Epoch   1 (23.59s)]	ELBO: -10368.039 (-8295.989)	Log prob: -10328.990 (-8251.789)	KLD: 13.690 (44.200)	Grad: 65.571
[Epoch   2 (23.12s)]	ELBO: -8253.937 (-7975.185)	Log prob: -8209.932 (-7931.503)	KLD: 19.108 (43.682)	Grad: 73.063
[Epoch   3 (22.87s)]	ELBO: -8017.369 (-7977.898)	Log prob: -7973.247 (-7933.730)	KLD: 19.367 (44.168)	Grad: 65.028
[Epoch   4 (23.00s)]	ELBO: -7879.262 (-7862.102)	Log prob: -7835.018 (-7817.769)	KLD: 19.703 (44.334)	Grad: 59.376
[Epoch   5 (23.15s)]	ELBO: -7826.057 (-7651.592)	Log prob: -7782.189 (-7606.924)	KLD: 19.633 (44.668)	Grad: 59.460
[Epoch   6 (23.26s)]	ELBO: -7810.827 (-8096.754)	Log prob: -7767.264 (-8052.927)	KLD: 19.402 (43.827)	Grad: 63.621
[Epoch   7 (23.59s)]	ELBO: -7787.636 (-7732.895)	Log prob: -7744.329 (-7689.274)	KLD: 19.213 (43.621)	Grad: 62.798
[Epoch   8 (22.86s)]	ELBO: -7723.748 (-7607.432)	Log prob: -7680.618 (-7564.427)	KLD: 19.249 (43.005)	Grad: 57.517
[Epoch   9 (24.12s)]	ELBO: -7690.616 (-7729.518)	Log prob: -7647.890 (-7686.597)	KLD: 19.019 (42.921)	Grad: 59.049
[Epoch  10 (23.42s)]	ELBO: -7693.966 (-7587.219)	Log prob: -7651.297 (-7544.628)	KLD: 19.087 (42.591)	Grad: 56.357
Training model 3/16...
[Epoch   1 (36.70s)]	ELBO: -7716.052 (-7198.943)	Log prob: -7662.208 (-7144.434)	KLD: 11.557 (54.509)	Grad: 47.296
[Epoch   2 (39.54s)]	ELBO: -7202.301 (-6914.154)	Log prob: -7146.173 (-6857.290)	KLD: 14.229 (56.863)	Grad: 42.822
[Epoch   3 (35.93s)]	ELBO: -6586.411 (-6214.610)	Log prob: -6528.039 (-6155.764)	KLD: 16.543 (58.846)	Grad: 56.547
[Epoch   4 (35.05s)]	ELBO: -6238.764 (-6092.047)	Log prob: -6179.105 (-6029.737)	KLD: 18.329 (62.310)	Grad: 67.160
[Epoch   5 (33.21s)]	ELBO: -6045.380 (-5941.570)	Log prob: -5985.162 (-5881.373)	KLD: 19.026 (60.197)	Grad: 65.268
[Epoch   6 (34.34s)]	ELBO: -5908.134 (-5836.127)	Log prob: -5847.884 (-5776.430)	KLD: 19.096 (59.697)	Grad: 60.663
[Epoch   7 (36.24s)]	ELBO: -5867.138 (-6141.872)	Log prob: -5807.050 (-6081.226)	KLD: 19.076 (60.646)	Grad: 62.977
[Epoch   8 (33.84s)]	ELBO: -5861.464 (-5761.378)	Log prob: -5801.860 (-5701.190)	KLD: 18.733 (60.188)	Grad: 59.283
[Epoch   9 (33.47s)]	ELBO: -5782.042 (-5851.708)	Log prob: -5722.702 (-5792.138)	KLD: 18.778 (59.570)	Grad: 61.623
[Epoch  10 (29.62s)]	ELBO: -5757.339 (-5628.614)	Log prob: -5698.563 (-5570.027)	KLD: 18.174 (58.587)	Grad: 61.245
Training model 4/16...
[Epoch   1 (37.45s)]	ELBO: -6125.958 (-5826.104)	Log prob: -6054.383 (-5753.220)	KLD: 12.971 (72.884)	Grad: 50.152
[Epoch   2 (36.37s)]	ELBO: -5664.952 (-5716.404)	Log prob: -5592.767 (-5644.437)	KLD: 14.073 (71.966)	Grad: 32.096
[Epoch   3 (34.85s)]	ELBO: -5553.538 (-5482.981)	Log prob: -5481.249 (-5410.252)	KLD: 14.562 (72.728)	Grad: 33.217
[Epoch   4 (35.20s)]	ELBO: -5306.601 (-5216.614)	Log prob: -5233.729 (-5143.083)	KLD: 15.338 (73.531)	Grad: 38.335
[Epoch   5 (35.67s)]	ELBO: -5015.730 (-4897.497)	Log prob: -4941.919 (-4824.118)	KLD: 16.467 (73.379)	Grad: 48.558
[Epoch   6 (38.33s)]	ELBO: -4872.050 (-4760.715)	Log prob: -4798.182 (-4686.430)	KLD: 16.713 (74.286)	Grad: 56.436
[Epoch   7 (42.13s)]	ELBO: -4763.111 (-4721.040)	Log prob: -4688.828 (-4644.924)	KLD: 17.253 (76.117)	Grad: 53.212
[Epoch   8 (42.91s)]	ELBO: -4659.281 (-4702.526)	Log prob: -4584.911 (-4626.722)	KLD: 17.468 (75.804)	Grad: 50.965
[Epoch   9 (42.75s)]	ELBO: -4572.235 (-4547.228)	Log prob: -4498.420 (-4472.191)	KLD: 17.276 (75.037)	Grad: 56.191
[Epoch  10 (42.37s)]	ELBO: -4482.426 (-4429.928)	Log prob: -4408.239 (-4356.176)	KLD: 17.632 (73.753)	Grad: 54.441
Training model 5/16...
[Epoch   1 (50.15s)]	ELBO: -4599.677 (-4455.185)	Log prob: -4512.270 (-4366.818)	KLD: 13.924 (88.367)	Grad: 39.390
[Epoch   2 (49.45s)]	ELBO: -4372.320 (-4243.751)	Log prob: -4285.048 (-4156.185)	KLD: 14.246 (87.566)	Grad: 25.353
[Epoch   3 (49.99s)]	ELBO: -4268.035 (-4189.105)	Log prob: -4181.613 (-4102.175)	KLD: 13.903 (86.930)	Grad: 26.288
[Epoch   4 (50.62s)]	ELBO: -4102.433 (-4004.374)	Log prob: -4015.752 (-3917.612)	KLD: 14.285 (86.762)	Grad: 30.408
[Epoch   5 (47.15s)]	ELBO: -3998.319 (-3961.506)	Log prob: -3911.778 (-3871.896)	KLD: 14.525 (89.610)	Grad: 31.931
[Epoch   6 (47.84s)]	ELBO: -3965.579 (-3992.345)	Log prob: -3879.422 (-3907.515)	KLD: 14.557 (84.830)	Grad: 33.739
[Epoch   7 (49.84s)]	ELBO: -3902.321 (-3852.045)	Log prob: -3816.366 (-3766.237)	KLD: 14.417 (85.808)	Grad: 31.923
[Epoch   8 (49.59s)]	ELBO: -3872.168 (-3879.963)	Log prob: -3786.395 (-3794.803)	KLD: 14.471 (85.159)	Grad: 32.447
[Epoch   9 (49.13s)]	ELBO: -3817.489 (-3851.452)	Log prob: -3731.721 (-3764.870)	KLD: 14.724 (86.582)	Grad: 33.302
[Epoch  10 (49.24s)]	ELBO: -3720.841 (-3745.815)	Log prob: -3634.928 (-3659.119)	KLD: 15.169 (86.697)	Grad: 38.257
Training model 6/16...
[Epoch   1 (56.24s)]	ELBO: -3860.657 (-3651.754)	Log prob: -3760.156 (-3550.922)	KLD: 14.644 (100.832)	Grad: 46.496
[Epoch   2 (50.52s)]	ELBO: -3641.102 (-3562.974)	Log prob: -3540.790 (-3462.835)	KLD: 15.101 (100.139)	Grad: 30.200
[Epoch   3 (48.15s)]	ELBO: -3621.739 (-3546.772)	Log prob: -3522.173 (-3447.605)	KLD: 14.662 (99.166)	Grad: 28.723
[Epoch   4 (49.67s)]	ELBO: -3549.584 (-3637.025)	Log prob: -3450.837 (-3536.582)	KLD: 14.024 (100.443)	Grad: 24.899
[Epoch   5 (48.85s)]	ELBO: -3497.677 (-3497.482)	Log prob: -3399.373 (-3400.475)	KLD: 13.574 (97.007)	Grad: 23.178
[Epoch   6 (54.66s)]	ELBO: -3402.624 (-3407.217)	Log prob: -3304.575 (-3310.470)	KLD: 13.200 (96.748)	Grad: 22.969
[Epoch   7 (51.46s)]	ELBO: -3324.257 (-3319.846)	Log prob: -3226.772 (-3222.368)	KLD: 12.768 (97.478)	Grad: 20.351
[Epoch   8 (56.66s)]	ELBO: -3310.957 (-3518.688)	Log prob: -3214.005 (-3421.964)	KLD: 12.392 (96.724)	Grad: 21.799
[Epoch   9 (64.81s)]	ELBO: -3286.091 (-3343.204)	Log prob: -3189.440 (-3249.642)	KLD: 12.311 (93.562)	Grad: 19.903
[Epoch  10 (62.98s)]	ELBO: -3248.960 (-3168.053)	Log prob: -3152.462 (-3069.547)	KLD: 12.235 (98.506)	Grad: 20.098
Training model 7/16...
[Epoch   1 (78.08s)]	ELBO: -3427.288 (-3322.083)	Log prob: -3315.659 (-3210.483)	KLD: 15.402 (111.600)	Grad: 58.510
[Epoch   2 (77.75s)]	ELBO: -3145.308 (-3127.899)	Log prob: -3032.939 (-3015.308)	KLD: 15.768 (112.591)	Grad: 34.285
[Epoch   3 (83.12s)]	ELBO: -3065.380 (-3107.911)	Log prob: -2953.421 (-2995.135)	KLD: 15.255 (112.776)	Grad: 30.624
[Epoch   4 (86.92s)]	ELBO: -2983.000 (-3172.269)	Log prob: -2871.798 (-3060.430)	KLD: 14.500 (111.838)	Grad: 25.692
[Epoch   5 (84.66s)]	ELBO: -2961.702 (-3009.058)	Log prob: -2851.636 (-2899.434)	KLD: 13.679 (109.624)	Grad: 23.240
[Epoch   6 (88.13s)]	ELBO: -2904.386 (-2986.329)	Log prob: -2795.021 (-2875.611)	KLD: 13.248 (110.718)	Grad: 19.877
[Epoch   7 (93.76s)]	ELBO: -2845.839 (-2865.411)	Log prob: -2736.819 (-2755.151)	KLD: 12.756 (110.261)	Grad: 18.615
[Epoch   8 (84.26s)]	ELBO: -2828.595 (-2889.662)	Log prob: -2720.368 (-2781.330)	KLD: 12.049 (108.332)	Grad: 19.410
[Epoch   9 (84.27s)]	ELBO: -2783.769 (-2766.285)	Log prob: -2675.931 (-2659.845)	KLD: 11.804 (106.440)	Grad: 17.261
[Epoch  10 (94.37s)]	ELBO: -2750.924 (-2731.873)	Log prob: -2643.582 (-2625.594)	KLD: 11.336 (106.278)	Grad: 15.527
Training model 8/16...
[Epoch   1 (84.13s)]	ELBO: -2941.509 (-2733.320)	Log prob: -2819.941 (-2612.711)	KLD: 15.438 (120.608)	Grad: 46.813
[Epoch   2 (78.93s)]	ELBO: -2741.335 (-2820.476)	Log prob: -2620.288 (-2699.904)	KLD: 15.701 (120.572)	Grad: 29.682
[Epoch   3 (64.95s)]	ELBO: -2716.403 (-2735.376)	Log prob: -2596.689 (-2615.726)	KLD: 14.789 (119.650)	Grad: 27.361
[Epoch   4 (78.09s)]	ELBO: -2721.349 (-2780.650)	Log prob: -2602.379 (-2661.250)	KLD: 14.239 (119.399)	Grad: 26.100
[Epoch   5 (91.20s)]	ELBO: -2620.994 (-2660.672)	Log prob: -2502.603 (-2543.915)	KLD: 13.259 (116.757)	Grad: 20.842
[Epoch   6 (84.12s)]	ELBO: -2561.643 (-2635.687)	Log prob: -2443.218 (-2515.679)	KLD: 12.611 (120.008)	Grad: 20.263
[Epoch   7 (61.84s)]	ELBO: -2479.921 (-2524.216)	Log prob: -2361.335 (-2404.566)	KLD: 12.139 (119.650)	Grad: 18.646
[Epoch   8 (52.36s)]	ELBO: -2425.404 (-2701.545)	Log prob: -2306.939 (-2584.464)	KLD: 11.592 (117.081)	Grad: 17.102
[Epoch   9 (48.97s)]	ELBO: -2395.448 (-2424.983)	Log prob: -2277.190 (-2307.698)	KLD: 11.412 (117.285)	Grad: 17.670
[Epoch  10 (49.81s)]	ELBO: -2324.437 (-2360.760)	Log prob: -2206.195 (-2240.831)	KLD: 11.165 (119.929)	Grad: 16.758
Training model 9/16...
[Epoch   1 (59.41s)]	ELBO: -2500.063 (-2432.206)	Log prob: -2365.393 (-2294.613)	KLD: 16.877 (137.593)	Grad: 70.394
[Epoch   2 (61.74s)]	ELBO: -2276.573 (-2248.909)	Log prob: -2141.878 (-2113.911)	KLD: 17.085 (134.998)	Grad: 43.705
[Epoch   3 (64.82s)]	ELBO: -2195.052 (-2210.796)	Log prob: -2060.874 (-2077.304)	KLD: 16.682 (133.492)	Grad: 35.949
[Epoch   4 (64.80s)]	ELBO: -2153.082 (-2226.764)	Log prob: -2020.093 (-2095.019)	KLD: 15.763 (131.745)	Grad: 30.285
[Epoch   5 (64.74s)]	ELBO: -2104.467 (-2148.794)	Log prob: -1972.500 (-2017.618)	KLD: 14.941 (131.177)	Grad: 26.446
[Epoch   6 (64.19s)]	ELBO: -2087.396 (-2168.440)	Log prob: -1956.461 (-2037.449)	KLD: 14.145 (130.990)	Grad: 23.921
[Epoch   7 (66.55s)]	ELBO: -2070.625 (-2098.822)	Log prob: -1940.700 (-1968.872)	KLD: 13.331 (129.949)	Grad: 22.216
[Epoch   8 (81.06s)]	ELBO: -2012.193 (-2192.792)	Log prob: -1882.920 (-2063.975)	KLD: 12.546 (128.817)	Grad: 19.061
[Epoch   9 (80.61s)]	ELBO: -1980.212 (-1976.766)	Log prob: -1851.520 (-1848.218)	KLD: 11.937 (128.548)	Grad: 18.490
[Epoch  10 (83.75s)]	ELBO: -1928.766 (-1988.102)	Log prob: -1800.789 (-1858.171)	KLD: 11.461 (129.932)	Grad: 15.794
Training model 10/16...
[Epoch   1 (72.43s)]	ELBO: -2126.709 (-2060.143)	Log prob: -1982.589 (-1915.850)	KLD: 17.192 (144.293)	Grad: 51.830
[Epoch   2 (74.81s)]	ELBO: -1968.939 (-2024.638)	Log prob: -1825.700 (-1883.777)	KLD: 16.960 (140.861)	Grad: 33.663
[Epoch   3 (70.20s)]	ELBO: -1926.795 (-1899.110)	Log prob: -1785.797 (-1758.855)	KLD: 15.504 (140.255)	Grad: 27.859
[Epoch   4 (70.26s)]	ELBO: -1910.321 (-1953.835)	Log prob: -1771.019 (-1817.811)	KLD: 14.254 (136.024)	Grad: 24.913
[Epoch   5 (68.04s)]	ELBO: -1898.656 (-2049.702)	Log prob: -1760.843 (-1909.532)	KLD: 13.121 (140.170)	Grad: 22.508
[Epoch   6 (67.24s)]	ELBO: -1887.839 (-2258.977)	Log prob: -1751.069 (-2121.612)	KLD: 12.317 (137.366)	Grad: 19.312
[Epoch   7 (76.03s)]	ELBO: -1869.848 (-2076.000)	Log prob: -1733.635 (-1937.746)	KLD: 11.856 (138.253)	Grad: 18.002
[Epoch   8 (76.19s)]	ELBO: -1845.426 (-1934.025)	Log prob: -1710.033 (-1799.347)	KLD: 11.574 (134.677)	Grad: 16.051
[Epoch   9 (74.29s)]	ELBO: -1844.434 (-1871.589)	Log prob: -1709.932 (-1734.280)	KLD: 10.920 (137.308)	Grad: 16.006
[Epoch  10 (76.18s)]	ELBO: -1824.752 (-1919.334)	Log prob: -1690.666 (-1785.294)	KLD: 10.619 (134.039)	Grad: 14.255
Training model 11/16...
[Epoch   1 (86.68s)]	ELBO: -2028.160 (-1900.466)	Log prob: -1876.130 (-1749.151)	KLD: 18.168 (151.315)	Grad: 67.911
[Epoch   2 (90.15s)]	ELBO: -1812.135 (-1956.943)	Log prob: -1660.434 (-1805.144)	KLD: 18.071 (151.800)	Grad: 43.209
[Epoch   3 (101.72s)]	ELBO: -1776.482 (-1857.177)	Log prob: -1626.542 (-1711.360)	KLD: 16.675 (145.816)	Grad: 35.481
[Epoch   4 (92.87s)]	ELBO: -1729.293 (-1777.029)	Log prob: -1581.217 (-1628.466)	KLD: 15.433 (148.563)	Grad: 28.554
[Epoch   5 (79.58s)]	ELBO: -1717.644 (-1813.917)	Log prob: -1570.812 (-1669.662)	KLD: 14.493 (144.255)	Grad: 25.059
[Epoch   6 (66.49s)]	ELBO: -1701.952 (-1839.503)	Log prob: -1556.165 (-1695.335)	KLD: 13.565 (144.169)	Grad: 23.291
[Epoch   7 (65.42s)]	ELBO: -1668.297 (-1777.321)	Log prob: -1522.605 (-1628.411)	KLD: 12.816 (148.910)	Grad: 20.849
[Epoch   8 (70.41s)]	ELBO: -1628.516 (-1647.032)	Log prob: -1482.515 (-1500.895)	KLD: 12.148 (146.137)	Grad: 20.560
[Epoch   9 (74.41s)]	ELBO: -1579.680 (-1702.976)	Log prob: -1434.658 (-1558.645)	KLD: 11.744 (144.331)	Grad: 17.140
[Epoch  10 (76.47s)]	ELBO: -1576.065 (-1606.184)	Log prob: -1432.223 (-1464.948)	KLD: 11.216 (141.236)	Grad: 16.465
Training model 12/16...
[Epoch   1 (85.01s)]	ELBO: -1771.583 (-1843.745)	Log prob: -1611.938 (-1687.702)	KLD: 16.963 (156.043)	Grad: 61.476
[Epoch   2 (85.79s)]	ELBO: -1613.556 (-1878.694)	Log prob: -1454.393 (-1719.027)	KLD: 17.458 (159.667)	Grad: 44.400
[Epoch   3 (84.71s)]	ELBO: -1582.272 (-1737.818)	Log prob: -1424.786 (-1580.237)	KLD: 16.598 (157.581)	Grad: 35.072
[Epoch   4 (84.39s)]	ELBO: -1557.772 (-1592.886)	Log prob: -1401.726 (-1437.750)	KLD: 15.626 (155.137)	Grad: 30.763
[Epoch   5 (86.84s)]	ELBO: -1512.488 (-1572.765)	Log prob: -1357.037 (-1416.511)	KLD: 14.553 (156.254)	Grad: 26.782
[Epoch   6 (82.01s)]	ELBO: -1464.504 (-1756.722)	Log prob: -1310.007 (-1603.903)	KLD: 13.684 (152.819)	Grad: 23.086
[Epoch   7 (76.30s)]	ELBO: -1454.091 (-1497.160)	Log prob: -1300.853 (-1345.995)	KLD: 12.882 (151.165)	Grad: 21.090
[Epoch   8 (76.29s)]	ELBO: -1433.083 (-1469.048)	Log prob: -1281.190 (-1316.537)	KLD: 12.284 (152.511)	Grad: 19.882
[Epoch   9 (75.55s)]	ELBO: -1424.514 (-1512.167)	Log prob: -1273.484 (-1359.792)	KLD: 11.735 (152.374)	Grad: 18.275
[Epoch  10 (76.71s)]	ELBO: -1425.341 (-1492.607)	Log prob: -1275.388 (-1345.026)	KLD: 11.210 (147.581)	Grad: 17.229
Training model 13/16...
[Epoch   1 (82.34s)]	ELBO: -1618.426 (-1712.213)	Log prob: -1452.503 (-1547.210)	KLD: 16.540 (165.003)	Grad: 75.075
[Epoch   2 (83.79s)]	ELBO: -1460.410 (-1501.735)	Log prob: -1295.176 (-1335.289)	KLD: 16.902 (166.446)	Grad: 52.522
[Epoch   3 (85.66s)]	ELBO: -1420.620 (-1492.820)	Log prob: -1256.294 (-1329.299)	KLD: 16.589 (163.522)	Grad: 42.729
[Epoch   4 (86.68s)]	ELBO: -1411.721 (-1549.562)	Log prob: -1248.668 (-1386.906)	KLD: 16.090 (162.656)	Grad: 37.375
[Epoch   5 (90.40s)]	ELBO: -1392.210 (-1556.279)	Log prob: -1230.172 (-1392.663)	KLD: 14.985 (163.617)	Grad: 33.411
[Epoch   6 (96.55s)]	ELBO: -1365.628 (-1393.978)	Log prob: -1203.686 (-1233.752)	KLD: 14.323 (160.226)	Grad: 27.369
[Epoch   7 (91.92s)]	ELBO: -1321.183 (-1350.731)	Log prob: -1160.120 (-1190.555)	KLD: 13.316 (160.177)	Grad: 22.713
[Epoch   8 (88.05s)]	ELBO: -1299.626 (-1302.092)	Log prob: -1139.573 (-1141.114)	KLD: 12.560 (160.978)	Grad: 19.819
[Epoch   9 (92.32s)]	ELBO: -1281.386 (-1452.842)	Log prob: -1122.278 (-1290.875)	KLD: 11.920 (161.967)	Grad: 17.994
[Epoch  10 (92.77s)]	ELBO: -1288.681 (-1457.512)	Log prob: -1130.718 (-1299.814)	KLD: 11.242 (157.698)	Grad: 17.088
Training model 14/16...
[Epoch   1 (100.75s)]	ELBO: -1465.266 (-1515.888)	Log prob: -1291.071 (-1340.036)	KLD: 17.431 (175.852)	Grad: 64.379
[Epoch   2 (102.12s)]	ELBO: -1316.945 (-1396.064)	Log prob: -1144.256 (-1223.832)	KLD: 17.114 (172.232)	Grad: 44.216
[Epoch   3 (104.53s)]	ELBO: -1296.849 (-1405.667)	Log prob: -1126.034 (-1236.291)	KLD: 16.108 (169.376)	Grad: 38.121
[Epoch   4 (109.54s)]	ELBO: -1259.863 (-1371.418)	Log prob: -1090.542 (-1203.708)	KLD: 15.075 (167.711)	Grad: 29.686
[Epoch   5 (104.65s)]	ELBO: -1252.526 (-1410.091)	Log prob: -1084.656 (-1241.627)	KLD: 14.129 (168.464)	Grad: 26.283
[Epoch   6 (105.91s)]	ELBO: -1257.282 (-1260.506)	Log prob: -1090.217 (-1094.526)	KLD: 13.475 (165.979)	Grad: 23.800
[Epoch   7 (104.48s)]	ELBO: -1236.338 (-1263.441)	Log prob: -1070.216 (-1096.747)	KLD: 12.780 (166.694)	Grad: 22.962
[Epoch   8 (100.26s)]	ELBO: -1221.799 (-1292.448)	Log prob: -1056.377 (-1131.562)	KLD: 12.005 (160.886)	Grad: 19.681
[Epoch   9 (104.18s)]	ELBO: -1177.057 (-1342.267)	Log prob: -1012.098 (-1177.965)	KLD: 11.502 (164.303)	Grad: 16.331
[Epoch  10 (100.56s)]	ELBO: -1165.369 (-1181.993)	Log prob: -999.961 (-1018.296)	KLD: 11.095 (163.697)	Grad: 17.276
Training model 15/16...
[Epoch   1 (116.93s)]	ELBO: -1334.050 (-1213.268)	Log prob: -1151.168 (-1029.389)	KLD: 17.485 (183.879)	Grad: 83.895
[Epoch   2 (117.08s)]	ELBO: -1140.729 (-1220.094)	Log prob: -958.889 (-1037.034)	KLD: 17.753 (183.060)	Grad: 48.679
[Epoch   3 (125.84s)]	ELBO: -1127.543 (-1170.986)	Log prob: -947.618 (-993.264)	KLD: 16.729 (177.722)	Grad: 42.204
[Epoch   4 (131.21s)]	ELBO: -1089.857 (-1163.111)	Log prob: -911.952 (-984.157)	KLD: 15.902 (178.954)	Grad: 32.779
[Epoch   5 (145.07s)]	ELBO: -1065.820 (-1235.454)	Log prob: -889.269 (-1060.777)	KLD: 15.153 (174.677)	Grad: 28.478
[Epoch   6 (117.37s)]	ELBO: -1073.289 (-1260.051)	Log prob: -898.085 (-1084.638)	KLD: 14.338 (175.414)	Grad: 23.776
[Epoch   7 (111.53s)]	ELBO: -1049.423 (-1102.484)	Log prob: -876.235 (-929.024)	KLD: 13.190 (173.460)	Grad: 19.767
[Epoch   8 (111.72s)]	ELBO: -1045.227 (-1210.774)	Log prob: -873.270 (-1037.047)	KLD: 12.397 (173.728)	Grad: 18.055
[Epoch   9 (132.97s)]	ELBO: -1040.887 (-1126.341)	Log prob: -870.138 (-958.560)	KLD: 11.669 (167.781)	Grad: 16.713
[Epoch  10 (140.39s)]	ELBO: -1021.198 (-1127.737)	Log prob: -851.029 (-957.665)	KLD: 11.280 (170.072)	Grad: 15.300
Training model 16/16...
[Epoch   1 (129.83s)]	ELBO: -1286.179 (-1251.352)	Log prob: -1099.002 (-1063.121)	KLD: 17.602 (188.231)	Grad: 84.245
[Epoch   2 (136.94s)]	ELBO: -1071.428 (-1068.979)	Log prob: -884.755 (-883.725)	KLD: 17.775 (185.254)	Grad: 52.060
[Epoch   3 (121.05s)]	ELBO: -1019.596 (-1223.575)	Log prob: -833.577 (-1039.452)	KLD: 17.327 (184.123)	Grad: 50.132
[Epoch   4 (118.72s)]	ELBO: -996.525 (-1178.500)	Log prob: -811.855 (-996.636)	KLD: 16.425 (181.864)	Grad: 41.589
[Epoch   5 (119.81s)]	ELBO: -981.050 (-1070.561)	Log prob: -797.482 (-887.040)	KLD: 15.770 (183.520)	Grad: 39.898
[Epoch   6 (122.80s)]	ELBO: -950.865 (-1052.775)	Log prob: -768.188 (-869.051)	KLD: 15.275 (183.724)	Grad: 33.409
[Epoch   7 (108.77s)]	ELBO: -944.841 (-1046.415)	Log prob: -763.500 (-862.065)	KLD: 14.764 (184.351)	Grad: 31.184
[Epoch   8 (108.61s)]	ELBO: -932.253 (-1180.996)	Log prob: -751.725 (-997.179)	KLD: 14.284 (183.817)	Grad: 28.843
[Epoch   9 (117.51s)]	ELBO: -919.955 (-1081.337)	Log prob: -739.736 (-898.597)	KLD: 13.989 (182.740)	Grad: 29.617
[Epoch  10 (109.21s)]	ELBO: -875.804 (-942.487)	Log prob: -695.302 (-765.155)	KLD: 13.635 (177.333)	Grad: 25.504
Best epoch(s): [10, 10, 10, 10, 10, 10, 10, 10, 9, 9, 10, 8, 8, 10, 7, 10]	Training time(s): 149.65s, 232.97s, 347.94s, 388.05s, 492.99s, 544.00s, 855.32s, 694.41s, 691.68s, 725.65s, 804.19s, 813.61s, 890.47s, 1036.97s, 1250.11s, 1193.24s (11111.23s)	Best ELBO: -875.804 (-942.487)	Best log prob: -695.302 (-765.155)
Avg. mu: 1.891, -0.268, -0.818, 0.448, 0.422, 0.014, 0.199, -0.083, -0.293, -0.059, 0.282, 0.194, -0.062, -0.078, 0.127, -0.081, -0.069, 0.334, -0.198, 0.031, -0.312, 0.047, -0.406, -0.028, -0.373, -0.351, -0.345, 0.152, -0.372, -0.144, -0.049, -0.511, -0.527, 0.200, 0.240, -0.421, -0.403, -0.086, 0.792, 0.367, 0.313, 0.488, 0.761, -0.028, -0.650, 0.216, 0.184, -0.211, -0.355, -0.455, -0.559, -0.473, -0.337, 0.361, -0.641, 0.279, 0.010, -0.169, 0.008, -0.223, 0.336, 0.164, -0.325, 0.248
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.002, 0.001, 0.001, 0.001, 0.002, 0.002, 0.001, 0.002, 0.002, 0.002, 0.001, 0.002, 0.003, 0.002, 0.001, 0.002, 0.001, 0.012, 0.024, 0.428, 0.031, 0.006, 0.123, 0.012, 0.010, 0.009, 0.012, 0.051, 0.010, 0.021, 0.033, 0.016, 0.010, 0.071, 0.019, 0.008, 0.021, 0.013, 0.009, 0.005, 0.010, 0.008, 0.001, 0.001, 0.001, 0.001
Max. mu: 4.866, 8.526, 3.774, 4.860, 5.292, 3.723, 4.777, 4.771, 4.424, 4.748, 7.440, 7.292, 5.344, 4.569, 6.526, 4.995, 3.513, 3.551, 3.934, 3.620, 2.745, 3.344, 3.606, 3.749, 2.361, 2.810, 2.411, 4.219, 2.036, 3.472, 3.456, 2.137, 0.994, 2.897, 2.147, 4.065, 1.597, 2.954, 2.708, 1.938, 3.806, 1.697, 2.083, 1.100, 0.046, 1.301, 1.389, 1.481, 0.296, 0.088, -0.007, 0.768, 0.269, 1.227, 0.853, 0.880, 0.678, 0.367, 0.443, 0.111, 0.618, 0.400, -0.124, 0.661
Max. var: 0.000, 0.002, 0.001, 0.001, 0.002, 0.002, 0.002, 0.003, 0.004, 0.005, 0.004, 0.003, 0.011, 0.006, 0.008, 0.005, 0.009, 0.005, 0.011, 0.010, 0.014, 0.010, 0.008, 0.013, 0.013, 0.012, 0.016, 0.014, 0.009, 0.011, 0.034, 0.019, 0.006, 0.020, 0.015, 0.119, 0.359, 1.981, 0.186, 0.077, 0.353, 0.083, 0.033, 0.069, 0.108, 0.145, 0.036, 0.095, 0.092, 0.057, 0.085, 0.294, 0.091, 0.080, 0.093, 0.064, 0.046, 0.040, 0.061, 0.043, 0.029, 0.031, 0.028, 0.026
Min. mu: -2.277, -4.936, -4.316, -4.543, -4.988, -5.602, -5.256, -5.031, -4.394, -9.396, -4.278, -7.083, -9.059, -4.353, -4.290, -4.177, -4.591, -3.113, -3.671, -3.581, -3.288, -4.042, -3.410, -3.264, -3.122, -4.105, -3.438, -4.056, -2.945, -3.161, -2.962, -3.586, -2.279, -2.273, -1.546, -3.580, -2.811, -2.141, -0.584, -0.807, -1.689, -0.974, -0.164, -2.642, -1.805, -1.495, -0.962, -1.688, -1.208, -1.452, -1.711, -1.911, -0.994, -0.499, -1.418, -0.517, -0.330, -0.607, -0.565, -0.581, 0.100, -0.086, -0.646, 0.091
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.002, 0.001, 0.048, 0.002, 0.000, 0.015, 0.001, 0.001, 0.001, 0.001, 0.002, 0.000, 0.003, 0.001, 0.001, 0.000, 0.005, 0.001, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.478 0.290 -0.023 ... 0.004 0.010 0.007]
 [0.290 1.834 0.169 ... 0.015 0.001 0.012]
 [-0.023 0.169 0.897 ... -0.001 -0.005 -0.001]
 ...
 [0.004 0.015 -0.001 ... 0.001 0.000 0.000]
 [0.010 0.001 -0.005 ... 0.000 0.001 0.000]
 [0.007 0.012 -0.001 ... 0.000 0.000 0.001]]
Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            786,688
├─Linear: 1-2                            65,792
=================================================================
Total params: 852,480
Trainable params: 852,480
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            65,792
├─Linear: 1-2                            789,504
=================================================================
Total params: 855,296
Trainable params: 855,296
Non-trainable params: 0
=================================================================
[Epoch   1 (148.76s)]	ELBO: -11273.196, -7480.693, -5433.725, -4016.420, -3060.782, -2342.847, -1738.591, -1268.295, -995.154, -973.781, -975.713, -986.242, -996.915, -1012.663, -1038.954, -1086.089 (-944.666)	Log prob: -11251.781, -7443.731, -5381.786, -3951.107, -2982.740, -2252.259, -1635.828, -1153.409, -868.993, -841.159, -835.696, -839.398, -843.697, -852.182, -870.033, -903.929 (-763.238)	KLD: 21.418, 15.545, 14.974, 13.378, 12.728, 12.544, 12.176, 12.122, 11.275, 6.461, 7.395, 6.827, 6.374, 7.263, 8.440, 13.239 (181.428)	Grad: 133.671, 31.269, 24.899, 18.235, 16.279, 14.071, 12.306, 14.334, 14.114, 5.010, 6.801, 6.871, 5.558, 7.306, 9.316, 26.551
[Epoch   2 (160.69s)]	ELBO: -11204.428, -7393.159, -5347.718, -3911.753, -2931.998, -2195.865, -1579.850, -1084.016, -775.004, -742.449, -739.637, -745.067, -752.941, -763.669, -780.291, -807.410 (-917.018)	Log prob: -11182.964, -7356.128, -5295.698, -3846.345, -2853.838, -2105.155, -1476.967, -968.976, -648.561, -610.098, -600.236, -599.242, -601.223, -605.015, -613.474, -627.791 (-737.970)	KLD: 21.463, 15.568, 14.991, 13.384, 12.754, 12.551, 12.172, 12.156, 11.404, 5.908, 7.050, 6.425, 5.892, 6.937, 8.162, 12.802 (179.048)	Grad: 103.223, 31.344, 24.740, 16.378, 14.327, 13.689, 13.590, 12.279, 12.692, 4.772, 5.573, 5.306, 4.266, 5.473, 7.744, 21.457
[Epoch   3 (164.53s)]	ELBO: -11199.465, -7383.626, -5356.084, -3923.430, -2946.241, -2210.280, -1593.730, -1098.299, -776.904, -746.352, -746.046, -752.473, -760.902, -772.052, -788.805, -817.964 (-960.208)	Log prob: -11177.981, -7346.576, -5304.063, -3858.014, -2868.103, -2119.590, -1490.880, -983.326, -650.412, -613.948, -606.724, -606.818, -609.504, -613.831, -622.498, -639.112 (-781.021)	KLD: 21.484, 15.561, 14.972, 13.399, 12.721, 12.553, 12.160, 12.121, 11.519, 5.913, 6.918, 6.332, 5.743, 6.823, 8.086, 12.546 (179.187)	Grad: 114.144, 33.265, 25.509, 18.155, 15.215, 13.884, 13.204, 14.137, 12.353, 4.902, 5.660, 5.214, 4.467, 5.605, 7.689, 22.014
[Epoch   4 (159.42s)]	ELBO: -11189.482, -7390.793, -5358.512, -3927.276, -2951.291, -2217.563, -1602.390, -1107.910, -773.972, -747.258, -748.727, -756.687, -764.372, -775.957, -793.465, -822.624 (-952.590)	Log prob: -11168.073, -7353.837, -5306.609, -3862.005, -2873.292, -2127.001, -1499.696, -993.088, -647.517, -614.844, -609.518, -611.223, -613.316, -618.098, -627.651, -644.446 (-774.591)	KLD: 21.404, 15.544, 14.955, 13.367, 12.728, 12.562, 12.131, 12.129, 11.632, 5.960, 6.795, 6.254, 5.592, 6.804, 7.954, 12.365 (177.999)	Grad: 127.612, 33.319, 26.810, 18.804, 15.620, 14.913, 13.715, 14.031, 14.150, 5.012, 5.579, 5.479, 4.313, 5.781, 7.870, 22.442
[Epoch   5 (149.70s)]	ELBO: -11193.738, -7388.037, -5361.145, -3927.010, -2950.220, -2217.804, -1602.185, -1106.778, -762.921, -741.002, -743.875, -751.801, -760.565, -771.376, -788.130, -817.628 (-1040.808)	Log prob: -11172.331, -7351.059, -5309.215, -3861.696, -2872.187, -2127.228, -1499.457, -991.936, -636.351, -608.551, -604.786, -606.525, -609.819, -613.844, -622.866, -640.363 (-862.053)	KLD: 21.412, 15.564, 14.956, 13.383, 12.716, 12.545, 12.152, 12.114, 11.727, 5.881, 6.638, 6.186, 5.470, 6.786, 7.733, 12.001 (178.755)	Grad: 126.836, 34.010, 25.987, 17.944, 16.166, 14.382, 13.820, 13.360, 14.095, 4.887, 5.597, 5.153, 4.418, 5.686, 7.681, 21.833
[Epoch   6 (156.87s)]	ELBO: -11210.656, -7405.120, -5369.838, -3936.450, -2958.541, -2225.335, -1609.522, -1112.734, -759.980, -742.158, -745.624, -753.539, -761.535, -772.671, -789.609, -817.633 (-1035.575)	Log prob: -11189.257, -7368.163, -5317.907, -3871.156, -2880.524, -2134.785, -1506.854, -997.934, -633.408, -609.782, -606.717, -608.585, -611.269, -615.791, -625.037, -641.388 (-859.905)	KLD: 21.396, 15.563, 14.971, 13.368, 12.717, 12.534, 12.117, 12.134, 11.771, 5.805, 6.531, 6.048, 5.310, 6.614, 7.692, 11.673 (175.670)	Grad: 142.565, 34.072, 25.062, 17.580, 15.287, 14.416, 13.534, 13.884, 13.932, 4.602, 5.419, 5.111, 4.235, 5.810, 7.534, 18.934
[Epoch   7 (148.86s)]	ELBO: -11174.533, -7370.894, -5348.542, -3917.891, -2942.777, -2210.639, -1595.067, -1099.203, -738.007, -722.441, -725.738, -734.149, -741.236, -752.249, -767.769, -794.908 (-963.056)	Log prob: -11153.114, -7333.915, -5296.601, -3852.582, -2864.759, -2120.087, -1492.393, -984.403, -611.372, -590.122, -586.969, -589.478, -591.488, -595.907, -603.956, -619.656 (-787.581)	KLD: 21.413, 15.564, 14.966, 13.370, 12.705, 12.535, 12.121, 12.126, 11.835, 5.684, 6.451, 5.901, 5.077, 6.595, 7.470, 11.438 (175.475)	Grad: 128.320, 33.965, 25.835, 17.265, 15.101, 14.841, 12.808, 13.666, 13.627, 4.549, 4.939, 4.841, 3.872, 5.612, 7.141, 17.658
[Epoch   8 (148.14s)]	ELBO: -11174.396, -7370.807, -5353.202, -3920.609, -2945.080, -2211.810, -1595.877, -1098.987, -731.142, -717.685, -720.729, -728.734, -736.322, -746.793, -762.470, -788.024 (-1158.480)	Log prob: -11152.981, -7333.830, -5301.250, -3855.281, -2867.031, -2121.251, -1493.216, -984.207, -604.450, -585.378, -582.075, -584.247, -586.862, -590.913, -599.135, -613.625 (-983.984)	KLD: 21.413, 15.565, 14.974, 13.376, 12.721, 12.511, 12.101, 12.121, 11.912, 5.614, 6.347, 5.834, 4.973, 6.420, 7.454, 11.065 (174.496)	Grad: 134.053, 32.309, 25.335, 17.695, 15.640, 14.939, 13.219, 14.066, 13.937, 4.457, 4.918, 4.749, 3.815, 5.420, 6.834, 16.209
[Epoch   9 (154.63s)]	ELBO: -11186.211, -7378.480, -5359.571, -3926.589, -2950.235, -2215.919, -1599.653, -1103.014, -731.522, -719.548, -722.106, -729.704, -736.657, -746.734, -762.493, -787.890 (-900.213)	Log prob: -11164.769, -7341.473, -5307.586, -3861.236, -2872.182, -2125.339, -1496.972, -988.240, -604.801, -587.316, -583.589, -585.422, -587.616, -591.257, -599.604, -614.059 (-727.273)	KLD: 21.443, 15.568, 14.974, 13.366, 12.703, 12.526, 12.101, 12.094, 11.948, 5.511, 6.284, 5.767, 4.757, 6.437, 7.411, 10.943 (172.940)	Grad: 129.777, 34.173, 25.911, 18.366, 16.221, 13.764, 12.990, 13.639, 13.132, 4.269, 4.965, 4.636, 3.560, 5.390, 7.239, 15.584
[Epoch  10 (149.39s)]	ELBO: -11177.783, -7379.523, -5355.982, -3920.334, -2941.535, -2208.203, -1592.081, -1092.324, -715.837, -703.542, -704.428, -712.567, -717.996, -728.211, -742.892, -766.347 (-870.603)	Log prob: -11156.387, -7342.575, -5304.078, -3855.052, -2863.548, -2117.712, -1489.499, -977.662, -589.214, -571.473, -566.145, -568.654, -569.498, -573.388, -580.823, -593.493 (-696.669)	KLD: 21.399, 15.546, 14.959, 13.375, 12.708, 12.504, 12.091, 12.080, 11.961, 5.444, 6.215, 5.630, 4.586, 6.324, 7.246, 10.784 (173.934)	Grad: 126.403, 34.963, 24.274, 17.100, 15.631, 14.139, 11.772, 13.031, 12.931, 4.207, 4.962, 4.445, 3.229, 5.330, 6.647, 15.133
[Epoch  11 (147.88s)]	ELBO: -11198.021, -7394.143, -5361.745, -3928.079, -2951.611, -2216.723, -1599.062, -1101.525, -723.673, -710.311, -709.288, -716.527, -721.976, -731.839, -745.959, -769.875 (-905.497)	Log prob: -11176.639, -7357.209, -5309.860, -3862.818, -2873.632, -2126.218, -1496.458, -986.836, -597.006, -578.224, -570.885, -572.498, -573.528, -577.204, -584.148, -597.469 (-732.806)	KLD: 21.387, 15.542, 14.957, 13.374, 12.717, 12.530, 12.096, 12.087, 11.978, 5.420, 6.316, 5.626, 4.419, 6.188, 7.175, 10.596 (172.691)	Grad: 143.748, 32.158, 24.763, 17.703, 15.812, 14.426, 12.816, 13.363, 13.946, 4.351, 5.079, 4.509, 3.121, 5.123, 6.659, 14.426
[Epoch  12 (153.09s)]	ELBO: -11210.503, -7401.509, -5361.418, -3926.669, -2949.740, -2215.509, -1597.821, -1099.311, -717.769, -702.183, -696.679, -703.416, -707.851, -717.101, -731.249, -753.582 (-978.986)	Log prob: -11189.095, -7364.552, -5309.493, -3861.378, -2871.751, -2124.998, -1495.215, -984.649, -591.103, -570.085, -558.048, -559.238, -559.498, -562.604, -569.660, -581.512 (-807.941)	KLD: 21.405, 15.557, 14.965, 13.365, 12.697, 12.520, 12.094, 12.059, 12.004, 5.432, 6.533, 5.546, 4.176, 6.145, 7.092, 10.480 (171.044)	Grad: 137.549, 31.791, 26.175, 17.769, 16.003, 14.942, 12.698, 12.987, 13.294, 4.357, 5.261, 4.598, 2.745, 5.039, 6.641, 14.391
[Epoch  13 (148.46s)]	ELBO: -11223.579, -7409.950, -5366.394, -3930.758, -2953.804, -2219.290, -1600.648, -1102.994, -720.376, -701.741, -690.070, -696.719, -700.948, -710.246, -723.533, -746.204 (-825.123)	Log prob: -11202.180, -7373.002, -5314.491, -3865.474, -2875.818, -2128.793, -1498.071, -988.339, -593.704, -569.601, -551.054, -552.150, -552.405, -555.660, -561.871, -574.304 (-654.575)	KLD: 21.394, 15.545, 14.967, 13.378, 12.703, 12.509, 12.081, 12.079, 12.017, 5.468, 6.876, 5.553, 3.974, 6.043, 7.076, 10.237 (170.548)	Grad: 137.845, 34.050, 25.379, 17.644, 15.875, 14.236, 12.588, 14.312, 12.949, 4.599, 5.887, 4.696, 2.457, 5.213, 6.632, 13.355
[Epoch  14 (148.71s)]	ELBO: -11218.079, -7407.172, -5366.338, -3932.485, -2955.437, -2219.887, -1601.000, -1099.867, -715.593, -690.178, -672.059, -678.056, -682.537, -691.262, -704.668, -726.648 (-864.352)	Log prob: -11196.687, -7370.230, -5314.409, -3867.202, -2877.439, -2129.381, -1498.419, -985.221, -588.938, -558.028, -532.891, -533.426, -534.121, -536.845, -543.283, -555.155 (-692.975)	KLD: 21.391, 15.548, 14.987, 13.356, 12.714, 12.510, 12.073, 12.066, 12.009, 5.495, 7.019, 5.461, 3.787, 6.002, 6.967, 10.110 (171.377)	Grad: 131.685, 33.419, 25.969, 17.678, 15.428, 13.911, 12.623, 13.260, 14.943, 4.720, 6.066, 4.715, 2.218, 5.119, 6.352, 13.291
[Epoch  15 (153.02s)]	ELBO: -11234.846, -7424.137, -5377.870, -3939.450, -2960.242, -2226.173, -1607.327, -1106.419, -720.258, -686.021, -665.297, -672.185, -675.682, -684.461, -697.774, -719.885 (-891.286)	Log prob: -11213.471, -7387.221, -5325.985, -3874.197, -2882.290, -2135.732, -1504.814, -991.876, -593.696, -553.892, -525.990, -527.478, -527.420, -530.364, -536.811, -548.880 (-720.006)	KLD: 21.375, 15.537, 14.974, 13.365, 12.699, 12.491, 12.071, 12.030, 12.019, 5.568, 7.178, 5.400, 3.555, 5.836, 6.866, 10.041 (171.279)	Grad: 144.245, 34.005, 25.591, 18.165, 15.234, 14.645, 12.090, 13.317, 14.369, 5.000, 5.808, 4.509, 2.134, 4.955, 6.510, 13.403
[Epoch  16 (154.28s)]	ELBO: -11227.014, -7404.271, -5368.884, -3931.599, -2953.429, -2218.188, -1597.774, -1097.504, -709.550, -666.123, -645.546, -652.063, -655.292, -664.418, -676.317, -697.116 (-932.231)	Log prob: -11205.562, -7367.278, -5316.914, -3866.236, -2875.359, -2127.637, -1495.162, -982.875, -582.887, -533.771, -506.123, -507.319, -507.185, -510.689, -515.943, -526.841 (-764.872)	KLD: 21.453, 15.543, 14.971, 13.398, 12.705, 12.480, 12.061, 12.018, 12.033, 5.689, 7.071, 5.321, 3.363, 5.622, 6.645, 9.900 (167.360)	Grad: 128.300, 33.602, 25.777, 18.149, 15.323, 14.659, 12.546, 13.788, 13.963, 5.209, 5.683, 4.392, 2.012, 4.529, 5.849, 11.841
[Epoch  17 (147.35s)]	ELBO: -11214.535, -7394.456, -5371.506, -3934.639, -2956.821, -2221.429, -1602.587, -1101.576, -712.722, -655.987, -637.854, -644.732, -647.561, -655.756, -667.554, -688.652 (-799.507)	Log prob: -11193.062, -7357.440, -5319.539, -3869.306, -2878.801, -2130.921, -1500.040, -987.002, -586.119, -523.546, -498.331, -499.890, -499.499, -502.215, -507.446, -518.894 (-629.441)	KLD: 21.473, 15.541, 14.953, 13.365, 12.687, 12.489, 12.039, 12.027, 12.028, 5.838, 7.081, 5.320, 3.220, 5.479, 6.567, 9.650 (170.066)	Grad: 142.225, 34.057, 25.173, 18.442, 15.597, 14.423, 12.668, 13.389, 14.173, 5.493, 5.756, 4.587, 2.102, 4.468, 5.986, 11.873
[Epoch  18 (148.10s)]	ELBO: -11183.721, -7382.859, -5360.677, -3924.185, -2946.387, -2212.304, -1591.589, -1090.505, -700.633, -623.697, -607.665, -613.695, -616.685, -624.111, -635.863, -656.710 (-775.177)	Log prob: -11162.330, -7345.939, -5308.831, -3858.981, -2868.488, -2121.950, -1489.196, -976.112, -574.229, -491.189, -468.070, -468.764, -468.651, -470.774, -475.927, -487.160 (-607.264)	KLD: 21.388, 15.532, 14.924, 13.361, 12.694, 12.456, 12.038, 12.000, 12.012, 6.104, 7.086, 5.337, 3.103, 5.302, 6.600, 9.614 (167.913)	Grad: 124.345, 34.427, 25.677, 16.867, 16.134, 14.079, 12.387, 13.905, 14.599, 5.591, 5.710, 4.310, 1.912, 3.808, 5.751, 11.993
[Epoch  19 (152.91s)]	ELBO: -11215.501, -7410.161, -5369.109, -3935.191, -2958.326, -2221.560, -1601.545, -1099.374, -707.804, -613.766, -599.516, -605.798, -607.856, -614.808, -626.397, -645.591 (-876.338)	Log prob: -11194.120, -7373.258, -5317.248, -3869.973, -2880.436, -2131.214, -1499.169, -985.014, -581.402, -481.089, -459.815, -460.669, -459.834, -461.628, -466.652, -476.299 (-709.821)	KLD: 21.381, 15.523, 14.959, 13.359, 12.669, 12.455, 12.029, 11.984, 12.041, 6.275, 7.025, 5.428, 2.893, 5.158, 6.565, 9.547 (166.517)	Grad: 137.463, 34.436, 25.401, 18.293, 16.407, 14.459, 12.705, 13.014, 14.173, 5.827, 5.231, 4.485, 1.941, 3.358, 5.699, 11.398
[Epoch  20 (149.65s)]	ELBO: -11199.104, -7382.247, -5361.917, -3924.989, -2949.170, -2214.746, -1593.674, -1091.722, -698.735, -593.231, -581.703, -587.164, -590.325, -595.954, -607.050, -625.911 (-726.590)	Log prob: -11177.675, -7345.286, -5310.016, -3859.729, -2871.247, -2124.372, -1491.263, -977.338, -572.315, -460.434, -441.973, -442.164, -442.505, -443.283, -447.982, -457.361 (-556.286)	KLD: 21.432, 15.530, 14.937, 13.361, 12.662, 12.452, 12.036, 11.975, 12.035, 6.376, 6.934, 5.269, 2.820, 4.852, 6.397, 9.481 (170.304)	Grad: 135.972, 34.356, 25.038, 17.187, 16.467, 13.962, 11.999, 12.541, 14.204, 5.949, 5.282, 4.348, 1.902, 2.785, 5.642, 10.819
[Epoch  21 (147.04s)]	ELBO: -11177.353, -7373.955, -5353.028, -3917.458, -2939.317, -2205.356, -1585.635, -1083.322, -688.349, -572.894, -562.856, -567.930, -570.243, -576.098, -587.104, -605.714 (-698.524)	Log prob: -11155.971, -7337.029, -5301.204, -3852.282, -2861.476, -2115.073, -1483.331, -969.035, -562.025, -440.115, -423.209, -422.986, -422.594, -423.958, -428.627, -437.859 (-534.606)	KLD: 21.383, 15.543, 14.903, 13.349, 12.663, 12.442, 12.022, 11.983, 12.037, 6.454, 6.869, 5.296, 2.705, 4.492, 6.337, 9.378 (163.917)	Grad: 129.481, 33.020, 25.637, 17.835, 15.362, 14.099, 13.145, 12.874, 12.949, 5.894, 4.750, 4.345, 1.867, 2.303, 5.451, 11.323
[Epoch  22 (153.38s)]	ELBO: -11184.478, -7389.293, -5363.420, -3924.939, -2946.664, -2211.768, -1591.408, -1089.626, -694.547, -571.251, -564.093, -568.538, -570.588, -575.944, -585.692, -604.677 (-788.432)	Log prob: -11163.072, -7352.345, -5311.554, -3859.713, -2868.770, -2121.448, -1489.090, -975.363, -568.279, -438.457, -424.557, -423.758, -423.231, -424.380, -427.958, -437.579 (-620.654)	KLD: 21.411, 15.535, 14.920, 13.360, 12.668, 12.425, 11.999, 11.946, 12.005, 6.525, 6.742, 5.244, 2.576, 4.207, 6.171, 9.362 (167.777)	Grad: 134.426, 35.565, 25.889, 18.093, 16.014, 13.732, 12.484, 12.611, 13.774, 5.935, 4.628, 4.449, 1.853, 2.206, 5.311, 10.801
[Epoch  23 (147.99s)]	ELBO: -11201.962, -7396.216, -5364.149, -3929.747, -2952.099, -2215.704, -1594.815, -1093.334, -697.548, -568.081, -562.079, -566.108, -568.576, -573.448, -583.153, -600.603 (-684.063)	Log prob: -11180.574, -7359.294, -5312.278, -3864.525, -2874.217, -2125.396, -1492.510, -979.088, -571.285, -435.257, -422.740, -421.461, -421.378, -422.251, -425.929, -434.134 (-520.814)	KLD: 21.382, 15.542, 14.945, 13.350, 12.662, 12.426, 11.998, 11.942, 12.015, 6.561, 6.516, 5.308, 2.551, 3.999, 6.026, 9.246 (163.248)	Grad: 140.051, 33.284, 26.674, 18.577, 16.433, 14.350, 12.207, 13.422, 13.405, 5.949, 4.458, 4.511, 1.807, 2.157, 5.211, 10.499
[Epoch  24 (146.62s)]	ELBO: -11187.435, -7380.251, -5356.543, -3921.554, -2944.785, -2209.048, -1587.649, -1084.549, -688.324, -551.936, -548.271, -552.059, -554.243, -558.999, -569.133, -586.552 (-706.504)	Log prob: -11166.045, -7343.321, -5304.681, -3856.343, -2866.905, -2118.746, -1485.346, -970.308, -562.086, -419.037, -408.974, -407.514, -407.312, -408.262, -412.366, -420.610 (-538.619)	KLD: 21.393, 15.539, 14.930, 13.347, 12.669, 12.425, 11.998, 11.940, 11.996, 6.661, 6.398, 5.248, 2.385, 3.807, 6.030, 9.174 (167.885)	Grad: 137.611, 34.207, 26.175, 18.067, 15.753, 14.364, 11.816, 12.411, 13.587, 6.033, 4.080, 4.367, 1.781, 2.114, 5.241, 10.271
[Epoch  25 (155.35s)]	ELBO: -11190.040, -7390.149, -5360.662, -3924.826, -2946.289, -2210.226, -1589.212, -1085.472, -688.329, -547.826, -545.123, -548.467, -550.120, -554.468, -564.183, -582.091 (-771.706)	Log prob: -11168.658, -7353.231, -5308.816, -3859.631, -2868.445, -2119.972, -1486.971, -971.327, -562.208, -414.965, -406.030, -404.175, -403.465, -404.134, -407.990, -416.811 (-604.002)	KLD: 21.384, 15.533, 14.932, 13.346, 12.650, 12.410, 11.985, 11.905, 11.976, 6.740, 6.232, 5.199, 2.364, 3.678, 5.860, 9.088 (167.704)	Grad: 134.683, 33.233, 25.471, 16.821, 14.966, 13.929, 11.965, 12.963, 14.201, 6.568, 4.003, 4.563, 1.790, 2.083, 4.943, 10.623
[Epoch  26 (148.63s)]	ELBO: -11203.210, -7395.640, -5368.471, -3929.864, -2954.253, -2218.427, -1598.000, -1095.294, -697.732, -552.313, -550.341, -553.858, -555.647, -559.346, -569.120, -586.817 (-693.892)	Log prob: -11181.828, -7358.715, -5316.644, -3864.703, -2876.429, -2128.194, -1495.776, -981.168, -571.630, -419.384, -411.397, -409.772, -409.223, -409.422, -413.365, -422.050 (-529.294)	KLD: 21.384, 15.543, 14.902, 13.331, 12.664, 12.410, 11.993, 11.901, 11.976, 6.827, 6.016, 5.141, 2.339, 3.500, 5.830, 9.012 (164.597)	Grad: 141.405, 33.621, 28.121, 18.610, 15.428, 13.772, 13.437, 12.478, 13.287, 6.349, 3.829, 4.434, 1.814, 2.055, 4.953, 10.162
[Epoch  27 (150.24s)]	ELBO: -11192.801, -7390.143, -5363.677, -3923.838, -2945.565, -2210.470, -1589.772, -1086.023, -687.546, -537.608, -535.515, -538.208, -539.904, -544.042, -552.979, -569.453 (-785.489)	Log prob: -11171.417, -7353.233, -5311.857, -3858.679, -2867.750, -2120.258, -1487.596, -971.958, -561.527, -404.714, -396.740, -394.359, -393.911, -394.704, -397.963, -405.486 (-621.390)	KLD: 21.387, 15.525, 14.911, 13.336, 12.656, 12.396, 11.963, 11.890, 11.954, 6.875, 5.882, 5.073, 2.145, 3.345, 5.679, 8.951 (164.099)	Grad: 133.411, 35.953, 25.741, 17.806, 15.225, 14.847, 11.987, 12.706, 12.797, 5.978, 3.657, 4.234, 1.747, 1.952, 4.662, 9.737
[Epoch  28 (153.51s)]	ELBO: -11188.582, -7385.922, -5358.676, -3922.692, -2946.020, -2210.545, -1589.313, -1084.978, -685.347, -530.893, -529.699, -531.844, -532.831, -536.553, -545.895, -561.355 (-655.576)	Log prob: -11167.236, -7349.048, -5306.864, -3857.541, -2868.221, -2120.341, -1487.147, -970.935, -559.337, -397.944, -390.958, -387.996, -386.837, -387.455, -391.245, -397.907 (-491.884)	KLD: 21.350, 15.530, 14.928, 13.343, 12.648, 12.404, 11.963, 11.877, 11.967, 6.940, 5.791, 5.108, 2.146, 3.104, 5.552, 8.798 (163.692)	Grad: 137.472, 33.026, 25.939, 17.788, 16.218, 14.340, 12.483, 13.367, 13.171, 6.430, 3.692, 4.248, 1.689, 2.021, 4.563, 9.375
[Epoch  29 (152.96s)]	ELBO: -11278.289, -7476.707, -5382.233, -3942.412, -2963.077, -2225.061, -1602.823, -1099.161, -699.514, -540.074, -537.856, -538.688, -540.220, -543.091, -552.324, -568.593 (-699.313)	Log prob: -11256.986, -7439.895, -5330.443, -3877.284, -2885.276, -2134.871, -1500.699, -985.155, -573.553, -407.022, -399.121, -394.635, -393.785, -393.539, -397.266, -404.751 (-535.782)	KLD: 21.298, 15.513, 14.980, 13.336, 12.675, 12.388, 11.936, 11.881, 11.956, 7.090, 5.683, 5.318, 2.382, 3.116, 5.506, 8.785 (163.531)	Grad: 144.440, 35.260, 28.497, 19.543, 15.657, 13.987, 12.375, 12.928, 13.780, 6.909, 3.653, 4.709, 1.748, 1.962, 4.644, 10.109
[Epoch  30 (149.25s)]	ELBO: -11181.019, -7381.939, -5354.809, -3917.675, -2939.976, -2204.177, -1582.940, -1077.953, -678.089, -512.221, -509.811, -509.695, -510.752, -513.931, -522.028, -537.479 (-612.668)	Log prob: -11159.603, -7344.997, -5302.949, -3852.474, -2862.129, -2113.930, -1480.739, -963.891, -552.091, -379.049, -371.073, -365.635, -364.730, -364.945, -367.765, -374.494 (-448.782)	KLD: 21.415, 15.526, 14.919, 13.340, 12.647, 12.400, 11.955, 11.860, 11.937, 7.173, 5.566, 5.322, 1.962, 2.964, 5.277, 8.721 (163.885)	Grad: 131.402, 35.412, 25.614, 17.758, 15.255, 14.501, 11.760, 12.303, 13.360, 6.713, 3.394, 4.464, 1.675, 1.864, 4.356, 9.241
[Epoch  31 (149.08s)]	ELBO: -11225.044, -7414.721, -5379.624, -3941.978, -2961.385, -2225.247, -1602.978, -1097.805, -698.668, -519.064, -514.993, -512.395, -513.087, -516.739, -524.656, -540.083 (-730.981)	Log prob: -11203.670, -7377.833, -5327.800, -3876.833, -2883.586, -2135.083, -1500.870, -983.808, -572.732, -385.703, -376.100, -368.054, -366.845, -367.487, -370.243, -376.923 (-567.141)	KLD: 21.379, 15.518, 14.927, 13.323, 12.650, 12.367, 11.944, 11.888, 11.939, 7.426, 5.532, 5.448, 1.901, 3.011, 5.160, 8.747 (163.840)	Grad: 137.470, 34.060, 26.186, 17.996, 15.521, 14.374, 13.083, 12.203, 13.637, 6.959, 3.530, 4.566, 1.728, 1.986, 4.225, 9.408
[Epoch  32 (153.54s)]	ELBO: -11201.908, -7393.885, -5363.745, -3927.701, -2948.423, -2214.595, -1592.869, -1086.631, -687.367, -495.080, -490.915, -488.475, -489.901, -492.470, -499.675, -514.552 (-648.427)	Log prob: -11180.553, -7357.032, -5311.957, -3862.581, -2870.664, -2124.467, -1490.804, -972.711, -561.520, -361.601, -351.940, -344.047, -343.646, -343.463, -345.638, -352.005 (-486.987)	KLD: 21.358, 15.498, 14.929, 13.337, 12.637, 12.369, 11.938, 11.855, 11.927, 7.632, 5.495, 5.454, 1.826, 2.753, 5.028, 8.511 (161.439)	Grad: 140.895, 33.869, 26.326, 18.255, 16.120, 14.555, 12.824, 12.338, 12.587, 7.201, 3.412, 4.349, 1.674, 1.963, 3.616, 9.121
[Epoch  33 (150.73s)]	ELBO: -11252.272, -7447.399, -5373.797, -3935.281, -2957.565, -2221.054, -1596.578, -1091.763, -691.533, -488.498, -484.796, -483.112, -484.052, -486.988, -492.775, -508.224 (-596.649)	Log prob: -11230.960, -7410.573, -5322.035, -3870.196, -2879.839, -2130.964, -1494.562, -977.897, -565.764, -354.936, -345.752, -338.611, -337.785, -338.007, -339.045, -346.039 (-436.651)	KLD: 21.309, 15.514, 14.940, 13.321, 12.641, 12.365, 11.926, 11.850, 11.902, 7.794, 5.481, 5.457, 1.766, 2.714, 4.749, 8.455 (159.998)	Grad: 144.900, 35.288, 27.089, 17.722, 16.171, 13.700, 11.821, 12.658, 11.864, 7.237, 3.428, 4.625, 1.758, 1.963, 2.905, 9.479
[Epoch  34 (149.61s)]	ELBO: -11180.323, -7378.474, -5359.367, -3921.807, -2943.672, -2206.447, -1582.658, -1077.400, -676.534, -466.326, -463.358, -463.358, -464.439, -467.010, -472.190, -486.854 (-623.020)	Log prob: -11158.928, -7341.554, -5307.533, -3856.636, -2865.851, -2116.250, -1480.536, -963.450, -550.673, -332.488, -324.293, -319.105, -318.479, -318.446, -319.245, -325.478 (-458.236)	KLD: 21.394, 15.529, 14.913, 13.337, 12.648, 12.377, 11.924, 11.829, 11.910, 7.977, 5.227, 5.189, 1.706, 2.604, 4.380, 8.432 (164.784)	Grad: 127.632, 33.715, 25.223, 17.870, 15.358, 13.857, 12.256, 12.537, 12.976, 8.076, 3.038, 4.293, 1.627, 1.837, 2.456, 9.165
[Epoch  35 (152.38s)]	ELBO: -11267.517, -7456.259, -5385.548, -3944.373, -2966.191, -2228.389, -1604.003, -1098.744, -698.186, -482.680, -480.786, -480.541, -481.448, -483.998, -489.253, -502.799 (-577.989)	Log prob: -11246.176, -7419.424, -5333.775, -3879.263, -2888.442, -2138.270, -1501.947, -984.846, -572.398, -348.823, -341.884, -336.413, -335.610, -335.540, -336.661, -341.902 (-419.638)	KLD: 21.336, 15.502, 14.936, 13.335, 12.640, 12.370, 11.937, 11.841, 11.891, 8.069, 5.044, 5.226, 1.710, 2.621, 4.134, 8.305 (158.351)	Grad: 141.983, 34.247, 27.470, 19.077, 16.550, 14.322, 11.889, 13.001, 13.729, 7.931, 3.103, 4.489, 1.622, 1.874, 2.383, 8.559
[Epoch  36 (153.96s)]	ELBO: -11282.051, -7470.857, -5373.197, -3935.783, -2957.265, -2219.385, -1596.976, -1094.356, -691.438, -470.705, -469.178, -469.509, -470.233, -472.644, -477.233, -491.394 (-592.783)	Log prob: -11260.714, -7433.989, -5321.343, -3870.601, -2879.450, -2129.223, -1494.885, -980.447, -565.650, -336.708, -330.315, -325.425, -324.465, -324.442, -325.141, -330.993 (-435.557)	KLD: 21.339, 15.525, 14.992, 13.327, 12.632, 12.347, 11.929, 11.818, 11.880, 8.208, 4.867, 5.221, 1.684, 2.434, 3.890, 8.308 (157.226)	Grad: 148.991, 34.075, 27.126, 18.764, 16.360, 14.259, 12.697, 13.813, 12.095, 8.121, 2.968, 4.370, 1.618, 1.837, 2.304, 8.640
[Epoch  37 (149.94s)]	ELBO: -11225.788, -7415.721, -5354.771, -3917.791, -2940.224, -2204.210, -1581.899, -1077.220, -674.224, -450.179, -449.144, -449.866, -450.727, -452.715, -456.843, -471.057 (-551.377)	Log prob: -11204.416, -7378.833, -5302.933, -3852.615, -2862.422, -2114.066, -1479.857, -963.350, -548.491, -316.083, -310.342, -306.047, -305.254, -305.002, -305.377, -311.361 (-393.125)	KLD: 21.372, 15.517, 14.953, 13.331, 12.631, 12.340, 11.899, 11.826, 11.863, 8.362, 4.707, 5.017, 1.655, 2.241, 3.753, 8.229 (158.251)	Grad: 134.398, 33.247, 26.249, 18.524, 15.504, 13.954, 12.076, 11.862, 12.913, 8.791, 2.769, 4.114, 1.589, 1.771, 2.078, 8.669
[Epoch  38 (149.38s)]	ELBO: -11257.699, -7455.891, -5364.702, -3927.300, -2948.117, -2211.039, -1587.971, -1082.547, -679.557, -451.931, -451.347, -451.919, -452.676, -454.986, -459.001, -471.973 (-708.279)	Log prob: -11236.369, -7419.042, -5312.877, -3862.146, -2870.342, -2120.914, -1485.939, -968.714, -553.859, -317.645, -312.387, -307.939, -307.181, -307.324, -307.724, -312.630 (-555.434)	KLD: 21.336, 15.514, 14.976, 13.328, 12.621, 12.350, 11.908, 11.800, 11.865, 8.588, 4.675, 5.020, 1.515, 2.167, 3.615, 8.066 (152.845)	Grad: 144.154, 35.654, 26.683, 18.700, 15.623, 13.557, 12.550, 11.829, 11.933, 8.750, 2.797, 4.320, 1.574, 1.770, 2.120, 7.714
[Epoch  39 (153.77s)]	ELBO: -11221.288, -7417.154, -5360.814, -3924.212, -2945.924, -2210.236, -1587.100, -1082.514, -680.208, -448.627, -448.406, -449.533, -450.048, -452.289, -455.930, -469.788 (-565.895)	Log prob: -11199.924, -7380.279, -5308.995, -3859.063, -2868.143, -2120.099, -1485.077, -968.706, -554.566, -314.208, -309.585, -305.876, -304.979, -304.960, -305.096, -310.912 (-406.425)	KLD: 21.366, 15.511, 14.944, 13.326, 12.636, 12.354, 11.887, 11.785, 11.835, 8.776, 4.402, 4.836, 1.412, 2.260, 3.504, 8.043 (159.470)	Grad: 141.770, 35.184, 25.593, 18.019, 15.522, 14.291, 11.909, 12.313, 12.898, 8.445, 2.624, 3.979, 1.575, 1.794, 2.069, 8.344
[Epoch  40 (152.66s)]	ELBO: -11170.937, -7376.874, -5356.196, -3917.109, -2936.065, -2198.718, -1575.192, -1069.376, -665.564, -429.582, -429.968, -431.178, -431.708, -433.743, -437.521, -450.184 (-548.930)	Log prob: -11149.570, -7340.015, -5304.438, -3852.025, -2858.367, -2108.691, -1473.281, -955.689, -540.032, -294.991, -291.182, -287.753, -286.899, -286.775, -287.306, -292.051 (-389.129)	KLD: 21.362, 15.497, 14.900, 13.324, 12.615, 12.328, 11.886, 11.775, 11.845, 9.060, 4.194, 4.639, 1.384, 2.159, 3.247, 7.918 (159.801)	Grad: 138.635, 32.717, 24.731, 17.844, 14.435, 13.169, 12.152, 11.758, 11.630, 8.326, 2.471, 3.700, 1.531, 1.800, 2.033, 7.648
[Epoch  41 (151.13s)]	ELBO: -11243.945, -7443.087, -5363.757, -3923.359, -2945.162, -2208.163, -1583.709, -1078.042, -673.892, -433.254, -433.137, -434.191, -434.891, -436.190, -440.023, -452.819 (-549.766)	Log prob: -11222.626, -7406.252, -5311.971, -3858.242, -2867.418, -2118.091, -1481.762, -964.328, -548.343, -298.400, -294.207, -290.533, -289.945, -289.206, -289.779, -294.687 (-392.653)	KLD: 21.317, 15.514, 14.956, 13.331, 12.626, 12.326, 11.877, 11.768, 11.835, 9.304, 4.076, 4.728, 1.288, 2.038, 3.259, 7.887 (157.113)	Grad: 142.516, 36.271, 26.845, 17.874, 15.813, 14.503, 11.619, 11.569, 13.076, 8.585, 2.474, 4.069, 1.557, 1.719, 2.011, 7.874
[Epoch  42 (110.41s)]	ELBO: -11204.430, -7406.492, -5361.766, -3923.354, -2944.647, -2207.290, -1584.486, -1079.024, -674.493, -428.528, -428.966, -429.895, -430.352, -432.063, -435.645, -448.665 (-667.858)	Log prob: -11183.083, -7369.639, -5309.985, -3858.242, -2866.919, -2117.242, -1482.564, -965.333, -548.985, -293.507, -289.865, -286.198, -285.406, -284.960, -285.506, -290.647 (-510.660)	KLD: 21.344, 15.511, 14.928, 13.329, 12.616, 12.320, 11.875, 11.769, 11.816, 9.513, 4.080, 4.597, 1.249, 2.157, 3.036, 7.879 (157.198)	Grad: 145.834, 33.367, 26.335, 18.183, 15.939, 12.894, 12.158, 11.476, 12.885, 9.419, 2.470, 3.909, 1.535, 1.822, 2.018, 8.038
[Epoch  43 (110.34s)]	ELBO: -11225.469, -7426.479, -5366.603, -3926.927, -2948.178, -2209.045, -1585.631, -1082.022, -677.495, -425.058, -425.255, -425.983, -425.893, -427.785, -431.786, -444.545 (-526.498)	Log prob: -11204.166, -7389.668, -5314.834, -3861.835, -2870.481, -2119.031, -1483.741, -968.399, -552.062, -289.849, -286.163, -282.313, -280.971, -280.852, -281.739, -286.805 (-366.801)	KLD: 21.297, 15.510, 14.961, 13.323, 12.606, 12.315, 11.877, 11.733, 11.809, 9.777, 3.883, 4.578, 1.253, 2.010, 3.114, 7.693 (159.696)	Grad: 137.033, 33.120, 26.751, 17.521, 15.873, 14.188, 12.814, 12.903, 12.674, 9.652, 2.358, 3.755, 1.509, 1.758, 1.990, 7.591
[Epoch  44 (111.09s)]	ELBO: -11219.740, -7418.876, -5358.050, -3918.639, -2939.684, -2202.501, -1579.403, -1075.139, -670.326, -409.116, -409.336, -410.438, -410.823, -412.660, -415.748, -428.039 (-554.017)	Log prob: -11198.431, -7382.082, -5306.281, -3853.547, -2861.992, -2112.506, -1477.547, -961.551, -544.939, -273.729, -270.120, -266.687, -265.640, -265.550, -265.745, -270.380 (-395.854)	KLD: 21.309, 15.490, 14.971, 13.320, 12.601, 12.303, 11.861, 11.734, 11.798, 10.000, 3.829, 4.536, 1.431, 1.926, 2.893, 7.657 (158.163)	Grad: 140.290, 34.313, 26.399, 19.080, 15.736, 13.542, 11.511, 12.233, 12.425, 8.721, 2.352, 3.735, 1.626, 1.684, 1.901, 7.362
[Epoch  45 (113.12s)]	ELBO: -11354.249, -7537.545, -5381.591, -3938.123, -2958.106, -2217.743, -1593.465, -1088.985, -683.696, -413.565, -413.373, -413.148, -412.949, -414.741, -417.645, -430.075 (-676.335)	Log prob: -11332.973, -7500.764, -5329.764, -3872.977, -2880.360, -2127.682, -1491.527, -975.325, -558.250, -277.907, -274.008, -269.187, -267.839, -267.848, -268.046, -272.924 (-520.213)	KLD: 21.275, 15.503, 15.050, 13.320, 12.601, 12.314, 11.876, 11.723, 11.785, 10.212, 3.707, 4.597, 1.148, 1.784, 2.706, 7.551 (156.122)	Grad: 152.060, 34.458, 28.078, 17.659, 15.482, 14.577, 12.621, 12.927, 12.212, 9.633, 2.409, 3.967, 1.542, 1.723, 1.842, 7.310
[Epoch  46 (110.58s)]	ELBO: -11268.429, -7463.261, -5371.178, -3931.474, -2953.087, -2214.135, -1589.348, -1084.341, -678.529, -402.159, -402.388, -403.324, -403.590, -404.934, -407.805, -419.832 (-505.802)	Log prob: -11247.073, -7426.411, -5319.336, -3866.319, -2875.329, -2124.073, -1487.427, -970.706, -553.109, -266.368, -263.008, -259.459, -258.604, -258.059, -258.313, -262.836 (-348.754)	KLD: 21.356, 15.496, 14.990, 13.313, 12.603, 12.304, 11.859, 11.714, 11.784, 10.371, 3.590, 4.484, 1.122, 1.889, 2.617, 7.504 (157.048)	Grad: 144.262, 35.229, 27.941, 19.182, 15.405, 13.365, 12.143, 11.703, 12.528, 9.737, 2.178, 3.560, 1.499, 1.726, 1.847, 7.073
[Epoch  47 (108.95s)]	ELBO: -11240.895, -7439.980, -5354.986, -3914.397, -2934.998, -2196.745, -1572.790, -1067.066, -660.215, -377.112, -377.411, -377.767, -377.887, -379.493, -382.474, -393.832 (-516.810)	Log prob: -11219.601, -7403.173, -5303.174, -3849.268, -2857.274, -2106.719, -1470.921, -953.461, -534.855, -241.293, -238.282, -234.241, -233.281, -233.147, -233.647, -237.695 (-360.801)	KLD: 21.301, 15.514, 14.996, 13.314, 12.601, 12.302, 11.840, 11.738, 11.755, 10.458, 3.310, 4.397, 1.080, 1.739, 2.482, 7.311 (156.009)	Grad: 142.942, 33.947, 27.050, 17.738, 16.027, 13.945, 11.959, 12.022, 12.321, 9.521, 2.084, 3.479, 1.415, 1.693, 1.819, 6.812
[Epoch  48 (112.80s)]	ELBO: -11362.896, -7550.114, -5379.603, -3934.034, -2955.056, -2216.951, -1590.788, -1085.069, -678.492, -388.535, -388.617, -388.556, -388.906, -390.124, -393.101, -404.113 (-528.395)	Log prob: -11341.576, -7513.296, -5327.726, -3868.844, -2877.265, -2126.852, -1488.838, -971.405, -553.082, -252.554, -249.343, -244.847, -244.179, -243.748, -244.052, -247.941 (-371.144)	KLD: 21.320, 15.498, 15.059, 13.314, 12.601, 12.308, 11.849, 11.713, 11.748, 10.570, 3.294, 4.434, 1.019, 1.649, 2.673, 7.123 (157.251)	Grad: 150.280, 34.929, 28.579, 18.207, 16.347, 13.713, 11.961, 12.094, 12.278, 9.386, 2.169, 3.707, 1.481, 1.728, 1.881, 6.807
[Epoch  49 (110.62s)]	ELBO: -11274.685, -7462.376, -5367.895, -3927.472, -2948.684, -2211.106, -1587.374, -1080.902, -674.253, -381.367, -382.035, -382.908, -382.993, -384.516, -386.767, -397.475 (-624.489)	Log prob: -11253.324, -7425.503, -5316.009, -3862.254, -2870.880, -2121.001, -1485.428, -967.260, -548.861, -245.360, -242.910, -239.488, -238.575, -238.497, -238.329, -242.131 (-469.689)	KLD: 21.361, 15.510, 15.016, 13.329, 12.590, 12.300, 11.839, 11.698, 11.749, 10.614, 3.118, 4.295, 0.998, 1.601, 2.420, 6.905 (154.800)	Grad: 143.282, 35.843, 26.790, 19.002, 16.797, 14.331, 12.393, 11.778, 12.035, 10.129, 2.072, 3.614, 1.444, 1.684, 1.825, 6.297
[Epoch  50 (109.40s)]	ELBO: -11241.063, -7438.405, -5363.225, -3924.672, -2945.177, -2208.812, -1584.167, -1079.176, -672.636, -376.769, -377.168, -377.842, -378.574, -379.743, -381.770, -391.295 (-565.209)	Log prob: -11219.724, -7401.563, -5311.397, -3859.541, -2867.460, -2118.818, -1482.330, -965.650, -547.384, -240.861, -238.221, -234.718, -234.276, -233.877, -233.658, -236.536 (-410.351)	KLD: 21.342, 15.495, 14.991, 13.303, 12.586, 12.277, 11.844, 11.689, 11.726, 10.657, 3.039, 4.177, 1.175, 1.568, 2.245, 6.647 (154.858)	Grad: 147.043, 34.339, 27.360, 17.622, 15.433, 13.500, 12.349, 12.139, 11.592, 9.768, 2.022, 3.376, 1.522, 1.707, 1.820, 5.148
[Epoch  51 (109.43s)]	ELBO: -11207.861, -7411.021, -5364.220, -3924.506, -2944.030, -2206.635, -1583.199, -1078.141, -671.499, -372.371, -372.714, -373.815, -373.749, -375.010, -377.513, -385.420 (-469.301)	Log prob: -11186.503, -7374.159, -5312.395, -3859.356, -2866.298, -2116.601, -1481.338, -964.581, -546.221, -236.402, -233.782, -230.831, -229.894, -229.614, -229.911, -231.539 (-319.570)	KLD: 21.359, 15.504, 14.963, 13.322, 12.584, 12.301, 11.829, 11.699, 11.717, 10.691, 2.963, 4.052, 0.870, 1.541, 2.207, 6.279 (149.732)	Grad: 143.127, 34.946, 27.221, 19.214, 16.673, 13.336, 12.281, 11.741, 12.126, 9.396, 2.009, 3.287, 1.433, 1.671, 1.790, 4.129
[Epoch  52 (112.36s)]	ELBO: -11183.397, -7382.105, -5358.698, -3918.498, -2938.084, -2198.745, -1573.708, -1068.596, -659.612, -356.967, -357.096, -358.672, -358.724, -359.735, -362.169, -369.445 (-494.869)	Log prob: -11162.042, -7345.261, -5306.931, -3853.432, -2860.423, -2108.809, -1471.955, -955.148, -534.462, -221.101, -218.453, -216.059, -215.259, -214.815, -215.070, -216.569 (-343.360)	KLD: 21.355, 15.485, 14.928, 13.300, 12.592, 12.276, 11.817, 11.695, 11.701, 10.716, 2.778, 3.971, 0.852, 1.454, 2.179, 5.778 (151.509)	Grad: 134.395, 32.057, 26.137, 17.052, 15.246, 12.908, 11.855, 11.960, 10.846, 9.644, 1.866, 3.032, 1.301, 1.568, 1.765, 3.174
[Epoch  53 (109.11s)]	ELBO: -11179.026, -7384.083, -5368.020, -3926.372, -2946.144, -2209.678, -1584.244, -1078.131, -669.951, -367.332, -367.560, -368.493, -368.933, -370.121, -372.309, -378.982 (-493.387)	Log prob: -11157.627, -7347.191, -5316.220, -3861.270, -2868.481, -2119.740, -1482.479, -964.690, -544.814, -231.464, -228.967, -226.029, -225.631, -225.334, -225.233, -226.454 (-341.203)	KLD: 21.397, 15.490, 14.913, 13.301, 12.564, 12.272, 11.828, 11.675, 11.697, 10.731, 2.725, 3.871, 0.839, 1.484, 2.289, 5.453 (152.184)	Grad: 145.182, 34.727, 25.906, 17.290, 15.108, 13.504, 11.109, 11.205, 11.678, 9.990, 1.862, 3.032, 1.407, 1.654, 1.757, 2.870
[Epoch  54 (109.73s)]	ELBO: -11143.259, -7354.263, -5342.642, -3902.824, -2922.698, -2186.466, -1562.958, -1056.769, -647.662, -342.925, -342.975, -343.985, -344.096, -344.918, -346.668, -353.125 (-474.715)	Log prob: -11121.893, -7317.400, -5290.834, -3837.706, -2844.990, -2096.486, -1461.159, -943.315, -522.498, -207.013, -204.412, -201.647, -200.989, -200.489, -200.173, -201.468 (-322.016)	KLD: 21.369, 15.495, 14.939, 13.318, 12.588, 12.269, 11.821, 11.656, 11.710, 10.748, 2.652, 3.775, 0.769, 1.321, 2.067, 5.162 (152.699)	Grad: 131.792, 35.059, 25.174, 17.596, 14.870, 13.084, 11.884, 11.812, 11.392, 10.035, 1.834, 2.886, 1.294, 1.594, 1.661, 2.558
[Epoch  55 (111.77s)]	ELBO: -11193.403, -7382.561, -5361.401, -3919.582, -2938.844, -2201.186, -1576.755, -1070.610, -662.092, -354.983, -355.184, -356.182, -356.059, -356.953, -358.949, -364.451 (-483.724)	Log prob: -11171.979, -7345.632, -5309.557, -3854.430, -2861.126, -2111.201, -1474.956, -957.178, -536.985, -219.125, -216.807, -214.118, -213.202, -212.863, -212.879, -213.481 (-331.774)	KLD: 21.431, 15.497, 14.916, 13.306, 12.569, 12.267, 11.813, 11.633, 11.676, 10.751, 2.519, 3.688, 0.792, 1.234, 1.979, 4.900 (151.950)	Grad: 146.386, 32.388, 27.005, 18.367, 15.958, 13.335, 11.480, 10.476, 11.490, 10.046, 1.810, 2.922, 1.358, 1.587, 1.679, 2.483
[Epoch  56 (110.45s)]	ELBO: -11234.666, -7415.255, -5370.450, -3926.109, -2944.358, -2206.080, -1580.990, -1074.046, -665.181, -356.311, -356.573, -357.418, -357.194, -357.775, -359.747, -365.198 (-494.808)	Log prob: -11213.184, -7378.282, -5318.505, -3860.835, -2866.499, -2115.948, -1479.037, -960.440, -539.903, -220.234, -218.060, -215.352, -214.433, -213.832, -213.778, -214.501 (-340.604)	KLD: 21.483, 15.494, 14.971, 13.325, 12.586, 12.276, 11.819, 11.652, 11.673, 10.798, 2.436, 3.553, 0.695, 1.181, 2.026, 4.728 (154.204)	Grad: 133.096, 34.005, 25.848, 17.036, 15.022, 12.787, 12.071, 11.144, 11.872, 9.492, 1.793, 2.781, 1.313, 1.546, 1.641, 2.325
[Epoch  57 (109.64s)]	ELBO: -11173.817, -7371.110, -5351.381, -3909.290, -2927.693, -2190.398, -1565.153, -1058.766, -649.473, -339.156, -339.440, -339.403, -339.488, -339.877, -341.607, -346.879 (-486.258)	Log prob: -11152.404, -7334.204, -5299.536, -3844.122, -2849.928, -2100.367, -1463.324, -945.285, -524.312, -203.214, -201.083, -197.460, -196.858, -196.060, -195.910, -196.553 (-336.679)	KLD: 21.414, 15.494, 14.938, 13.323, 12.595, 12.267, 11.797, 11.652, 11.680, 10.782, 2.414, 3.588, 0.686, 1.187, 1.880, 4.628 (149.580)	Grad: 128.921, 34.416, 24.913, 17.907, 14.245, 13.280, 12.051, 11.131, 10.748, 9.525, 1.860, 2.824, 1.313, 1.475, 1.616, 2.250
[Epoch  58 (109.72s)]	ELBO: -11204.710, -7390.500, -5365.386, -3923.261, -2941.252, -2202.791, -1577.505, -1070.617, -660.556, -348.178, -348.045, -347.288, -347.304, -348.164, -349.613, -354.711 (-675.315)	Log prob: -11183.306, -7353.587, -5313.538, -3858.102, -2863.513, -2112.788, -1475.705, -957.167, -535.433, -212.263, -209.766, -205.339, -204.705, -204.455, -204.100, -204.718 (-522.790)	KLD: 21.405, 15.510, 14.934, 13.310, 12.580, 12.264, 11.797, 11.649, 11.674, 10.791, 2.365, 3.670, 0.650, 1.109, 1.804, 4.479 (152.525)	Grad: 138.637, 34.458, 26.454, 18.455, 15.917, 13.772, 11.535, 11.649, 11.285, 9.741, 1.814, 2.979, 1.304, 1.555, 1.677, 2.360
[Epoch  59 (108.53s)]	ELBO: -11168.908, -7377.922, -5365.157, -3924.908, -2944.709, -2207.536, -1582.266, -1075.443, -667.057, -353.711, -353.952, -351.519, -351.125, -351.603, -353.272, -358.770 (-506.566)	Log prob: -11147.531, -7341.050, -5313.393, -3859.825, -2867.040, -2117.618, -1480.544, -962.064, -542.022, -217.866, -215.640, -209.400, -208.342, -207.780, -207.593, -208.471 (-354.999)	KLD: 21.376, 15.498, 14.887, 13.323, 12.586, 12.248, 11.805, 11.656, 11.657, 10.810, 2.466, 3.807, 0.665, 1.039, 1.857, 4.620 (151.566)	Grad: 136.734, 33.844, 25.731, 18.099, 15.755, 13.680, 11.923, 10.715, 11.403, 10.076, 1.765, 3.238, 1.269, 1.495, 1.636, 2.199
[Epoch  60 (109.46s)]	ELBO: -11168.475, -7381.575, -5355.632, -3914.952, -2932.490, -2194.565, -1568.561, -1061.346, -651.895, -337.516, -337.346, -332.670, -332.509, -332.374, -334.340, -339.097 (-495.275)	Log prob: -11147.174, -7344.780, -5303.896, -3849.896, -2854.868, -2104.688, -1466.869, -948.030, -526.933, -201.746, -199.291, -190.662, -189.942, -188.789, -188.937, -189.538 (-343.431)	KLD: 21.297, 15.498, 14.937, 13.323, 12.569, 12.256, 11.813, 11.624, 11.646, 10.808, 2.285, 3.953, 0.559, 1.019, 1.817, 4.157 (151.844)	Grad: 133.127, 34.871, 25.003, 17.254, 15.182, 13.403, 11.003, 12.160, 11.507, 9.461, 1.778, 3.376, 1.279, 1.464, 1.620, 2.116
No improvement after 10 epochs...
Best epoch(s): [51]	Training time(s): 8313.12s (8313.12s)	Best ELBO: -339.097 (-469.301)	Best log prob: -189.538 (-319.570)
Avg. mu: 1.929, 0.057, -0.564, 0.422, 0.348, 0.003, 0.140, -0.081, -0.221, -0.110, 0.163, 0.139, -0.028, -0.074, -0.019, -0.118, -0.049, 0.201, -0.173, 0.101, -0.267, 0.014, -0.363, -0.021, -0.259, -0.180, -0.297, 0.117, -0.541, -0.055, -0.203, -0.289, -0.608, 0.227, 0.503, -0.318, -0.045, -0.215, 0.440, 0.339, 0.706, 0.724, 1.011, -0.216, -0.894, 0.058, 0.440, -0.417, -0.086, -0.188, -0.688, 0.054, -0.462, 0.788, -0.770, 0.452, 0.598, -0.827, 0.062, -0.647, 0.613, 1.246, -0.572, 0.615
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.001, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.003, 0.003, 0.001, 0.003, 0.004, 0.003, 0.001, 0.003, 0.002, 0.006, 0.004, 0.010, 0.003, 0.003, 0.280, 0.303, 0.122, 0.727, 0.023, 0.930, 0.085, 0.073, 0.922, 0.872, 0.287, 0.963, 0.458, 0.272, 0.459, 0.541, 0.321, 0.270, 0.612, 0.383, 0.070, 0.118, 0.021, 0.033
Max. mu: 4.956, 10.006, 4.081, 5.159, 6.013, 3.828, 5.620, 4.829, 4.612, 5.104, 7.587, 7.120, 6.948, 4.543, 7.521, 6.278, 4.109, 3.869, 3.439, 3.998, 3.704, 3.915, 3.266, 3.779, 3.442, 3.453, 2.873, 4.503, 2.348, 4.980, 4.115, 3.375, 1.792, 3.768, 3.146, 4.903, 7.121, 4.129, 5.307, 2.696, 3.654, 5.376, 4.397, 4.694, 0.680, 2.308, 5.868, 2.833, 0.742, 0.876, 0.001, 0.831, 0.404, 2.563, 0.227, 2.351, 3.255, 0.161, 0.990, 0.018, 1.231, 2.117, 0.102, 1.709
Max. var: 0.000, 0.002, 0.000, 0.002, 0.003, 0.002, 0.002, 0.005, 0.004, 0.005, 0.004, 0.005, 0.012, 0.010, 0.012, 0.016, 0.043, 0.019, 0.016, 0.012, 0.038, 0.051, 0.019, 0.083, 0.023, 0.029, 0.052, 0.037, 0.013, 0.037, 0.045, 0.032, 0.021, 0.050, 0.066, 0.374, 0.047, 0.075, 0.056, 0.037, 4.597, 1.424, 0.594, 2.776, 0.304, 2.550, 0.379, 0.660, 2.719, 8.705, 1.188, 1.318, 1.070, 0.917, 0.910, 1.495, 1.178, 0.735, 1.387, 1.615, 0.266, 0.568, 0.147, 0.088
Min. mu: -3.553, -5.381, -4.414, -4.893, -6.185, -5.732, -5.323, -4.997, -4.647, -9.972, -3.869, -7.187, -9.622, -4.607, -4.622, -4.699, -4.794, -3.799, -4.104, -3.726, -3.537, -4.660, -4.194, -3.575, -3.573, -5.541, -4.050, -4.247, -3.320, -3.813, -3.879, -4.096, -3.306, -3.281, -2.282, -5.661, -3.814, -4.087, -3.351, -2.254, -4.380, -0.836, -0.013, -2.579, -4.023, -3.252, -2.546, -2.566, -0.869, -0.995, -3.069, -0.689, -1.866, -0.023, -2.147, -0.114, -0.567, -3.108, -1.229, -2.413, -0.085, 0.464, -2.296, -0.011
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.005, 0.001, 0.004, 0.009, 0.001, 0.128, 0.010, 0.001, 0.347, 0.373, 0.019, 0.525, 0.119, 0.069, 0.107, 0.116, 0.011, 0.009, 0.156, 0.048, 0.011, 0.006, 0.003, 0.002
Cov. mu:
[[0.515 0.420 0.033 0.379 -0.065 -0.046 0.058 0.008 -0.002 0.035 -0.129
  0.096 -0.014 -0.007 -0.021 -0.014 -0.009 -0.020 -0.007 -0.009 -0.006
  0.001 0.020 -0.017 -0.010 -0.015 -0.022 -0.004 0.004 -0.010 -0.018
  0.030 -0.001 -0.018 0.029 -0.040 0.019 -0.034 0.010 -0.002 0.026 0.008
  0.009 0.026 0.009 0.025 0.037 0.047 0.002 -0.009 0.002 -0.006 0.006
  0.001 -0.001 0.001 -0.023 -0.001 -0.030 0.021 0.013 0.078 0.011 0.030]
 [0.420 2.266 0.365 0.449 -0.014 -0.132 0.075 0.037 0.045 0.014 -0.326
  0.183 -0.002 0.011 -0.012 -0.011 0.008 -0.013 0.042 -0.012 -0.011 0.015
  -0.035 -0.002 0.011 -0.019 -0.054 -0.023 0.004 0.025 0.003 0.068 0.030
  -0.042 0.003 -0.085 0.002 0.014 -0.007 -0.002 0.024 0.030 0.036 0.020
  0.032 0.097 0.086 0.142 0.006 -0.001 0.009 -0.023 0.004 -0.038 0.049
  -0.027 0.022 0.025 -0.010 0.093 0.034 0.021 -0.011 0.040]
 [0.033 0.365 1.088 -0.127 0.037 0.094 0.033 0.015 0.045 -0.061 -0.000
  -0.044 -0.002 0.013 0.047 -0.021 0.011 -0.030 0.024 -0.010 -0.029 0.012
  -0.030 0.024 0.020 0.008 0.012 -0.014 0.036 0.004 0.023 -0.008 0.012
  0.009 -0.025 -0.035 -0.018 0.059 -0.010 -0.011 -0.026 0.014 0.049
  -0.010 0.020 0.045 0.017 0.072 0.002 0.015 -0.020 -0.004 -0.011 -0.037
  0.051 0.003 -0.030 -0.019 -0.002 0.083 0.032 -0.002 -0.014 -0.016]
 [0.379 0.449 -0.127 1.252 -0.053 -0.162 0.056 0.019 -0.040 0.066 -0.150
  0.129 0.005 -0.051 -0.097 0.013 0.012 -0.011 -0.039 -0.030 0.001 -0.011
  0.015 -0.010 -0.022 -0.026 -0.027 0.018 0.001 -0.046 -0.013 0.004 0.001
  -0.009 0.010 -0.029 0.033 -0.003 0.015 0.003 0.030 0.016 0.007 0.017
  -0.023 -0.003 0.095 0.034 -0.012 -0.045 -0.001 -0.020 0.018 -0.010
  0.041 -0.021 -0.015 -0.047 0.010 0.013 -0.029 0.034 0.009 0.025]
 [-0.065 -0.014 0.037 -0.053 0.846 0.039 -0.053 0.000 0.051 -0.018 -0.012
  -0.018 -0.021 -0.026 -0.005 0.019 0.008 -0.003 0.019 0.008 -0.023 0.022
  0.001 0.015 0.003 0.014 0.005 -0.005 0.000 0.007 0.002 -0.004 0.009
  0.011 -0.004 0.008 0.022 -0.007 -0.004 -0.003 0.026 0.029 0.001 0.011
  -0.012 0.026 0.030 -0.021 0.008 0.006 -0.021 0.008 0.006 0.013 0.003
  -0.006 0.009 -0.031 0.014 -0.004 -0.000 -0.001 -0.015 0.002]
 [-0.046 -0.132 0.094 -0.162 0.039 0.888 0.012 -0.064 -0.085 -0.122 0.186
  -0.143 -0.007 0.038 0.018 -0.007 -0.009 0.005 0.030 -0.005 0.009 0.003
  0.005 0.010 0.012 0.007 0.003 -0.014 -0.004 0.022 0.017 0.009 0.019
  0.019 -0.005 -0.003 0.013 0.015 -0.013 -0.012 -0.011 0.021 0.007 -0.026
  0.005 -0.009 -0.004 -0.022 -0.009 0.006 -0.019 0.004 0.031 -0.026 0.031
  -0.011 0.011 0.009 0.036 -0.017 -0.018 -0.017 -0.005 0.002]
 [0.058 0.075 0.033 0.056 -0.053 0.012 0.988 -0.008 -0.006 -0.081 -0.096
  0.083 0.027 -0.017 -0.038 -0.035 -0.007 0.009 -0.013 0.015 0.009 -0.000
  -0.023 -0.007 -0.005 0.009 -0.008 -0.001 -0.012 -0.010 -0.015 0.009
  -0.009 0.011 0.019 -0.033 -0.009 0.003 0.017 0.004 0.044 -0.002 -0.017
  0.027 -0.009 0.000 0.038 0.044 -0.018 -0.013 -0.016 0.006 0.003 0.002
  -0.021 -0.015 0.064 -0.016 -0.005 -0.005 0.005 0.006 0.010 0.000]
 [0.008 0.037 0.015 0.019 0.000 -0.064 -0.008 0.942 0.076 -0.017 -0.160
  0.023 0.010 0.011 -0.026 0.017 -0.028 -0.014 -0.003 0.016 0.027 0.018
  0.017 0.014 0.010 -0.035 -0.031 -0.026 0.029 -0.011 -0.008 -0.002
  -0.012 -0.001 -0.012 -0.011 -0.012 -0.012 -0.005 -0.018 0.004 -0.017
  0.006 0.011 -0.006 -0.003 0.041 0.051 -0.011 -0.025 -0.014 0.004 -0.002
  0.008 0.011 -0.024 0.014 -0.021 -0.013 0.034 0.006 -0.005 -0.015 -0.008]
 [-0.002 0.045 0.045 -0.040 0.051 -0.085 -0.006 0.076 0.791 -0.124 -0.018
  0.013 -0.009 0.025 -0.011 -0.006 0.009 -0.012 -0.008 0.024 0.017 0.058
  -0.042 0.035 0.028 -0.009 -0.005 -0.004 0.008 0.001 -0.000 -0.007
  -0.006 -0.021 -0.018 -0.042 0.004 0.028 0.015 0.009 0.008 -0.027 -0.008
  -0.037 0.005 -0.009 0.007 0.021 0.000 -0.012 -0.001 -0.002 0.002 -0.028
  0.027 -0.024 0.004 -0.005 -0.017 0.007 0.006 -0.014 -0.003 0.005]
 [0.035 0.014 -0.061 0.066 -0.018 -0.122 -0.081 -0.017 -0.124 1.090
  -0.077 0.177 -0.008 0.019 -0.013 0.007 -0.026 -0.003 -0.057 0.016
  -0.060 -0.039 -0.017 -0.038 -0.012 -0.026 -0.010 -0.011 0.002 -0.023
  0.007 -0.005 0.012 0.011 -0.012 -0.001 0.001 -0.013 -0.016 -0.010 0.001
  -0.006 -0.006 0.014 -0.001 -0.001 -0.039 -0.005 -0.014 -0.012 0.000
  -0.002 0.002 0.001 -0.007 0.005 -0.017 0.004 0.013 0.012 -0.001 0.024
  0.001 0.005]
 [-0.129 -0.326 -0.000 -0.150 -0.012 0.186 -0.096 -0.160 -0.018 -0.077
  1.011 -0.215 0.013 -0.011 -0.014 -0.005 0.009 0.008 0.003 0.023 0.021
  0.020 0.016 0.004 -0.005 0.014 0.013 -0.016 0.004 0.017 0.006 -0.019
  0.001 0.010 -0.016 0.009 0.009 -0.017 -0.004 0.011 0.052 -0.032 0.016
  -0.024 0.002 -0.007 -0.069 -0.100 -0.006 0.006 0.014 -0.003 0.011 0.015
  -0.017 0.020 -0.010 -0.001 0.023 0.029 -0.015 0.008 -0.023 0.027]
 [0.096 0.183 -0.044 0.129 -0.018 -0.143 0.083 0.023 0.013 0.177 -0.215
  0.941 -0.001 -0.023 -0.033 0.035 0.001 -0.015 -0.023 0.023 -0.029
  -0.041 0.008 -0.017 -0.016 -0.017 -0.020 0.032 -0.021 -0.011 0.001
  0.019 -0.008 -0.008 0.015 0.018 0.001 -0.025 0.014 0.008 -0.011 -0.014
  -0.014 0.000 0.008 0.013 0.022 0.042 0.010 0.009 -0.003 0.004 -0.003
  -0.004 0.001 0.006 0.012 -0.014 -0.004 -0.002 -0.007 0.012 0.014 -0.016]
 [-0.014 -0.002 -0.002 0.005 -0.021 -0.007 0.027 0.010 -0.009 -0.008
  0.013 -0.001 0.785 -0.007 -0.024 0.025 0.029 -0.015 0.005 0.022 -0.008
  -0.003 -0.002 -0.010 0.023 0.013 0.000 0.011 0.010 -0.013 0.010 0.013
  -0.016 0.001 -0.012 0.003 -0.035 -0.007 0.016 0.001 -0.003 -0.019
  -0.022 -0.009 -0.012 -0.004 -0.004 0.002 -0.000 0.007 0.006 0.001 0.008
  -0.003 0.008 -0.000 0.022 -0.010 -0.000 -0.011 0.013 -0.014 -0.004
  0.003]
 [-0.007 0.011 0.013 -0.051 -0.026 0.038 -0.017 0.011 0.025 0.019 -0.011
  -0.023 -0.007 0.784 0.034 0.003 0.007 0.011 0.004 0.001 0.003 0.002
  -0.013 -0.023 0.012 0.001 0.009 -0.034 0.005 0.019 0.008 -0.001 0.006
  0.001 -0.004 0.006 -0.008 0.014 -0.004 0.003 -0.021 -0.017 0.007 0.010
  0.009 0.015 -0.032 0.005 0.008 0.007 0.002 0.005 -0.000 -0.021 0.009
  -0.004 0.005 0.006 0.003 0.006 0.004 -0.009 -0.006 0.005]
 [-0.021 -0.012 0.047 -0.097 -0.005 0.018 -0.038 -0.026 -0.011 -0.013
  -0.014 -0.033 -0.024 0.034 0.852 -0.000 -0.015 0.005 -0.003 0.002 0.004
  0.022 -0.024 -0.006 -0.012 -0.003 -0.001 -0.001 0.001 -0.004 -0.008
  -0.000 -0.009 -0.023 -0.012 -0.024 0.004 0.012 -0.018 0.004 -0.008
  0.028 0.016 0.012 -0.005 0.012 -0.010 -0.015 0.002 0.002 -0.005 -0.007
  -0.006 -0.004 -0.003 0.003 -0.018 0.005 -0.009 -0.021 -0.005 -0.003
  -0.009 -0.001]
 [-0.014 -0.011 -0.021 0.013 0.019 -0.007 -0.035 0.017 -0.006 0.007
  -0.005 0.035 0.025 0.003 -0.000 0.823 0.011 -0.005 -0.009 -0.001 0.004
  0.021 0.012 0.004 -0.010 -0.005 0.013 0.003 0.010 0.023 0.002 0.013
  0.007 -0.010 0.006 0.007 0.011 0.014 0.019 -0.022 0.013 0.024 0.021
  -0.014 0.004 -0.012 0.028 -0.008 0.001 0.004 -0.009 0.002 0.006 0.011
  0.009 -0.005 0.008 -0.015 0.006 -0.019 0.007 -0.015 -0.005 0.006]
 [-0.009 0.008 0.011 0.012 0.008 -0.009 -0.007 -0.028 0.009 -0.026 0.009
  0.001 0.029 0.007 -0.015 0.011 0.803 -0.020 -0.001 -0.007 -0.016 -0.009
  0.035 0.034 -0.011 0.006 -0.002 0.002 0.005 -0.001 0.005 -0.011 -0.006
  0.011 0.001 -0.007 0.005 0.013 0.008 -0.011 -0.006 -0.009 -0.004 -0.012
  0.018 -0.003 -0.031 -0.006 -0.003 -0.004 -0.000 -0.000 -0.002 -0.010
  0.004 0.004 -0.002 0.001 0.002 0.012 -0.006 -0.024 -0.000 0.002]
 [-0.020 -0.013 -0.030 -0.011 -0.003 0.005 0.009 -0.014 -0.012 -0.003
  0.008 -0.015 -0.015 0.011 0.005 -0.005 -0.020 0.647 0.031 -0.013 0.004
  0.005 0.021 0.015 -0.014 -0.002 0.004 0.020 0.000 0.003 0.004 -0.004
  0.004 0.016 -0.000 -0.010 -0.004 -0.004 -0.003 0.001 -0.023 -0.003
  -0.017 -0.010 0.006 -0.007 -0.008 -0.010 -0.006 -0.000 -0.007 0.004
  -0.001 0.013 -0.001 0.004 0.005 0.001 -0.003 0.007 0.003 0.008 0.000
  0.007]
 [-0.007 0.042 0.024 -0.039 0.019 0.030 -0.013 -0.003 -0.008 -0.057 0.003
  -0.023 0.005 0.004 -0.003 -0.009 -0.001 0.031 0.756 -0.003 0.023 0.021
  -0.036 0.006 0.022 -0.006 -0.011 -0.006 0.002 0.010 -0.005 -0.018
  -0.015 -0.001 -0.001 -0.040 0.010 -0.009 0.008 -0.017 0.002 -0.007
  -0.017 -0.000 0.001 -0.000 -0.011 -0.003 0.003 0.003 -0.007 -0.000
  -0.001 -0.004 -0.006 0.008 -0.001 -0.007 -0.001 0.004 -0.004 -0.003
  0.002 0.002]
 [-0.009 -0.012 -0.010 -0.030 0.008 -0.005 0.015 0.016 0.024 0.016 0.023
  0.023 0.022 0.001 0.002 -0.001 -0.007 -0.013 -0.003 0.765 -0.001 0.004
  0.008 -0.012 -0.006 -0.013 0.008 0.010 0.002 0.017 0.017 -0.010 0.009
  0.001 -0.009 -0.006 -0.011 -0.006 -0.018 0.004 0.008 -0.012 0.009 0.009
  0.005 0.024 -0.003 -0.004 -0.004 0.008 0.008 -0.001 0.003 -0.001 -0.003
  -0.001 0.021 0.006 -0.002 0.001 -0.005 -0.007 -0.003 -0.004]
 [-0.006 -0.011 -0.029 0.001 -0.023 0.009 0.009 0.027 0.017 -0.060 0.021
  -0.029 -0.008 0.003 0.004 0.004 -0.016 0.004 0.023 -0.001 0.555 0.021
  -0.048 0.018 0.003 -0.017 -0.005 -0.005 0.004 0.006 0.028 -0.002 -0.000
  0.009 -0.003 0.023 0.003 -0.002 0.000 0.012 0.014 0.010 -0.019 -0.005
  0.002 -0.022 -0.001 -0.008 -0.001 0.002 0.001 -0.005 -0.000 0.003
  -0.004 -0.007 -0.001 0.010 0.002 0.011 -0.009 -0.011 -0.005 0.000]
 [0.001 0.015 0.012 -0.011 0.022 0.003 -0.000 0.018 0.058 -0.039 0.020
  -0.041 -0.003 0.002 0.022 0.021 -0.009 0.005 0.021 0.004 0.021 0.735
  -0.017 0.037 -0.008 -0.007 -0.019 -0.014 -0.004 0.003 0.013 -0.006
  -0.000 -0.005 0.006 -0.003 0.007 -0.014 -0.001 0.013 0.009 -0.011 0.009
  -0.002 0.015 -0.017 -0.022 0.013 0.002 -0.003 0.013 -0.004 0.004 0.003
  -0.002 -0.005 0.002 0.007 -0.006 0.004 0.003 -0.015 -0.001 0.004]
 [0.020 -0.035 -0.030 0.015 0.001 0.005 -0.023 0.017 -0.042 -0.017 0.016
  0.008 -0.002 -0.013 -0.024 0.012 0.035 0.021 -0.036 0.008 -0.048 -0.017
  0.577 0.040 -0.015 0.004 0.009 0.009 0.004 -0.015 0.011 0.010 -0.001
  -0.001 0.006 -0.005 -0.013 0.027 -0.009 0.001 0.010 -0.028 -0.002
  -0.005 -0.001 -0.010 -0.016 -0.016 0.003 0.006 0.008 0.005 -0.008
  -0.001 -0.001 0.005 0.004 -0.001 0.005 0.000 0.001 -0.002 0.002 0.010]
 [-0.017 -0.002 0.024 -0.010 0.015 0.010 -0.007 0.014 0.035 -0.038 0.004
  -0.017 -0.010 -0.023 -0.006 0.004 0.034 0.015 0.006 -0.012 0.018 0.037
  0.040 0.758 0.016 -0.005 -0.020 0.005 0.001 -0.019 0.010 -0.003 -0.002
  0.004 -0.007 0.012 0.004 0.021 0.005 -0.004 0.001 -0.013 0.016 -0.002
  0.016 -0.013 -0.015 -0.004 0.003 -0.006 -0.005 0.001 -0.000 -0.009
  -0.012 0.005 0.003 -0.003 0.003 0.016 -0.004 0.002 -0.005 0.007]
 [-0.010 0.011 0.020 -0.022 0.003 0.012 -0.005 0.010 0.028 -0.012 -0.005
  -0.016 0.023 0.012 -0.012 -0.010 -0.011 -0.014 0.022 -0.006 0.003
  -0.008 -0.015 0.016 0.599 -0.012 -0.018 0.009 -0.006 0.004 0.007 0.001
  -0.009 0.003 -0.001 0.007 -0.005 0.002 0.002 0.011 -0.012 0.004 -0.010
  -0.013 0.001 -0.005 -0.009 -0.000 -0.003 -0.001 -0.009 -0.000 -0.002
  -0.002 0.004 0.000 0.010 -0.010 0.002 -0.002 -0.005 -0.017 -0.001
  -0.009]
 [-0.015 -0.019 0.008 -0.026 0.014 0.007 0.009 -0.035 -0.009 -0.026 0.014
  -0.017 0.013 0.001 -0.003 -0.005 0.006 -0.002 -0.006 -0.013 -0.017
  -0.007 0.004 -0.005 -0.012 0.665 -0.020 0.001 -0.010 0.014 0.004 0.004
  0.004 0.019 -0.006 0.011 0.024 0.034 0.004 0.005 -0.002 -0.010 0.001
  -0.001 0.006 -0.012 -0.005 -0.003 -0.001 0.005 0.008 0.003 0.003 0.008
  0.001 0.007 -0.004 0.011 -0.002 0.024 0.001 -0.005 -0.001 0.001]
 [-0.022 -0.054 0.012 -0.027 0.005 0.003 -0.008 -0.031 -0.005 -0.010
  0.013 -0.020 0.000 0.009 -0.001 0.013 -0.002 0.004 -0.011 0.008 -0.005
  -0.019 0.009 -0.020 -0.018 -0.020 0.635 -0.003 0.000 -0.010 0.013 0.008
  -0.008 0.003 0.002 0.001 -0.001 0.007 -0.004 -0.002 0.002 -0.000 0.000
  0.015 -0.008 0.011 0.012 -0.012 0.007 0.011 0.004 0.004 -0.006 0.005
  -0.003 0.000 -0.007 -0.000 0.004 -0.005 -0.011 -0.009 0.004 -0.012]
 [-0.004 -0.023 -0.014 0.018 -0.005 -0.014 -0.001 -0.026 -0.004 -0.011
  -0.016 0.032 0.011 -0.034 -0.001 0.003 0.002 0.020 -0.006 0.010 -0.005
  -0.014 0.009 0.005 0.009 0.001 -0.003 0.722 -0.004 -0.012 0.018 0.001
  -0.003 -0.004 -0.006 0.013 0.016 -0.019 -0.016 0.008 -0.002 0.025
  -0.007 -0.006 0.002 -0.015 0.033 0.001 -0.004 0.006 0.003 -0.001 0.004
  0.002 0.008 -0.001 0.001 -0.011 -0.001 -0.007 0.002 -0.004 0.000 -0.001]
 [0.004 0.004 0.036 0.001 0.000 -0.004 -0.012 0.029 0.008 0.002 0.004
  -0.021 0.010 0.005 0.001 0.010 0.005 0.000 0.002 0.002 0.004 -0.004
  0.004 0.001 -0.006 -0.010 0.000 -0.004 0.390 -0.033 -0.002 -0.011
  -0.005 0.001 0.000 -0.002 -0.010 -0.001 -0.005 -0.007 0.005 -0.001
  0.004 0.005 0.004 -0.017 0.002 -0.003 -0.001 -0.002 -0.006 -0.001
  -0.002 -0.002 0.005 -0.003 -0.005 -0.001 -0.004 0.003 0.001 -0.004
  -0.002 -0.002]
 [-0.010 0.025 0.004 -0.046 0.007 0.022 -0.010 -0.011 0.001 -0.023 0.017
  -0.011 -0.013 0.019 -0.004 0.023 -0.001 0.003 0.010 0.017 0.006 0.003
  -0.015 -0.019 0.004 0.014 -0.010 -0.012 -0.033 0.714 -0.006 0.005
  -0.005 -0.010 0.003 -0.014 0.006 0.005 -0.004 -0.005 -0.007 0.033 0.010
  0.003 0.005 0.009 0.001 -0.003 -0.002 -0.005 0.005 0.003 -0.000 0.004
  0.004 -0.006 0.007 -0.016 0.003 -0.009 0.001 0.007 -0.003 0.005]
 [-0.018 0.003 0.023 -0.013 0.002 0.017 -0.015 -0.008 -0.000 0.007 0.006
  0.001 0.010 0.008 -0.008 0.002 0.005 0.004 -0.005 0.017 0.028 0.013
  0.011 0.010 0.007 0.004 0.013 0.018 -0.002 -0.006 0.649 -0.008 -0.008
  0.001 0.009 0.002 0.004 -0.021 -0.005 -0.008 0.010 0.026 0.005 -0.018
  -0.003 0.001 -0.011 -0.008 -0.001 -0.001 -0.005 0.002 -0.010 0.009
  -0.015 0.009 -0.004 -0.006 0.009 -0.002 -0.008 0.002 0.000 -0.008]
 [0.030 0.068 -0.008 0.004 -0.004 0.009 0.009 -0.002 -0.007 -0.005 -0.019
  0.019 0.013 -0.001 -0.000 0.013 -0.011 -0.004 -0.018 -0.010 -0.002
  -0.006 0.010 -0.003 0.001 0.004 0.008 0.001 -0.011 0.005 -0.008 0.608
  -0.004 -0.002 -0.005 0.000 -0.004 0.009 0.019 0.002 0.003 -0.009 0.008
  -0.021 -0.001 0.006 0.008 -0.004 0.005 -0.001 0.009 0.003 -0.000 0.002
  0.005 -0.006 0.004 0.003 -0.002 -0.000 -0.001 0.003 0.002 -0.001]
 [-0.001 0.030 0.012 0.001 0.009 0.019 -0.009 -0.012 -0.006 0.012 0.001
  -0.008 -0.016 0.006 -0.009 0.007 -0.006 0.004 -0.015 0.009 -0.000
  -0.000 -0.001 -0.002 -0.009 0.004 -0.008 -0.003 -0.005 -0.005 -0.008
  -0.004 0.247 0.049 -0.002 -0.025 -0.000 0.007 -0.002 0.001 0.007 -0.009
  -0.001 -0.007 -0.003 -0.010 0.001 0.003 0.000 -0.002 0.001 -0.002 0.007
  -0.009 0.008 -0.009 -0.004 0.001 0.003 -0.000 0.004 -0.008 -0.001
  -0.000]
 [-0.018 -0.042 0.009 -0.009 0.011 0.019 0.011 -0.001 -0.021 0.011 0.010
  -0.008 0.001 0.001 -0.023 -0.010 0.011 0.016 -0.001 0.001 0.009 -0.005
  -0.001 0.004 0.003 0.019 0.003 -0.004 0.001 -0.010 0.001 -0.002 0.049
  0.513 -0.039 0.105 0.010 -0.007 0.004 -0.011 -0.009 0.007 -0.017 0.019
  0.001 0.013 0.010 0.004 0.002 0.001 -0.006 0.001 0.005 0.001 -0.003
  0.003 0.003 -0.007 0.004 -0.010 0.003 0.000 -0.002 0.005]
 [0.029 0.003 -0.025 0.010 -0.004 -0.005 0.019 -0.012 -0.018 -0.012
  -0.016 0.015 -0.012 -0.004 -0.012 0.006 0.001 -0.000 -0.001 -0.009
  -0.003 0.006 0.006 -0.007 -0.001 -0.006 0.002 -0.006 0.000 0.003 0.009
  -0.005 -0.002 -0.039 0.384 0.011 0.001 -0.006 0.001 -0.002 0.007 0.023
  -0.006 0.025 0.001 0.008 0.005 -0.004 -0.002 0.001 0.001 -0.002 0.007
  -0.003 -0.005 -0.010 0.000 0.001 0.004 -0.008 0.003 0.003 0.005 -0.003]
 [-0.040 -0.085 -0.035 -0.029 0.008 -0.003 -0.033 -0.011 -0.042 -0.001
  0.009 0.018 0.003 0.006 -0.024 0.007 -0.007 -0.010 -0.040 -0.006 0.023
  -0.003 -0.005 0.012 0.007 0.011 0.001 0.013 -0.002 -0.014 0.002 0.000
  -0.025 0.105 0.011 0.909 0.005 0.012 0.011 0.007 -0.030 -0.027 -0.002
  -0.007 0.002 0.008 0.025 -0.006 0.001 0.001 0.006 -0.001 -0.004 0.007
  -0.004 0.020 0.007 0.009 0.023 0.002 -0.001 -0.013 -0.004 -0.001]
 [0.019 0.002 -0.018 0.033 0.022 0.013 -0.009 -0.012 0.004 0.001 0.009
  0.001 -0.035 -0.008 0.004 0.011 0.005 -0.004 0.010 -0.011 0.003 0.007
  -0.013 0.004 -0.005 0.024 -0.001 0.016 -0.010 0.006 0.004 -0.004 -0.000
  0.010 0.001 0.005 0.690 0.030 0.148 0.047 -0.090 0.141 -0.079 0.017
  0.007 -0.140 0.185 0.007 -0.019 -0.001 -0.009 -0.007 0.005 0.004 -0.015
  -0.008 0.002 -0.004 -0.022 0.013 -0.001 -0.009 0.002 0.003]
 [-0.034 0.014 0.059 -0.003 -0.007 0.015 0.003 -0.012 0.028 -0.013 -0.017
  -0.025 -0.007 0.014 0.012 0.014 0.013 -0.004 -0.009 -0.006 -0.002
  -0.014 0.027 0.021 0.002 0.034 0.007 -0.019 -0.001 0.005 -0.021 0.009
  0.007 -0.007 -0.006 0.012 0.030 0.945 0.014 0.010 0.063 -0.099 0.150
  -0.065 -0.203 -0.124 0.041 0.037 0.002 0.007 0.006 0.003 -0.028 -0.011
  0.010 0.008 0.019 -0.005 -0.016 0.019 0.001 0.004 -0.000 0.002]
 [0.010 -0.007 -0.010 0.015 -0.004 -0.013 0.017 -0.005 0.015 -0.016
  -0.004 0.014 0.016 -0.004 -0.018 0.019 0.008 -0.003 0.008 -0.018 0.000
  -0.001 -0.009 0.005 0.002 0.004 -0.004 -0.016 -0.005 -0.004 -0.005
  0.019 -0.002 0.004 0.001 0.011 0.148 0.014 0.506 -0.054 -0.256 -0.086
  0.034 -0.116 -0.021 -0.011 -0.020 0.004 -0.003 -0.004 -0.002 -0.002
  0.001 0.005 -0.005 -0.005 0.003 -0.007 -0.002 -0.000 0.000 -0.006 0.000
  -0.001]
 [-0.002 -0.002 -0.011 0.003 -0.003 -0.012 0.004 -0.018 0.009 -0.010
  0.011 0.008 0.001 0.003 0.004 -0.022 -0.011 0.001 -0.017 0.004 0.012
  0.013 0.001 -0.004 0.011 0.005 -0.002 0.008 -0.007 -0.005 -0.008 0.002
  0.001 -0.011 -0.002 0.007 0.047 0.010 -0.054 0.316 -0.084 0.011 -0.016
  -0.108 -0.000 0.060 -0.022 -0.019 0.000 0.007 -0.002 0.001 0.006 0.008
  0.005 -0.001 -0.000 0.005 0.009 0.000 0.000 0.001 -0.000 -0.004]
 [0.026 0.024 -0.026 0.030 0.026 -0.011 0.044 0.004 0.008 0.001 0.052
  -0.011 -0.003 -0.021 -0.008 0.013 -0.006 -0.023 0.002 0.008 0.014 0.009
  0.010 0.001 -0.012 -0.002 0.002 -0.002 0.005 -0.007 0.010 0.003 0.007
  -0.009 0.007 -0.030 -0.090 0.063 -0.256 -0.084 0.313 0.009 -0.027 0.047
  -0.015 -0.062 -0.005 -0.058 -0.001 -0.002 0.012 -0.005 0.006 -0.014
  -0.013 -0.004 -0.000 0.009 -0.006 0.007 0.000 -0.003 -0.001 0.008]
 [0.008 0.030 0.014 0.016 0.029 0.021 -0.002 -0.017 -0.027 -0.006 -0.032
  -0.014 -0.019 -0.017 0.028 0.024 -0.009 -0.003 -0.007 -0.012 0.010
  -0.011 -0.028 -0.013 0.004 -0.010 -0.000 0.025 -0.001 0.033 0.026
  -0.009 -0.009 0.007 0.023 -0.027 0.141 -0.099 -0.086 0.011 0.009 0.127
  -0.035 0.067 0.019 -0.004 0.099 0.041 -0.005 0.000 -0.011 -0.002 0.003
  0.009 0.003 -0.001 0.001 -0.012 -0.000 -0.006 0.002 0.006 -0.004 -0.000]
 [0.009 0.036 0.049 0.007 0.001 0.007 -0.017 0.006 -0.008 -0.006 0.016
  -0.014 -0.022 0.007 0.016 0.021 -0.004 -0.017 -0.017 0.009 -0.019 0.009
  -0.002 0.016 -0.010 0.001 0.000 -0.007 0.004 0.010 0.005 0.008 -0.001
  -0.017 -0.006 -0.002 -0.079 0.150 0.034 -0.016 -0.027 -0.035 0.087
  -0.010 -0.037 0.008 -0.020 0.002 0.006 0.003 -0.007 0.001 -0.017 0.010
  0.001 0.007 0.005 -0.011 -0.001 0.000 0.003 0.014 -0.004 0.003]
 [0.026 0.020 -0.010 0.017 0.011 -0.026 0.027 0.011 -0.037 0.014 -0.024
  0.000 -0.009 0.010 0.012 -0.014 -0.012 -0.010 -0.000 0.009 -0.005
  -0.002 -0.005 -0.002 -0.013 -0.001 0.015 -0.006 0.005 0.003 -0.018
  -0.021 -0.007 0.019 0.025 -0.007 0.017 -0.065 -0.116 -0.108 0.047 0.067
  -0.010 0.157 0.023 -0.005 0.046 0.045 0.001 -0.003 0.001 0.001 -0.009
  0.003 0.007 -0.004 0.001 -0.003 -0.005 -0.001 -0.002 0.008 0.002 0.000]
 [0.009 0.032 0.020 -0.023 -0.012 0.005 -0.009 -0.006 0.005 -0.001 0.002
  0.008 -0.012 0.009 -0.005 0.004 0.018 0.006 0.001 0.005 0.002 0.015
  -0.001 0.016 0.001 0.006 -0.008 0.002 0.004 0.005 -0.003 -0.001 -0.003
  0.001 0.001 0.002 0.007 -0.203 -0.021 -0.000 -0.015 0.019 -0.037 0.023
  0.063 0.024 -0.019 -0.011 -0.000 0.001 0.002 0.000 0.006 -0.003 0.000
  -0.001 -0.004 0.008 0.001 0.001 -0.000 -0.005 0.002 0.000]
 [0.025 0.097 0.045 -0.003 0.026 -0.009 0.000 -0.003 -0.009 -0.001 -0.007
  0.013 -0.004 0.015 0.012 -0.012 -0.003 -0.007 -0.000 0.024 -0.022
  -0.017 -0.010 -0.013 -0.005 -0.012 0.011 -0.015 -0.017 0.009 0.001
  0.006 -0.010 0.013 0.008 0.008 -0.140 -0.124 -0.011 0.060 -0.062 -0.004
  0.008 -0.005 0.024 0.137 -0.035 0.009 0.007 0.001 -0.003 0.005 0.002
  0.010 0.007 0.008 -0.004 -0.000 0.010 -0.005 -0.001 0.013 -0.002 -0.005]
 [0.037 0.086 0.017 0.095 0.030 -0.004 0.038 0.041 0.007 -0.039 -0.069
  0.022 -0.004 -0.032 -0.010 0.028 -0.031 -0.008 -0.011 -0.003 -0.001
  -0.022 -0.016 -0.015 -0.009 -0.005 0.012 0.033 0.002 0.001 -0.011 0.008
  0.001 0.010 0.005 0.025 0.185 0.041 -0.020 -0.022 -0.005 0.099 -0.020
  0.046 -0.019 -0.035 0.196 0.094 -0.010 -0.007 -0.013 -0.001 0.008 0.007
  0.004 0.000 0.007 -0.017 -0.006 0.000 0.002 0.008 -0.004 0.004]
 [0.047 0.142 0.072 0.034 -0.021 -0.022 0.044 0.051 0.021 -0.005 -0.100
  0.042 0.002 0.005 -0.015 -0.008 -0.006 -0.010 -0.003 -0.004 -0.008
  0.013 -0.016 -0.004 -0.000 -0.003 -0.012 0.001 -0.003 -0.003 -0.008
  -0.004 0.003 0.004 -0.004 -0.006 0.007 0.037 0.004 -0.019 -0.058 0.041
  0.002 0.045 -0.011 0.009 0.094 0.164 -0.007 -0.004 -0.005 -0.001 0.003
  -0.007 0.026 -0.008 0.005 -0.007 0.005 0.016 0.011 0.007 -0.001 0.002]
 [0.002 0.006 0.002 -0.012 0.008 -0.009 -0.018 -0.011 0.000 -0.014 -0.006
  0.010 -0.000 0.008 0.002 0.001 -0.003 -0.006 0.003 -0.004 -0.001 0.002
  0.003 0.003 -0.003 -0.001 0.007 -0.004 -0.001 -0.002 -0.001 0.005 0.000
  0.002 -0.002 0.001 -0.019 0.002 -0.003 0.000 -0.001 -0.005 0.006 0.001
  -0.000 0.007 -0.010 -0.007 0.010 0.005 0.001 0.001 -0.002 -0.001 0.001
  -0.001 -0.004 -0.000 -0.001 -0.001 -0.001 0.000 -0.000 -0.001]
 [-0.009 -0.001 0.015 -0.045 0.006 0.006 -0.013 -0.025 -0.012 -0.012
  0.006 0.009 0.007 0.007 0.002 0.004 -0.004 -0.000 0.003 0.008 0.002
  -0.003 0.006 -0.006 -0.001 0.005 0.011 0.006 -0.002 -0.005 -0.001
  -0.001 -0.002 0.001 0.001 0.001 -0.001 0.007 -0.004 0.007 -0.002 0.000
  0.003 -0.003 0.001 0.001 -0.007 -0.004 0.005 0.013 -0.001 0.002 -0.001
  -0.001 -0.002 0.003 -0.000 0.001 -0.000 0.001 0.002 -0.000 -0.000
  -0.001]
 [0.002 0.009 -0.020 -0.001 -0.021 -0.019 -0.016 -0.014 -0.001 0.000
  0.014 -0.003 0.006 0.002 -0.005 -0.009 -0.000 -0.007 -0.007 0.008 0.001
  0.013 0.008 -0.005 -0.009 0.008 0.004 0.003 -0.006 0.005 -0.005 0.009
  0.001 -0.006 0.001 0.006 -0.009 0.006 -0.002 -0.002 0.012 -0.011 -0.007
  0.001 0.002 -0.003 -0.013 -0.005 0.001 -0.001 0.015 -0.001 0.003 -0.004
  0.003 -0.004 -0.002 0.009 -0.000 0.002 -0.002 -0.005 0.002 -0.000]
 [-0.006 -0.023 -0.004 -0.020 0.008 0.004 0.006 0.004 -0.002 -0.002
  -0.003 0.004 0.001 0.005 -0.007 0.002 -0.000 0.004 -0.000 -0.001 -0.005
  -0.004 0.005 0.001 -0.000 0.003 0.004 -0.001 -0.001 0.003 0.002 0.003
  -0.002 0.001 -0.002 -0.001 -0.007 0.003 -0.002 0.001 -0.005 -0.002
  0.001 0.001 0.000 0.005 -0.001 -0.001 0.001 0.002 -0.001 0.004 -0.001
  0.002 -0.002 0.002 0.002 -0.001 0.000 -0.002 -0.000 0.002 0.000 -0.001]
 [0.006 0.004 -0.011 0.018 0.006 0.031 0.003 -0.002 0.002 0.002 0.011
  -0.003 0.008 -0.000 -0.006 0.006 -0.002 -0.001 -0.001 0.003 -0.000
  0.004 -0.008 -0.000 -0.002 0.003 -0.006 0.004 -0.002 -0.000 -0.010
  -0.000 0.007 0.005 0.007 -0.004 0.005 -0.028 0.001 0.006 0.006 0.003
  -0.017 -0.009 0.006 0.002 0.008 0.003 -0.002 -0.001 0.003 -0.001 0.015
  -0.006 0.004 -0.004 -0.002 0.004 0.004 0.002 -0.002 -0.005 0.000 0.001]
 [0.001 -0.038 -0.037 -0.010 0.013 -0.026 0.002 0.008 -0.028 0.001 0.015
  -0.004 -0.003 -0.021 -0.004 0.011 -0.010 0.013 -0.004 -0.001 0.003
  0.003 -0.001 -0.009 -0.002 0.008 0.005 0.002 -0.002 0.004 0.009 0.002
  -0.009 0.001 -0.003 0.007 0.004 -0.011 0.005 0.008 -0.014 0.009 0.010
  0.003 -0.003 0.010 0.007 -0.007 -0.001 -0.001 -0.004 0.002 -0.006 0.020
  -0.011 0.005 0.002 -0.006 -0.002 -0.007 -0.000 0.009 -0.002 0.000]
 [-0.001 0.049 0.051 0.041 0.003 0.031 -0.021 0.011 0.027 -0.007 -0.017
  0.001 0.008 0.009 -0.003 0.009 0.004 -0.001 -0.006 -0.003 -0.004 -0.002
  -0.001 -0.012 0.004 0.001 -0.003 0.008 0.005 0.004 -0.015 0.005 0.008
  -0.003 -0.005 -0.004 -0.015 0.010 -0.005 0.005 -0.013 0.003 0.001 0.007
  0.000 0.007 0.004 0.026 0.001 -0.002 0.003 -0.002 0.004 -0.011 0.024
  -0.011 -0.003 -0.001 0.005 0.007 -0.001 -0.007 -0.001 -0.001]
 [0.001 -0.027 0.003 -0.021 -0.006 -0.011 -0.015 -0.024 -0.024 0.005
  0.020 0.006 -0.000 -0.004 0.003 -0.005 0.004 0.004 0.008 -0.001 -0.007
  -0.005 0.005 0.005 0.000 0.007 0.000 -0.001 -0.003 -0.006 0.009 -0.006
  -0.009 0.003 -0.010 0.020 -0.008 0.008 -0.005 -0.001 -0.004 -0.001
  0.007 -0.004 -0.001 0.008 0.000 -0.008 -0.001 0.003 -0.004 0.002 -0.004
  0.005 -0.011 0.018 0.001 -0.000 -0.001 -0.002 0.001 0.008 0.001 0.001]
 [-0.023 0.022 -0.030 -0.015 0.009 0.011 0.064 0.014 0.004 -0.017 -0.010
  0.012 0.022 0.005 -0.018 0.008 -0.002 0.005 -0.001 0.021 -0.001 0.002
  0.004 0.003 0.010 -0.004 -0.007 0.001 -0.005 0.007 -0.004 0.004 -0.004
  0.003 0.000 0.007 0.002 0.019 0.003 -0.000 -0.000 0.001 0.005 0.001
  -0.004 -0.004 0.007 0.005 -0.004 -0.000 -0.002 0.002 -0.002 0.002
  -0.003 0.001 0.021 -0.001 0.002 -0.003 0.001 -0.002 -0.001 0.000]
 [-0.001 0.025 -0.019 -0.047 -0.031 0.009 -0.016 -0.021 -0.005 0.004
  -0.001 -0.014 -0.010 0.006 0.005 -0.015 0.001 0.001 -0.007 0.006 0.010
  0.007 -0.001 -0.003 -0.010 0.011 -0.000 -0.011 -0.001 -0.016 -0.006
  0.003 0.001 -0.007 0.001 0.009 -0.004 -0.005 -0.007 0.005 0.009 -0.012
  -0.011 -0.003 0.008 -0.000 -0.017 -0.007 -0.000 0.001 0.009 -0.001
  0.004 -0.006 -0.001 -0.000 -0.001 0.019 -0.001 0.004 -0.000 -0.006
  0.003 -0.001]
 [-0.030 -0.010 -0.002 0.010 0.014 0.036 -0.005 -0.013 -0.017 0.013 0.023
  -0.004 -0.000 0.003 -0.009 0.006 0.002 -0.003 -0.001 -0.002 0.002
  -0.006 0.005 0.003 0.002 -0.002 0.004 -0.001 -0.004 0.003 0.009 -0.002
  0.003 0.004 0.004 0.023 -0.022 -0.016 -0.002 0.009 -0.006 -0.000 -0.001
  -0.005 0.001 0.010 -0.006 0.005 -0.001 -0.000 -0.000 0.000 0.004 -0.002
  0.005 -0.001 0.002 -0.001 0.013 -0.001 -0.003 -0.005 -0.002 -0.000]
 [0.021 0.093 0.083 0.013 -0.004 -0.017 -0.005 0.034 0.007 0.012 0.029
  -0.002 -0.011 0.006 -0.021 -0.019 0.012 0.007 0.004 0.001 0.011 0.004
  0.000 0.016 -0.002 0.024 -0.005 -0.007 0.003 -0.009 -0.002 -0.000
  -0.000 -0.010 -0.008 0.002 0.013 0.019 -0.000 0.000 0.007 -0.006 0.000
  -0.001 0.001 -0.005 0.000 0.016 -0.001 0.001 0.002 -0.002 0.002 -0.007
  0.007 -0.002 -0.003 0.004 -0.001 0.027 0.003 -0.000 -0.002 0.003]
 [0.013 0.034 0.032 -0.029 -0.000 -0.018 0.005 0.006 0.006 -0.001 -0.015
  -0.007 0.013 0.004 -0.005 0.007 -0.006 0.003 -0.004 -0.005 -0.009 0.003
  0.001 -0.004 -0.005 0.001 -0.011 0.002 0.001 0.001 -0.008 -0.001 0.004
  0.003 0.003 -0.001 -0.001 0.001 0.000 0.000 0.000 0.002 0.003 -0.002
  -0.000 -0.001 0.002 0.011 -0.001 0.002 -0.002 -0.000 -0.002 -0.000
  -0.001 0.001 0.001 -0.000 -0.003 0.003 0.010 0.004 -0.001 0.002]
 [0.078 0.021 -0.002 0.034 -0.001 -0.017 0.006 -0.005 -0.014 0.024 0.008
  0.012 -0.014 -0.009 -0.003 -0.015 -0.024 0.008 -0.003 -0.007 -0.011
  -0.015 -0.002 0.002 -0.017 -0.005 -0.009 -0.004 -0.004 0.007 0.002
  0.003 -0.008 0.000 0.003 -0.013 -0.009 0.004 -0.006 0.001 -0.003 0.006
  0.014 0.008 -0.005 0.013 0.008 0.007 0.000 -0.000 -0.005 0.002 -0.005
  0.009 -0.007 0.008 -0.002 -0.006 -0.005 -0.000 0.004 0.029 0.000 0.007]
 [0.011 -0.011 -0.014 0.009 -0.015 -0.005 0.010 -0.015 -0.003 0.001
  -0.023 0.014 -0.004 -0.006 -0.009 -0.005 -0.000 0.000 0.002 -0.003
  -0.005 -0.001 0.002 -0.005 -0.001 -0.001 0.004 0.000 -0.002 -0.003
  0.000 0.002 -0.001 -0.002 0.005 -0.004 0.002 -0.000 0.000 -0.000 -0.001
  -0.004 -0.004 0.002 0.002 -0.002 -0.004 -0.001 -0.000 -0.000 0.002
  0.000 0.000 -0.002 -0.001 0.001 -0.001 0.003 -0.002 -0.002 -0.001 0.000
  0.005 -0.001]
 [0.030 0.040 -0.016 0.025 0.002 0.002 0.000 -0.008 0.005 0.005 0.027
  -0.016 0.003 0.005 -0.001 0.006 0.002 0.007 0.002 -0.004 0.000 0.004
  0.010 0.007 -0.009 0.001 -0.012 -0.001 -0.002 0.005 -0.008 -0.001
  -0.000 0.005 -0.003 -0.001 0.003 0.002 -0.001 -0.004 0.008 -0.000 0.003
  0.000 0.000 -0.005 0.004 0.002 -0.001 -0.001 -0.000 -0.001 0.001 0.000
  -0.001 0.001 0.000 -0.001 -0.000 0.003 0.002 0.007 -0.001 0.009]]
