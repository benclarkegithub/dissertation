Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            786,688
├─Linear: 1-2                            65,792
=================================================================
Total params: 852,480
Trainable params: 852,480
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Latents to Latents
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToLatentsComplicated              --
├─Linear: 1-1                            1,700
├─Linear: 1-2                            10,100
├─Linear: 1-3                            10,100
├─Linear: 1-4                            404
├─Linear: 1-5                            404
=================================================================
Total params: 22,708
Trainable params: 22,708
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            65,792
├─Linear: 1-2                            789,504
=================================================================
Total params: 855,296
Trainable params: 855,296
Non-trainable params: 0
=================================================================
Training model 1/16...
[Epoch   1 (12.18s)]	ELBO: -17598.441 (-13818.969)	Log prob: -17581.562 (-13800.587)	KLD: 16.878 (18.382)	Grad: 61.720
[Epoch   2 (12.12s)]	ELBO: -13638.774 (-13576.557)	Log prob: -13621.121 (-13559.471)	KLD: 17.653 (17.086)	Grad: 43.569
[Epoch   3 (11.83s)]	ELBO: -12461.671 (-12191.383)	Log prob: -12443.006 (-12172.842)	KLD: 18.663 (18.541)	Grad: 48.721
[Epoch   4 (13.06s)]	ELBO: -11965.389 (-11857.536)	Log prob: -11946.613 (-11838.536)	KLD: 18.776 (18.999)	Grad: 45.179
[Epoch   5 (12.29s)]	ELBO: -11780.500 (-11681.027)	Log prob: -11761.247 (-11662.343)	KLD: 19.250 (18.684)	Grad: 48.564
[Epoch   6 (12.24s)]	ELBO: -11621.618 (-11488.236)	Log prob: -11602.137 (-11468.674)	KLD: 19.482 (19.561)	Grad: 51.794
[Epoch   7 (12.77s)]	ELBO: -11429.169 (-11344.667)	Log prob: -11409.274 (-11323.771)	KLD: 19.897 (20.896)	Grad: 58.368
[Epoch   8 (11.07s)]	ELBO: -11314.854 (-11285.181)	Log prob: -11294.313 (-11264.373)	KLD: 20.541 (20.808)	Grad: 57.064
[Epoch   9 (10.44s)]	ELBO: -11267.361 (-11468.978)	Log prob: -11246.613 (-11447.159)	KLD: 20.747 (21.819)	Grad: 63.672
[Epoch  10 (10.35s)]	ELBO: -11213.043 (-11526.319)	Log prob: -11191.881 (-11505.612)	KLD: 21.162 (20.708)	Grad: 65.512
Training model 2/16...
[Epoch   1 (15.81s)]	ELBO: -9716.628 (-8602.634)	Log prob: -9676.538 (-8561.191)	KLD: 18.800 (41.444)	Grad: 26.117
[Epoch   2 (15.93s)]	ELBO: -8560.760 (-8344.568)	Log prob: -8520.277 (-8305.905)	KLD: 19.241 (38.663)	Grad: 20.110
[Epoch   3 (17.73s)]	ELBO: -8235.011 (-7935.390)	Log prob: -8194.790 (-7897.016)	KLD: 18.899 (38.375)	Grad: 21.020
[Epoch   4 (19.01s)]	ELBO: -8065.038 (-8044.949)	Log prob: -8025.278 (-8005.079)	KLD: 18.841 (39.870)	Grad: 19.763
[Epoch   5 (19.13s)]	ELBO: -8161.872 (-8260.390)	Log prob: -8122.078 (-8219.377)	KLD: 18.613 (41.013)	Grad: 21.283
[Epoch   6 (19.09s)]	ELBO: -8001.874 (-7831.625)	Log prob: -7962.255 (-7793.013)	KLD: 18.366 (38.612)	Grad: 22.408
[Epoch   7 (19.28s)]	ELBO: -7914.754 (-7976.706)	Log prob: -7875.462 (-7938.422)	KLD: 18.181 (38.284)	Grad: 20.948
[Epoch   8 (19.16s)]	ELBO: -7897.735 (-7970.933)	Log prob: -7858.747 (-7930.906)	KLD: 18.357 (40.027)	Grad: 19.739
[Epoch   9 (19.27s)]	ELBO: -8131.805 (-7829.698)	Log prob: -8091.527 (-7788.896)	KLD: 18.550 (40.802)	Grad: 26.488
[Epoch  10 (19.32s)]	ELBO: -7891.625 (-7872.519)	Log prob: -7852.617 (-7832.557)	KLD: 17.914 (39.962)	Grad: 20.956
Training model 3/16...
[Epoch   1 (26.73s)]	ELBO: -7467.821 (-7122.050)	Log prob: -7404.649 (-7065.771)	KLD: 25.147 (56.279)	Grad: 24.979
[Epoch   2 (26.77s)]	ELBO: -7139.623 (-6889.028)	Log prob: -7083.580 (-6831.515)	KLD: 17.611 (57.513)	Grad: 18.026
[Epoch   3 (27.32s)]	ELBO: -6776.850 (-6517.403)	Log prob: -6719.821 (-6462.909)	KLD: 18.419 (54.494)	Grad: 13.918
[Epoch   4 (27.17s)]	ELBO: -6514.329 (-6290.921)	Log prob: -6457.223 (-6233.417)	KLD: 18.255 (57.503)	Grad: 17.980
[Epoch   5 (27.24s)]	ELBO: -6319.928 (-6063.164)	Log prob: -6262.456 (-6005.433)	KLD: 18.191 (57.731)	Grad: 18.009
[Epoch   6 (27.34s)]	ELBO: -6159.026 (-6049.792)	Log prob: -6101.829 (-5992.335)	KLD: 17.785 (57.456)	Grad: 17.267
[Epoch   7 (27.58s)]	ELBO: -6078.401 (-5932.130)	Log prob: -6020.386 (-5873.516)	KLD: 17.900 (58.614)	Grad: 16.841
[Epoch   8 (27.64s)]	ELBO: -5991.568 (-5895.412)	Log prob: -5934.197 (-5838.966)	KLD: 18.047 (56.445)	Grad: 17.462
[Epoch   9 (27.61s)]	ELBO: -5903.004 (-5777.704)	Log prob: -5845.318 (-5716.864)	KLD: 17.833 (60.840)	Grad: 16.318
[Epoch  10 (28.10s)]	ELBO: -6168.185 (-5927.460)	Log prob: -6107.727 (-5865.609)	KLD: 17.835 (61.852)	Grad: 19.622
Training model 4/16...
[Epoch   1 (36.28s)]	ELBO: -5952.438 (-5674.093)	Log prob: -5876.894 (-5603.030)	KLD: 16.961 (71.064)	Grad: 23.479
[Epoch   2 (36.01s)]	ELBO: -5644.165 (-5450.786)	Log prob: -5572.150 (-5377.916)	KLD: 15.231 (72.870)	Grad: 14.411
[Epoch   3 (35.84s)]	ELBO: -5445.090 (-5266.686)	Log prob: -5373.036 (-5193.839)	KLD: 15.852 (72.848)	Grad: 17.432
[Epoch   4 (28.18s)]	ELBO: -5310.627 (-5123.004)	Log prob: -5238.175 (-5049.991)	KLD: 16.380 (73.013)	Grad: 19.945
[Epoch   5 (33.34s)]	ELBO: -5109.659 (-4853.515)	Log prob: -5037.538 (-4782.400)	KLD: 16.549 (71.115)	Grad: 18.536
[Epoch   6 (61.55s)]	ELBO: -4953.361 (-4939.077)	Log prob: -4880.549 (-4867.968)	KLD: 16.796 (71.109)	Grad: 21.057
[Epoch   7 (61.29s)]	ELBO: -4800.662 (-4792.230)	Log prob: -4728.026 (-4720.099)	KLD: 16.782 (72.131)	Grad: 15.764
[Epoch   8 (64.33s)]	ELBO: -4805.203 (-4646.247)	Log prob: -4731.009 (-4571.440)	KLD: 16.955 (74.808)	Grad: 23.705
[Epoch   9 (64.96s)]	ELBO: -4584.090 (-4578.624)	Log prob: -4511.292 (-4505.970)	KLD: 16.817 (72.654)	Grad: 15.738
[Epoch  10 (64.15s)]	ELBO: -4665.473 (-4528.496)	Log prob: -4593.845 (-4458.609)	KLD: 16.727 (69.887)	Grad: 16.739
Training model 5/16...
[Epoch   1 (83.88s)]	ELBO: -4833.955 (-4404.411)	Log prob: -4740.204 (-4317.910)	KLD: 21.121 (86.501)	Grad: 16.226
[Epoch   2 (83.57s)]	ELBO: -4430.297 (-4182.702)	Log prob: -4342.701 (-4095.208)	KLD: 15.631 (87.494)	Grad: 5.432
[Epoch   3 (81.11s)]	ELBO: -4188.329 (-4633.582)	Log prob: -4101.094 (-4543.853)	KLD: 15.871 (89.729)	Grad: 7.198
[Epoch   4 (78.96s)]	ELBO: -4082.031 (-3865.177)	Log prob: -3994.170 (-3777.538)	KLD: 16.582 (87.639)	Grad: 8.983
[Epoch   5 (77.50s)]	ELBO: -3970.208 (-3908.395)	Log prob: -3882.576 (-3819.627)	KLD: 15.935 (88.767)	Grad: 8.305
[Epoch   6 (77.29s)]	ELBO: -3948.838 (-4013.484)	Log prob: -3861.478 (-3928.515)	KLD: 15.774 (84.969)	Grad: 6.443
[Epoch   7 (79.66s)]	ELBO: -3999.487 (-3843.392)	Log prob: -3911.957 (-3756.447)	KLD: 16.101 (86.945)	Grad: 7.524
[Epoch   8 (78.51s)]	ELBO: -3880.509 (-3772.459)	Log prob: -3794.014 (-3689.349)	KLD: 16.031 (83.109)	Grad: 6.744
[Epoch   9 (76.75s)]	ELBO: -3848.618 (-3860.492)	Log prob: -3761.179 (-3770.679)	KLD: 16.185 (89.813)	Grad: 9.583
[Epoch  10 (76.04s)]	ELBO: -3713.375 (-3821.419)	Log prob: -3625.432 (-3733.087)	KLD: 16.023 (88.332)	Grad: 8.842
Training model 6/16...
[Epoch   1 (92.45s)]	ELBO: -3946.368 (-3581.234)	Log prob: -3841.343 (-3478.257)	KLD: 17.467 (102.977)	Grad: 51.074
[Epoch   2 (96.64s)]	ELBO: -3690.949 (-3542.609)	Log prob: -3589.433 (-3442.669)	KLD: 14.053 (99.939)	Grad: 24.158
[Epoch   3 (95.43s)]	ELBO: -3561.273 (-3431.637)	Log prob: -3459.396 (-3330.489)	KLD: 13.427 (101.148)	Grad: 10.496
[Epoch   4 (93.99s)]	ELBO: -3437.688 (-3337.810)	Log prob: -3335.394 (-3235.701)	KLD: 13.802 (102.110)	Grad: 10.387
[Epoch   5 (93.34s)]	ELBO: -3506.338 (-3331.558)	Log prob: -3405.090 (-3229.622)	KLD: 12.688 (101.936)	Grad: 15.165
[Epoch   6 (93.88s)]	ELBO: -3431.274 (-3454.247)	Log prob: -3330.264 (-3351.119)	KLD: 12.938 (103.128)	Grad: 10.079
[Epoch   7 (96.80s)]	ELBO: -3404.119 (-3366.189)	Log prob: -3300.996 (-3257.489)	KLD: 15.008 (108.700)	Grad: 12.526
[Epoch   8 (94.04s)]	ELBO: -3313.051 (-3250.056)	Log prob: -3209.714 (-3146.615)	KLD: 14.630 (103.442)	Grad: 8.618
[Epoch   9 (93.08s)]	ELBO: -3253.229 (-3214.421)	Log prob: -3150.373 (-3114.635)	KLD: 14.517 (99.786)	Grad: 7.022
[Epoch  10 (94.31s)]	ELBO: -3195.315 (-3164.336)	Log prob: -3092.901 (-3060.958)	KLD: 14.639 (103.378)	Grad: 7.871
Training model 7/16...
[Epoch   1 (113.73s)]	ELBO: -3735.039 (-3293.320)	Log prob: -3613.909 (-3171.547)	KLD: 16.617 (121.773)	Grad: 35.063
[Epoch   2 (113.93s)]	ELBO: -3330.862 (-3191.411)	Log prob: -3213.438 (-3074.974)	KLD: 13.966 (116.437)	Grad: 9.972
[Epoch   3 (116.33s)]	ELBO: -3243.811 (-4269.238)	Log prob: -3125.403 (-4148.292)	KLD: 14.328 (120.947)	Grad: 6.018
[Epoch   4 (111.79s)]	ELBO: -3173.823 (-3071.386)	Log prob: -3054.173 (-2956.561)	KLD: 14.645 (114.825)	Grad: 2.886
[Epoch   5 (112.21s)]	ELBO: -3076.322 (-3077.217)	Log prob: -2957.912 (-2956.385)	KLD: 14.284 (120.831)	Grad: 2.544
[Epoch   6 (112.75s)]	ELBO: -3011.799 (-2920.958)	Log prob: -2893.654 (-2802.075)	KLD: 14.358 (118.883)	Grad: 2.520
[Epoch   7 (115.13s)]	ELBO: -2864.090 (-2808.120)	Log prob: -2746.056 (-2690.789)	KLD: 14.521 (117.331)	Grad: 2.789
[Epoch   8 (114.25s)]	ELBO: -2752.613 (-2681.267)	Log prob: -2633.922 (-2561.718)	KLD: 15.030 (119.549)	Grad: 2.760
[Epoch   9 (113.04s)]	ELBO: -2692.769 (-2632.840)	Log prob: -2573.591 (-2514.421)	KLD: 15.309 (118.419)	Grad: 2.977
[Epoch  10 (111.82s)]	ELBO: -2639.463 (-2580.245)	Log prob: -2520.296 (-2460.810)	KLD: 15.262 (119.435)	Grad: 2.826
Training model 8/16...
[Epoch   1 (137.22s)]	ELBO: -2687.659 (-2557.621)	Log prob: -2552.186 (-2425.116)	KLD: 16.562 (132.505)	Grad: 7.448
[Epoch   2 (131.78s)]	ELBO: -2527.566 (-2440.190)	Log prob: -2393.513 (-2306.403)	KLD: 15.069 (133.787)	Grad: 3.547
[Epoch   3 (131.86s)]	ELBO: -2400.374 (-2316.715)	Log prob: -2265.561 (-2182.296)	KLD: 15.705 (134.419)	Grad: 3.557
[Epoch   4 (136.68s)]	ELBO: -2289.820 (-2214.058)	Log prob: -2154.591 (-2082.231)	KLD: 15.904 (131.827)	Grad: 2.873
[Epoch   5 (132.84s)]	ELBO: -2214.790 (-2131.485)	Log prob: -2079.195 (-1995.933)	KLD: 16.438 (135.552)	Grad: 2.703
[Epoch   6 (133.09s)]	ELBO: -2140.760 (-2051.049)	Log prob: -2004.917 (-1913.350)	KLD: 16.556 (137.699)	Grad: 3.127
[Epoch   7 (134.83s)]	ELBO: -2057.228 (-2007.099)	Log prob: -1921.865 (-1874.053)	KLD: 16.469 (133.046)	Grad: 2.236
[Epoch   8 (133.55s)]	ELBO: -1990.046 (-1906.458)	Log prob: -1854.589 (-1770.634)	KLD: 16.752 (135.824)	Grad: 2.177
[Epoch   9 (134.85s)]	ELBO: -1942.054 (-1883.690)	Log prob: -1806.523 (-1747.402)	KLD: 16.792 (136.288)	Grad: 2.242
[Epoch  10 (132.35s)]	ELBO: -1910.998 (-1894.025)	Log prob: -1775.454 (-1757.201)	KLD: 16.861 (136.824)	Grad: 2.186
Training model 9/16...
[Epoch   1 (152.32s)]	ELBO: -2024.887 (-1839.559)	Log prob: -1875.436 (-1688.400)	KLD: 14.823 (151.159)	Grad: 12.196
[Epoch   2 (156.51s)]	ELBO: -1801.721 (-1708.253)	Log prob: -1652.461 (-1558.735)	KLD: 14.881 (149.518)	Grad: 5.264
[Epoch   3 (154.67s)]	ELBO: -1709.370 (-1676.479)	Log prob: -1559.518 (-1526.761)	KLD: 15.360 (149.718)	Grad: 4.584
[Epoch   4 (152.74s)]	ELBO: -1655.389 (-1602.422)	Log prob: -1505.341 (-1454.559)	KLD: 15.705 (147.863)	Grad: 4.628
[Epoch   5 (155.57s)]	ELBO: -1630.543 (-1557.981)	Log prob: -1479.477 (-1406.998)	KLD: 15.755 (150.983)	Grad: 3.696
[Epoch   6 (155.34s)]	ELBO: -1594.697 (-1559.826)	Log prob: -1444.559 (-1410.248)	KLD: 16.056 (149.577)	Grad: 2.695
[Epoch   7 (154.56s)]	ELBO: -1587.358 (-1562.151)	Log prob: -1437.463 (-1411.104)	KLD: 16.141 (151.048)	Grad: 2.664
[Epoch   8 (153.13s)]	ELBO: -1588.693 (-1546.124)	Log prob: -1439.130 (-1398.238)	KLD: 16.151 (147.886)	Grad: 2.093
[Epoch   9 (156.85s)]	ELBO: -1572.487 (-1535.670)	Log prob: -1423.035 (-1389.481)	KLD: 16.214 (146.189)	Grad: 1.905
[Epoch  10 (157.47s)]	ELBO: -1577.293 (-1512.067)	Log prob: -1427.956 (-1366.940)	KLD: 16.193 (145.128)	Grad: 2.048
Training model 10/16...
[Epoch   1 (164.69s)]	ELBO: -1753.595 (-1499.071)	Log prob: -1583.896 (-1333.687)	KLD: 19.765 (165.384)	Grad: 22.957
[Epoch   2 (143.02s)]	ELBO: -1497.203 (-1457.995)	Log prob: -1333.205 (-1292.726)	KLD: 15.544 (165.268)	Grad: 3.604
[Epoch   3 (146.81s)]	ELBO: -1461.358 (-1446.696)	Log prob: -1298.193 (-1282.195)	KLD: 15.090 (164.501)	Grad: 2.822
[Epoch   4 (145.49s)]	ELBO: -1449.735 (-1476.974)	Log prob: -1286.394 (-1312.560)	KLD: 15.007 (164.414)	Grad: 2.613
[Epoch   5 (144.54s)]	ELBO: -1417.974 (-1392.460)	Log prob: -1255.329 (-1231.391)	KLD: 14.887 (161.069)	Grad: 2.407
[Epoch   6 (148.31s)]	ELBO: -1396.307 (-1355.964)	Log prob: -1233.969 (-1196.834)	KLD: 14.876 (159.130)	Grad: 2.273
[Epoch   7 (146.17s)]	ELBO: -1374.795 (-1315.719)	Log prob: -1212.895 (-1155.453)	KLD: 14.949 (160.266)	Grad: 2.303
[Epoch   8 (144.69s)]	ELBO: -1335.082 (-1288.804)	Log prob: -1173.458 (-1124.928)	KLD: 15.027 (163.875)	Grad: 2.220
[Epoch   9 (148.52s)]	ELBO: -1300.783 (-1285.540)	Log prob: -1139.442 (-1122.758)	KLD: 15.134 (162.782)	Grad: 2.319
[Epoch  10 (143.62s)]	ELBO: -1270.901 (-1252.221)	Log prob: -1109.782 (-1092.187)	KLD: 15.283 (160.034)	Grad: 2.362
Training model 11/16...
[Epoch   1 (164.73s)]	ELBO: -1402.093 (-1275.749)	Log prob: -1225.349 (-1097.847)	KLD: 15.824 (177.902)	Grad: 10.618
[Epoch   2 (166.62s)]	ELBO: -1210.794 (-1184.413)	Log prob: -1034.484 (-1008.262)	KLD: 15.177 (176.152)	Grad: 3.308
[Epoch   3 (165.70s)]	ELBO: -1159.339 (-1104.130)	Log prob: -982.520 (-928.927)	KLD: 15.324 (175.203)	Grad: 2.759
[Epoch   4 (163.77s)]	ELBO: -1110.989 (-1528.933)	Log prob: -934.642 (-1351.533)	KLD: 15.085 (177.400)	Grad: 3.248
[Epoch   5 (139.08s)]	ELBO: -1072.243 (-973.050)	Log prob: -895.367 (-795.645)	KLD: 15.663 (177.406)	Grad: 3.264
[Epoch   6 (116.17s)]	ELBO: -981.004 (-962.504)	Log prob: -804.259 (-785.588)	KLD: 15.617 (176.916)	Grad: 2.359
[Epoch   7 (116.49s)]	ELBO: -963.334 (-946.721)	Log prob: -786.686 (-769.943)	KLD: 15.711 (176.777)	Grad: 2.237
[Epoch   8 (116.27s)]	ELBO: -936.579 (-920.571)	Log prob: -759.978 (-743.488)	KLD: 15.649 (177.083)	Grad: 1.797
[Epoch   9 (116.60s)]	ELBO: -925.287 (-972.722)	Log prob: -748.877 (-798.173)	KLD: 15.670 (174.549)	Grad: 1.578
[Epoch  10 (116.62s)]	ELBO: -907.750 (-890.303)	Log prob: -731.720 (-714.948)	KLD: 15.693 (175.354)	Grad: 1.287
Training model 12/16...
[Epoch   1 (132.97s)]	ELBO: -1105.777 (-987.638)	Log prob: -912.355 (-793.532)	KLD: 16.615 (194.106)	Grad: 11.008
[Epoch   2 (131.00s)]	ELBO: -884.861 (-861.335)	Log prob: -693.741 (-670.899)	KLD: 15.745 (190.436)	Grad: 3.778
[Epoch   3 (130.48s)]	ELBO: -836.119 (-833.821)	Log prob: -645.287 (-645.913)	KLD: 15.647 (187.907)	Grad: 3.024
[Epoch   4 (130.68s)]	ELBO: -807.856 (-801.784)	Log prob: -617.604 (-612.971)	KLD: 15.490 (188.814)	Grad: 2.804
[Epoch   5 (132.25s)]	ELBO: -782.716 (-777.671)	Log prob: -592.934 (-585.607)	KLD: 15.523 (192.065)	Grad: 2.382
[Epoch   6 (131.08s)]	ELBO: -751.088 (-772.416)	Log prob: -561.371 (-581.219)	KLD: 15.631 (191.198)	Grad: 2.196
[Epoch   7 (131.11s)]	ELBO: -734.741 (-717.076)	Log prob: -545.243 (-528.677)	KLD: 15.592 (188.399)	Grad: 1.976
[Epoch   8 (132.11s)]	ELBO: -714.105 (-715.769)	Log prob: -524.967 (-526.042)	KLD: 15.607 (189.727)	Grad: 1.940
[Epoch   9 (131.42s)]	ELBO: -703.303 (-710.101)	Log prob: -514.595 (-520.619)	KLD: 15.546 (189.481)	Grad: 1.851
[Epoch  10 (131.93s)]	ELBO: -693.069 (-692.475)	Log prob: -504.877 (-502.577)	KLD: 15.546 (189.898)	Grad: 1.710
Training model 13/16...
[Epoch   1 (146.90s)]	ELBO: -906.844 (-736.651)	Log prob: -703.527 (-529.806)	KLD: 15.725 (206.845)	Grad: 6.933
[Epoch   2 (147.88s)]	ELBO: -710.216 (-700.224)	Log prob: -507.490 (-498.700)	KLD: 15.647 (201.524)	Grad: 2.667
[Epoch   3 (146.30s)]	ELBO: -654.221 (-651.148)	Log prob: -452.111 (-450.548)	KLD: 15.536 (200.601)	Grad: 1.946
[Epoch   4 (146.40s)]	ELBO: -614.948 (-591.438)	Log prob: -413.350 (-390.855)	KLD: 15.427 (200.583)	Grad: 1.759
[Epoch   5 (146.35s)]	ELBO: -579.723 (-574.461)	Log prob: -378.454 (-374.771)	KLD: 15.472 (199.690)	Grad: 1.558
[Epoch   6 (145.86s)]	ELBO: -564.661 (-551.990)	Log prob: -363.631 (-353.072)	KLD: 15.436 (198.918)	Grad: 1.585
[Epoch   7 (145.89s)]	ELBO: -534.492 (-553.534)	Log prob: -333.877 (-353.464)	KLD: 15.428 (200.069)	Grad: 1.439
[Epoch   8 (145.80s)]	ELBO: -524.690 (-499.834)	Log prob: -324.747 (-302.268)	KLD: 15.445 (197.567)	Grad: 1.363
[Epoch   9 (147.35s)]	ELBO: -505.996 (-511.346)	Log prob: -306.541 (-307.956)	KLD: 15.350 (203.390)	Grad: 1.248
[Epoch  10 (145.66s)]	ELBO: -491.639 (-486.487)	Log prob: -292.658 (-285.492)	KLD: 15.375 (200.995)	Grad: 1.209
Training model 14/16...
[Epoch   1 (161.65s)]	ELBO: -688.945 (-525.412)	Log prob: -473.907 (-308.521)	KLD: 16.804 (216.891)	Grad: 12.414
[Epoch   2 (161.33s)]	ELBO: -492.948 (-445.820)	Log prob: -278.727 (-229.503)	KLD: 16.188 (216.318)	Grad: 3.096
[Epoch   3 (160.33s)]	ELBO: -460.595 (-445.912)	Log prob: -246.959 (-230.584)	KLD: 16.016 (215.328)	Grad: 2.540
[Epoch   4 (161.55s)]	ELBO: -431.258 (-432.551)	Log prob: -218.165 (-216.997)	KLD: 15.843 (215.553)	Grad: 2.475
[Epoch   5 (160.72s)]	ELBO: -412.060 (-406.643)	Log prob: -199.568 (-190.807)	KLD: 15.695 (215.835)	Grad: 2.262
[Epoch   6 (162.31s)]	ELBO: -381.863 (-378.938)	Log prob: -169.991 (-169.647)	KLD: 15.632 (209.291)	Grad: 1.952
[Epoch   7 (162.14s)]	ELBO: -352.936 (-347.411)	Log prob: -141.602 (-133.444)	KLD: 15.570 (213.967)	Grad: 1.845
[Epoch   8 (161.02s)]	ELBO: -328.909 (-340.659)	Log prob: -117.995 (-130.362)	KLD: 15.487 (210.297)	Grad: 1.907
[Epoch   9 (160.29s)]	ELBO: -298.181 (-314.822)	Log prob: -87.551 (-102.705)	KLD: 15.558 (212.117)	Grad: 1.859
[Epoch  10 (159.87s)]	ELBO: -267.630 (-318.376)	Log prob: -57.304 (-109.584)	KLD: 15.628 (208.791)	Grad: 1.787
Training model 15/16...
[Epoch   1 (176.03s)]	ELBO: -428.866 (-313.771)	Log prob: -202.251 (-85.237)	KLD: 16.845 (228.534)	Grad: 8.240
[Epoch   2 (176.50s)]	ELBO: -267.969 (-244.343)	Log prob: -41.958 (-18.632)	KLD: 16.545 (225.711)	Grad: 2.868
[Epoch   3 (177.47s)]	ELBO: -238.129 (-237.902)	Log prob: -12.763 (-12.938)	KLD: 16.280 (224.964)	Grad: 2.494
[Epoch   4 (176.72s)]	ELBO: -203.375 (-197.213)	Log prob: 21.166 (26.480)	KLD: 15.990 (223.693)	Grad: 2.159
[Epoch   5 (176.85s)]	ELBO: -172.035 (-152.943)	Log prob: 52.092 (72.564)	KLD: 15.931 (225.507)	Grad: 1.569
[Epoch   6 (177.64s)]	ELBO: -151.761 (-188.786)	Log prob: 72.142 (35.605)	KLD: 15.973 (224.391)	Grad: 1.474
[Epoch   7 (175.72s)]	ELBO: -140.454 (-208.570)	Log prob: 82.912 (13.417)	KLD: 15.864 (221.987)	Grad: 1.471
[Epoch   8 (176.47s)]	ELBO: -133.342 (-131.197)	Log prob: 89.579 (91.385)	KLD: 15.887 (222.582)	Grad: 1.412
[Epoch   9 (175.89s)]	ELBO: -97.125 (-118.239)	Log prob: 125.471 (104.019)	KLD: 15.967 (222.259)	Grad: 1.360
[Epoch  10 (177.14s)]	ELBO: -76.078 (-78.322)	Log prob: 146.021 (146.992)	KLD: 15.942 (225.313)	Grad: 1.280
Training model 16/16...
[Epoch   1 (192.09s)]	ELBO: -245.412 (-135.868)	Log prob: -7.374 (103.185)	KLD: 16.475 (239.053)	Grad: 9.452
[Epoch   2 (193.14s)]	ELBO: -92.845 (-86.109)	Log prob: 144.212 (150.236)	KLD: 15.711 (236.345)	Grad: 2.807
[Epoch   3 (192.01s)]	ELBO: -56.520 (-83.327)	Log prob: 179.533 (154.758)	KLD: 15.246 (238.085)	Grad: 2.043
[Epoch   4 (191.48s)]	ELBO: -42.137 (-85.741)	Log prob: 192.957 (149.819)	KLD: 14.709 (235.560)	Grad: 1.766
[Epoch   5 (194.02s)]	ELBO: -31.645 (-51.146)	Log prob: 202.761 (184.501)	KLD: 14.467 (235.647)	Grad: 1.396
[Epoch   6 (191.87s)]	ELBO: -13.113 (-19.859)	Log prob: 219.997 (210.102)	KLD: 14.102 (229.961)	Grad: 1.286
[Epoch   7 (191.57s)]	ELBO: -21.728 (-94.606)	Log prob: 211.290 (140.317)	KLD: 14.066 (234.924)	Grad: 1.389
[Epoch   8 (194.10s)]	ELBO: 19.679 (30.446)	Log prob: 251.979 (261.111)	KLD: 13.960 (230.665)	Grad: 1.190
[Epoch   9 (194.78s)]	ELBO: 42.889 (36.728)	Log prob: 274.472 (267.093)	KLD: 14.071 (230.365)	Grad: 1.142
[Epoch  10 (193.80s)]	ELBO: 61.970 (43.629)	Log prob: 293.007 (276.851)	KLD: 14.082 (233.222)	Grad: 1.120
Best epoch(s): [8, 9, 9, 10, 8, 10, 10, 9, 10, 10, 10, 10, 10, 9, 10, 10]	Training time(s): 118.34s, 183.73s, 273.50s, 485.93s, 793.26s, 943.96s, 1134.97s, 1339.06s, 1549.17s, 1475.84s, 1382.04s, 1315.04s, 1464.38s, 1611.22s, 1766.44s, 1928.85s (17765.73s)	Best ELBO: 61.970 (43.629)	Best log prob: 293.007 (276.851)
Avg. mu: 0.840, 0.201, 0.280, 0.172, 0.085, -0.116, 0.109, 0.121, 0.153, -0.033, 0.070, 0.133, 0.104, -0.092, 0.105, 0.099, 0.069, -0.092, 0.093, 0.119, 0.081, -0.021, 0.094, 0.096, 0.061, -0.031, 0.054, 0.125, 0.145, -0.018, 0.051, 0.052, 0.088, -0.029, 0.061, 0.091, 0.071, -0.079, 0.030, 0.116, 0.077, -0.023, 0.068, 0.112, 0.097, -0.159, 0.176, 0.116, 0.110, -0.038, 0.085, 0.128, 0.118, -0.099, 0.103, 0.109, 0.123, -0.081, 0.084, 0.121, 0.101, 0.014, 0.066, 0.112
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Max. mu: 6.485, 5.009, 4.584, 4.919, 3.479, 2.990, 2.842, 2.968, 3.118, 3.118, 2.940, 2.978, 3.530, 2.315, 3.187, 2.006, 2.727, 2.105, 2.462, 2.076, 2.437, 2.645, 2.622, 2.279, 2.765, 2.009, 2.325, 1.831, 2.984, 3.601, 2.087, 2.100, 2.094, 1.990, 1.879, 1.608, 2.141, 1.568, 1.406, 1.390, 2.335, 1.710, 1.595, 1.242, 1.703, 0.676, 1.027, 0.924, 0.895, 0.923, 0.798, 0.693, 0.442, 0.216, 0.434, 0.370, 0.451, 0.233, 0.523, 0.354, 0.307, 0.325, 0.219, 0.299
Max. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.002, 0.001, 0.003, 0.005, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.004, 0.004, 0.003, 0.003, 0.005, 0.006, 0.003, 0.003, 0.002, 0.002, 0.002, 0.001, 0.003, 0.004, 0.002, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000
Min. mu: -3.020, -2.924, -4.554, -4.662, -3.402, -2.778, -3.921, -2.797, -3.044, -3.299, -3.288, -2.437, -2.576, -2.407, -2.668, -1.570, -2.684, -2.696, -3.199, -1.773, -2.442, -3.367, -4.216, -2.069, -2.288, -1.930, -2.156, -2.005, -2.095, -2.409, -2.849, -1.702, -2.011, -2.427, -1.787, -1.102, -1.766, -1.495, -1.492, -1.373, -1.264, -1.345, -1.377, -0.874, -1.111, -1.017, -0.566, -0.782, -0.890, -0.717, -0.583, -0.345, -0.306, -0.469, -0.202, -0.218, -0.143, -0.331, -0.206, -0.125, -0.114, -0.226, -0.085, -0.144
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.795 -0.081 -0.291 0.016 0.009 -0.000 0.005 0.010 -0.036 -0.019 0.000
  -0.007 -0.006 0.000 -0.006 -0.002 0.007 0.003 -0.004 -0.003 -0.012
  0.009 0.004 0.009 -0.003 0.003 -0.011 0.015 -0.006 0.002 -0.003 -0.007
  0.015 0.005 -0.007 -0.001 0.007 -0.003 -0.001 0.003 0.002 0.010 -0.018
  -0.006 -0.011 -0.011 -0.019 0.007 -0.006 -0.002 0.017 -0.014 -0.003
  0.003 0.001 0.002 -0.000 0.003 0.001 0.001 0.001 0.001 -0.000 0.004]
 [-0.081 1.070 -0.058 -0.202 0.004 0.002 -0.015 0.015 0.005 -0.011 0.023
  0.007 0.009 0.004 -0.016 -0.000 -0.002 0.010 -0.011 -0.005 0.006 0.006
  0.011 0.007 -0.013 0.013 0.001 0.005 0.007 -0.003 -0.006 -0.006 -0.013
  0.007 -0.001 -0.010 -0.002 0.008 0.003 0.005 -0.001 -0.008 0.008 0.003
  0.002 -0.008 0.021 -0.005 -0.004 -0.019 -0.011 0.008 -0.003 -0.007
  -0.002 -0.000 0.005 -0.001 -0.001 -0.000 0.002 0.002 0.002 0.002]
 [-0.291 -0.058 0.901 0.181 -0.002 0.014 0.012 -0.011 -0.013 -0.018
  -0.002 -0.011 -0.008 -0.002 -0.002 0.000 -0.001 -0.001 0.006 -0.007
  0.001 -0.008 0.017 0.000 0.009 -0.000 0.007 0.002 0.013 0.007 -0.004
  0.012 0.004 -0.003 0.002 -0.002 -0.009 -0.001 0.008 0.008 -0.012 -0.013
  0.015 0.008 -0.001 0.019 0.032 -0.001 0.011 0.017 -0.023 -0.001 0.001
  0.000 0.006 -0.003 -0.002 -0.003 -0.002 -0.003 0.000 -0.005 0.001
  -0.000]
 [0.016 -0.202 0.181 1.602 0.032 0.018 -0.019 0.009 -0.032 -0.049 -0.013
  -0.012 0.014 0.004 0.013 -0.000 0.000 -0.005 -0.008 -0.005 -0.015
  -0.019 0.001 0.011 0.031 -0.012 -0.016 0.001 -0.003 -0.005 0.014 -0.012
  -0.006 -0.000 0.006 0.007 -0.018 -0.010 -0.006 0.010 -0.010 -0.005
  0.006 0.003 -0.040 -0.049 0.013 -0.001 -0.005 -0.012 0.001 0.014 0.006
  -0.003 0.003 -0.002 0.006 -0.002 0.006 -0.007 0.004 0.024 -0.005 0.000]
 [0.009 0.004 -0.002 0.032 0.437 0.025 -0.017 0.017 0.003 0.004 -0.010
  0.008 -0.011 -0.002 0.012 -0.009 0.002 0.001 -0.001 0.000 -0.016 0.001
  0.001 -0.004 -0.004 0.001 -0.009 -0.004 0.010 0.004 -0.006 0.000 -0.004
  0.006 0.006 0.006 0.005 0.001 -0.001 0.003 0.001 -0.002 -0.004 0.002
  0.007 -0.004 -0.002 -0.003 0.004 0.002 0.000 -0.002 -0.008 0.003 0.007
  0.001 -0.006 -0.001 -0.001 -0.004 0.002 0.003 -0.001 -0.002]
 [-0.000 0.002 0.014 0.018 0.025 0.416 -0.007 -0.016 -0.008 -0.001 0.001
  0.001 0.009 0.005 0.001 -0.012 -0.003 -0.002 0.002 -0.007 -0.009 -0.002
  0.004 0.000 0.002 -0.002 0.000 -0.000 -0.007 -0.005 -0.001 -0.003
  -0.005 -0.003 0.001 0.003 0.004 -0.000 -0.003 -0.003 -0.004 -0.001
  0.001 -0.001 0.008 -0.001 0.004 -0.000 -0.003 0.008 0.005 0.003 -0.006
  -0.002 0.005 -0.001 0.000 0.000 0.001 -0.000 0.001 0.001 0.000 -0.002]
 [0.005 -0.015 0.012 -0.019 -0.017 -0.007 0.459 -0.002 0.000 0.007 0.012
  0.010 -0.003 0.002 0.004 0.003 -0.001 -0.009 0.011 -0.007 0.003 0.010
  0.012 -0.000 0.004 0.002 0.000 -0.001 0.005 -0.005 -0.009 0.000 0.002
  -0.005 0.008 0.001 0.004 -0.003 0.001 -0.000 0.004 0.001 -0.000 -0.000
  -0.011 -0.001 -0.006 0.004 -0.004 -0.003 -0.003 0.001 0.003 0.003
  -0.003 0.001 0.000 -0.001 -0.004 -0.000 -0.001 -0.002 -0.001 0.002]
 [0.010 0.015 -0.011 0.009 0.017 -0.016 -0.002 0.318 0.011 -0.003 0.004
  0.002 0.004 -0.006 0.002 0.005 0.003 0.002 -0.002 0.004 0.003 0.001
  -0.000 -0.001 0.002 -0.000 -0.004 0.003 -0.004 0.004 -0.002 0.004
  -0.003 -0.002 -0.005 -0.004 0.001 0.001 -0.003 0.001 0.000 0.001 -0.001
  0.001 -0.003 0.001 0.001 0.001 0.002 -0.006 -0.005 0.001 0.004 0.004
  -0.004 -0.001 0.001 -0.001 0.002 -0.000 -0.001 -0.000 0.001 0.000]
 [-0.036 0.005 -0.013 -0.032 0.003 -0.008 0.000 0.011 0.435 0.036 -0.002
  0.024 0.004 0.002 -0.001 -0.007 -0.000 -0.006 -0.005 0.003 0.006 -0.004
  -0.009 0.008 0.008 0.002 0.001 0.006 0.002 0.003 0.002 -0.000 0.000
  -0.000 -0.012 0.002 -0.005 0.000 0.004 -0.001 0.000 -0.004 -0.003
  -0.002 0.004 -0.012 -0.008 -0.001 -0.007 0.001 -0.010 0.003 -0.002
  -0.002 -0.003 0.000 0.002 -0.001 -0.006 0.002 -0.002 -0.004 -0.000
  0.002]
 [-0.019 -0.011 -0.018 -0.049 0.004 -0.001 0.007 -0.003 0.036 0.327
  -0.013 0.019 0.001 -0.005 0.000 0.003 0.003 -0.001 -0.008 -0.000 0.004
  0.002 0.002 0.004 0.003 0.001 -0.002 -0.002 -0.017 -0.003 -0.002 0.002
  0.002 0.011 -0.001 0.002 0.000 0.004 -0.002 -0.004 -0.002 0.004 0.005
  0.001 0.003 -0.006 -0.009 0.003 0.001 -0.003 -0.002 -0.005 0.000 -0.002
  0.002 0.001 -0.002 -0.003 0.000 0.002 -0.001 -0.002 0.002 0.001]
 [0.000 0.023 -0.002 -0.013 -0.010 0.001 0.012 0.004 -0.002 -0.013 0.366
  -0.021 0.005 0.004 0.013 0.004 -0.005 -0.006 0.007 0.002 0.002 0.002
  -0.006 -0.000 -0.008 0.001 0.007 0.001 -0.003 -0.004 -0.002 0.001
  -0.004 -0.002 0.006 0.001 -0.003 -0.002 -0.003 0.002 0.001 0.002 0.005
  0.000 0.001 -0.004 0.001 -0.001 0.004 0.001 -0.001 -0.004 -0.003 -0.002
  0.001 -0.002 0.001 -0.002 -0.001 0.003 0.003 0.000 -0.001 -0.002]
 [-0.007 0.007 -0.011 -0.012 0.008 0.001 0.010 0.002 0.024 0.019 -0.021
  0.241 -0.002 -0.005 0.003 -0.007 0.000 0.002 -0.003 0.001 0.001 -0.003
  0.006 -0.004 -0.006 -0.002 -0.002 0.003 0.004 0.000 -0.000 -0.002 0.006
  0.003 0.006 -0.001 0.000 0.002 -0.002 -0.002 -0.004 -0.001 -0.000 0.000
  0.001 -0.003 -0.004 0.001 0.001 0.001 -0.004 -0.004 0.000 -0.003 -0.002
  0.001 0.001 0.001 -0.001 0.001 -0.001 -0.001 0.000 0.000]
 [-0.006 0.009 -0.008 0.014 -0.011 0.009 -0.003 0.004 0.004 0.001 0.005
  -0.002 0.281 0.004 0.003 -0.007 0.017 0.002 -0.006 0.001 -0.002 -0.009
  0.005 0.004 0.002 0.005 -0.008 -0.002 -0.001 0.001 -0.001 -0.003 0.000
  -0.004 -0.002 0.001 0.004 0.001 0.001 0.000 -0.003 -0.001 0.004 -0.000
  -0.000 -0.006 -0.007 0.001 0.001 0.000 -0.004 0.002 -0.003 -0.001 0.000
  0.000 0.001 0.001 -0.000 0.003 -0.002 -0.001 0.001 0.000]
 [0.000 0.004 -0.002 0.004 -0.002 0.005 0.002 -0.006 0.002 -0.005 0.004
  -0.005 0.004 0.235 -0.006 0.004 0.003 0.002 -0.007 0.002 0.004 -0.005
  -0.006 -0.004 0.002 0.000 0.002 -0.002 0.003 0.005 -0.000 0.001 -0.004
  0.004 -0.003 -0.001 -0.000 -0.001 -0.001 -0.003 0.001 0.001 0.002
  -0.002 0.006 -0.001 0.001 0.000 0.002 -0.002 -0.001 -0.002 -0.002
  -0.001 -0.000 0.000 0.000 0.001 -0.001 0.001 -0.001 -0.001 0.001 0.000]
 [-0.006 -0.016 -0.002 0.013 0.012 0.001 0.004 0.002 -0.001 0.000 0.013
  0.003 0.003 -0.006 0.305 -0.004 0.002 0.005 0.016 -0.002 -0.002 0.003
  0.000 -0.006 -0.001 -0.000 -0.006 -0.004 0.001 -0.000 -0.001 0.002
  -0.006 0.004 0.001 0.004 -0.003 -0.001 -0.004 -0.000 -0.001 -0.003
  -0.001 -0.000 -0.005 -0.000 0.000 0.003 0.001 0.001 0.003 0.003 -0.001
  0.003 0.001 0.001 0.000 0.001 -0.001 0.002 0.001 -0.001 -0.000 0.000]
 [-0.002 -0.000 0.000 -0.000 -0.009 -0.012 0.003 0.005 -0.007 0.003 0.004
  -0.007 -0.007 0.004 -0.004 0.190 0.001 -0.003 0.003 0.001 -0.000 0.009
  0.002 -0.001 -0.009 -0.003 -0.003 0.002 0.002 0.002 -0.002 -0.000 0.000
  -0.003 -0.001 -0.001 -0.001 -0.002 -0.000 0.001 -0.002 -0.003 -0.000
  0.001 -0.005 0.004 0.004 0.001 0.004 0.000 0.000 -0.001 0.000 -0.001
  -0.001 0.000 0.002 -0.000 0.001 -0.001 0.001 0.001 0.000 -0.000]
 [0.007 -0.002 -0.001 0.000 0.002 -0.003 -0.001 0.003 -0.000 0.003 -0.005
  0.000 0.017 0.003 0.002 0.001 0.238 -0.002 -0.012 -0.004 0.007 0.013
  0.007 -0.003 0.004 -0.002 -0.007 0.000 0.003 -0.003 -0.004 0.000 0.003
  -0.004 -0.006 -0.001 0.002 -0.003 -0.004 0.001 0.001 -0.001 -0.001
  -0.001 -0.004 0.000 -0.000 -0.003 0.000 -0.003 0.003 -0.000 -0.003
  -0.000 0.002 -0.000 0.002 0.001 0.001 -0.000 -0.000 -0.001 0.001 0.001]
 [0.003 0.010 -0.001 -0.005 0.001 -0.002 -0.009 0.002 -0.006 -0.001
  -0.006 0.002 0.002 0.002 0.005 -0.003 -0.002 0.201 -0.014 0.006 -0.003
  0.002 0.001 -0.004 -0.001 0.002 0.001 -0.002 0.003 -0.001 -0.001 -0.000
  0.005 0.004 0.000 -0.003 -0.001 -0.001 -0.000 0.001 0.001 0.002 0.000
  -0.001 0.001 0.002 0.002 -0.001 0.001 -0.000 0.002 0.002 0.001 -0.001
  -0.000 -0.001 -0.001 -0.002 -0.001 0.001 -0.002 -0.001 0.000 0.000]
 [-0.004 -0.011 0.006 -0.008 -0.001 0.002 0.011 -0.002 -0.005 -0.008
  0.007 -0.003 -0.006 -0.007 0.016 0.003 -0.012 -0.014 0.244 -0.007
  -0.010 0.011 0.002 0.004 -0.001 -0.001 0.003 -0.001 -0.007 -0.005 0.003
  0.002 0.004 0.001 0.001 -0.002 0.001 0.002 -0.000 0.001 0.001 -0.000
  0.001 0.001 0.001 0.003 -0.001 -0.001 -0.002 0.001 -0.000 0.003 -0.001
  0.001 -0.001 0.001 -0.002 0.001 0.003 -0.002 0.002 -0.000 -0.000 -0.001]
 [-0.003 -0.005 -0.007 -0.005 0.000 -0.007 -0.007 0.004 0.003 -0.000
  0.002 0.001 0.001 0.002 -0.002 0.001 -0.004 0.006 -0.007 0.158 -0.006
  0.001 0.005 0.003 -0.001 -0.005 -0.002 0.001 -0.003 -0.004 -0.003 0.000
  -0.001 0.004 0.001 0.001 0.001 -0.001 0.000 0.000 -0.000 -0.001 0.002
  -0.001 -0.002 0.001 0.002 0.004 0.002 -0.001 -0.002 -0.002 0.000 -0.001
  -0.000 -0.001 -0.000 -0.000 0.002 -0.001 0.001 0.000 0.001 0.001]
 [-0.012 0.006 0.001 -0.015 -0.016 -0.009 0.003 0.003 0.006 0.004 0.002
  0.001 -0.002 0.004 -0.002 -0.000 0.007 -0.003 -0.010 -0.006 0.250
  -0.011 -0.011 -0.004 0.001 -0.001 0.003 0.001 -0.001 -0.005 0.003 0.006
  0.001 -0.003 0.002 -0.005 -0.005 -0.001 0.001 -0.001 0.003 -0.006 0.001
  0.001 0.001 0.002 0.002 -0.003 -0.004 -0.005 -0.001 -0.002 -0.001
  -0.002 0.000 0.000 0.001 -0.000 0.001 -0.001 -0.000 -0.002 0.000 0.000]
 [0.009 0.006 -0.008 -0.019 0.001 -0.002 0.010 0.001 -0.004 0.002 0.002
  -0.003 -0.009 -0.005 0.003 0.009 0.013 0.002 0.011 0.001 -0.011 0.269
  0.025 0.002 -0.001 0.002 -0.000 -0.004 0.001 0.004 0.001 -0.000 0.002
  -0.002 -0.002 0.001 0.001 -0.001 0.000 -0.000 -0.001 0.006 -0.001 0.003
  0.000 0.001 -0.002 0.003 0.000 0.001 -0.002 0.001 0.000 -0.000 -0.000
  0.001 0.001 0.001 0.000 -0.000 -0.001 -0.001 0.000 0.001]
 [0.004 0.011 0.017 0.001 0.001 0.004 0.012 -0.000 -0.009 0.002 -0.006
  0.006 0.005 -0.006 0.000 0.002 0.007 0.001 0.002 0.005 -0.011 0.025
  0.264 -0.004 -0.009 0.003 -0.008 0.001 -0.000 0.003 0.001 0.000 0.001
  0.003 -0.002 -0.009 0.003 -0.002 -0.001 -0.001 -0.001 0.003 0.000 0.001
  0.001 0.002 0.000 0.002 0.001 0.003 0.001 0.001 0.000 0.001 -0.000
  -0.000 -0.000 0.001 0.000 -0.001 -0.001 -0.001 0.000 0.000]
 [0.009 0.007 0.000 0.011 -0.004 0.000 -0.000 -0.001 0.008 0.004 -0.000
  -0.004 0.004 -0.004 -0.006 -0.001 -0.003 -0.004 0.004 0.003 -0.004
  0.002 -0.004 0.176 -0.001 0.000 -0.004 0.003 -0.005 -0.004 -0.004 0.003
  0.005 -0.000 0.000 -0.005 -0.000 0.002 -0.001 -0.001 0.001 0.001 -0.002
  -0.000 -0.001 0.001 0.002 0.001 -0.001 0.001 0.001 0.002 0.001 0.000
  -0.000 0.000 0.000 -0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000]
 [-0.003 -0.013 0.009 0.031 -0.004 0.002 0.004 0.002 0.008 0.003 -0.008
  -0.006 0.002 0.002 -0.001 -0.009 0.004 -0.001 -0.001 -0.001 0.001
  -0.001 -0.009 -0.001 0.215 0.003 -0.006 -0.003 -0.004 -0.004 -0.002
  -0.002 -0.005 -0.003 -0.004 0.002 -0.003 0.004 -0.002 0.001 -0.001
  0.005 -0.000 0.001 -0.002 -0.004 0.001 -0.000 -0.000 0.001 0.002 0.003
  -0.002 0.001 0.001 -0.000 0.001 0.000 -0.001 0.000 -0.000 0.001 -0.000
  0.000]
 [0.003 0.013 -0.000 -0.012 0.001 -0.002 0.002 -0.000 0.002 0.001 0.001
  -0.002 0.005 0.000 -0.000 -0.003 -0.002 0.002 -0.001 -0.005 -0.001
  0.002 0.003 0.000 0.003 0.168 -0.007 0.005 0.002 -0.003 -0.002 0.002
  -0.001 0.004 0.000 -0.000 -0.003 0.000 -0.002 0.000 -0.002 0.002 -0.003
  -0.000 -0.000 0.001 -0.001 -0.000 0.001 0.001 -0.000 -0.001 -0.001
  -0.001 -0.001 0.001 -0.000 0.001 -0.001 0.000 -0.000 -0.001 -0.001
  -0.000]
 [-0.011 0.001 0.007 -0.016 -0.009 0.000 0.000 -0.004 0.001 -0.002 0.007
  -0.002 -0.008 0.002 -0.006 -0.003 -0.007 0.001 0.003 -0.002 0.003
  -0.000 -0.008 -0.004 -0.006 -0.007 0.201 0.002 -0.000 0.002 0.003
  -0.003 0.002 0.001 -0.002 -0.001 -0.002 0.002 0.004 -0.002 0.001 0.000
  0.002 -0.000 0.001 0.001 0.000 -0.000 0.000 0.002 0.001 0.000 -0.001
  0.001 -0.001 0.001 -0.001 -0.000 -0.000 0.000 -0.002 -0.000 -0.001
  -0.000]
 [0.015 0.005 0.002 0.001 -0.004 -0.000 -0.001 0.003 0.006 -0.002 0.001
  0.003 -0.002 -0.002 -0.004 0.002 0.000 -0.002 -0.001 0.001 0.001 -0.004
  0.001 0.003 -0.003 0.005 0.002 0.137 0.004 -0.000 -0.001 -0.000 -0.001
  0.001 -0.001 0.000 0.000 -0.000 0.001 0.003 -0.001 -0.000 -0.000 -0.000
  0.001 -0.001 0.002 0.002 0.002 -0.001 -0.000 -0.000 -0.000 -0.000 0.000
  -0.000 0.000 -0.000 0.001 0.001 -0.000 0.000 0.001 0.000]
 [-0.006 0.007 0.013 -0.003 0.010 -0.007 0.005 -0.004 0.002 -0.017 -0.003
  0.004 -0.001 0.003 0.001 0.002 0.003 0.003 -0.007 -0.003 -0.001 0.001
  -0.000 -0.005 -0.004 0.002 -0.000 0.004 0.211 0.009 -0.002 -0.004
  -0.000 0.000 -0.002 0.006 0.001 -0.002 -0.003 0.003 -0.001 0.004 -0.000
  0.002 -0.002 0.001 0.000 0.000 -0.000 0.001 0.000 0.000 -0.000 0.000
  -0.001 -0.000 -0.001 0.001 0.000 -0.001 0.000 -0.002 -0.000 0.001]
 [0.002 -0.003 0.007 -0.005 0.004 -0.005 -0.005 0.004 0.003 -0.003 -0.004
  0.000 0.001 0.005 -0.000 0.002 -0.003 -0.001 -0.005 -0.004 -0.005 0.004
  0.003 -0.004 -0.004 -0.003 0.002 -0.000 0.009 0.210 -0.000 0.006 -0.002
  0.005 0.001 0.001 0.001 0.002 0.003 0.001 0.004 -0.006 -0.002 0.000
  0.001 -0.001 -0.000 -0.004 -0.000 0.001 -0.001 -0.001 -0.000 -0.000
  -0.001 0.000 -0.001 0.001 -0.000 -0.000 -0.001 -0.001 -0.000 0.001]
 [-0.003 -0.006 -0.004 0.014 -0.006 -0.001 -0.009 -0.002 0.002 -0.002
  -0.002 -0.000 -0.001 -0.000 -0.001 -0.002 -0.004 -0.001 0.003 -0.003
  0.003 0.001 0.001 -0.004 -0.002 -0.002 0.003 -0.001 -0.002 -0.000 0.204
  0.004 0.008 -0.003 -0.000 -0.002 -0.002 0.001 -0.001 0.001 -0.004
  -0.002 -0.001 0.001 0.001 -0.001 0.003 0.000 0.001 -0.002 0.001 -0.000
  -0.000 -0.001 0.000 0.001 -0.000 0.001 -0.001 0.001 0.001 0.001 -0.000
  -0.001]
 [-0.007 -0.006 0.012 -0.012 0.000 -0.003 0.000 0.004 -0.000 0.002 0.001
  -0.002 -0.003 0.001 0.002 -0.000 0.000 -0.000 0.002 0.000 0.006 -0.000
  0.000 0.003 -0.002 0.002 -0.003 -0.000 -0.004 0.006 0.004 0.126 -0.000
  -0.002 0.002 0.000 -0.000 -0.001 0.000 -0.001 0.002 -0.003 -0.001 0.001
  -0.000 0.002 0.002 0.000 0.001 0.000 -0.001 -0.000 0.000 -0.000 0.001
  0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.001 0.000]
 [0.015 -0.013 0.004 -0.006 -0.004 -0.005 0.002 -0.003 0.000 0.002 -0.004
  0.006 0.000 -0.004 -0.006 0.000 0.003 0.005 0.004 -0.001 0.001 0.002
  0.001 0.005 -0.005 -0.001 0.002 -0.001 -0.000 -0.002 0.008 -0.000 0.149
  0.013 -0.006 -0.013 -0.003 -0.000 -0.000 -0.001 0.002 -0.003 0.002
  -0.002 -0.000 0.001 -0.000 0.003 0.000 0.001 -0.000 -0.001 -0.001
  -0.001 0.000 0.000 -0.000 0.000 -0.000 0.001 -0.000 -0.000 -0.000
  -0.000]
 [0.005 0.007 -0.003 -0.000 0.006 -0.003 -0.005 -0.002 -0.000 0.011
  -0.002 0.003 -0.004 0.004 0.004 -0.003 -0.004 0.004 0.001 0.004 -0.003
  -0.002 0.003 -0.000 -0.003 0.004 0.001 0.001 0.000 0.005 -0.003 -0.002
  0.013 0.151 0.010 0.005 -0.001 -0.002 -0.002 0.000 -0.001 -0.000 0.001
  0.001 -0.000 0.001 0.002 0.001 -0.000 -0.000 0.000 -0.000 -0.000 -0.000
  0.000 0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000]
 [-0.007 -0.001 0.002 0.006 0.006 0.001 0.008 -0.005 -0.012 -0.001 0.006
  0.006 -0.002 -0.003 0.001 -0.001 -0.006 0.000 0.001 0.001 0.002 -0.002
  -0.002 0.000 -0.004 0.000 -0.002 -0.001 -0.002 0.001 -0.000 0.002
  -0.006 0.010 0.145 0.001 0.001 -0.000 0.002 0.002 0.000 -0.000 -0.002
  0.001 -0.000 0.003 0.002 -0.002 -0.001 -0.001 0.000 0.000 0.001 -0.001
  -0.000 0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000]
 [-0.001 -0.010 -0.002 0.007 0.006 0.003 0.001 -0.004 0.002 0.002 0.001
  -0.001 0.001 -0.001 0.004 -0.001 -0.001 -0.003 -0.002 0.001 -0.005
  0.001 -0.009 -0.005 0.002 -0.000 -0.001 0.000 0.006 0.001 -0.002 0.000
  -0.013 0.005 0.001 0.091 0.001 0.000 0.003 0.003 -0.000 0.000 0.000
  0.000 -0.000 -0.001 0.001 -0.001 0.001 0.000 0.001 -0.000 -0.000 0.000
  0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 0.000 -0.000 0.000]
 [0.007 -0.002 -0.009 -0.018 0.005 0.004 0.004 0.001 -0.005 0.000 -0.003
  0.000 0.004 -0.000 -0.003 -0.001 0.002 -0.001 0.001 0.001 -0.005 0.001
  0.003 -0.000 -0.003 -0.003 -0.002 0.000 0.001 0.001 -0.002 -0.000
  -0.003 -0.001 0.001 0.001 0.112 0.005 0.008 0.004 0.003 0.002 0.001
  -0.002 -0.000 0.000 0.002 0.002 -0.001 -0.001 0.000 -0.001 0.000 0.001
  -0.000 0.001 -0.000 -0.001 0.000 0.000 0.000 -0.000 0.000 -0.000]
 [-0.003 0.008 -0.001 -0.010 0.001 -0.000 -0.003 0.001 0.000 0.004 -0.002
  0.002 0.001 -0.001 -0.001 -0.002 -0.003 -0.001 0.002 -0.001 -0.001
  -0.001 -0.002 0.002 0.004 0.000 0.002 -0.000 -0.002 0.002 0.001 -0.001
  -0.000 -0.002 -0.000 0.000 0.005 0.087 0.000 0.012 -0.001 0.000 0.001
  -0.001 -0.001 -0.001 -0.002 0.002 0.001 -0.001 -0.002 0.001 -0.000
  0.001 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000 0.000 0.000 -0.000]
 [-0.001 0.003 0.008 -0.006 -0.001 -0.003 0.001 -0.003 0.004 -0.002
  -0.003 -0.002 0.001 -0.001 -0.004 -0.000 -0.004 -0.000 -0.000 0.000
  0.001 0.000 -0.001 -0.001 -0.002 -0.002 0.004 0.001 -0.003 0.003 -0.001
  0.000 -0.000 -0.002 0.002 0.003 0.008 0.000 0.107 -0.004 0.003 -0.004
  0.002 -0.004 -0.000 -0.000 -0.001 0.001 0.000 0.001 0.000 -0.000 0.000
  -0.001 -0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 0.000]
 [0.003 0.005 0.008 0.010 0.003 -0.003 -0.000 0.001 -0.001 -0.004 0.002
  -0.002 0.000 -0.003 -0.000 0.001 0.001 0.001 0.001 0.000 -0.001 -0.000
  -0.001 -0.001 0.001 0.000 -0.002 0.003 0.003 0.001 0.001 -0.001 -0.001
  0.000 0.002 0.003 0.004 0.012 -0.004 0.065 -0.002 0.001 0.002 0.000
  0.003 -0.000 0.002 -0.000 0.001 -0.002 -0.001 -0.001 0.000 0.000 0.001
  0.000 0.000 -0.000 0.000 -0.000 0.000 0.000 -0.000 0.000]
 [0.002 -0.001 -0.012 -0.010 0.001 -0.004 0.004 0.000 0.000 -0.002 0.001
  -0.004 -0.003 0.001 -0.001 -0.002 0.001 0.001 0.001 -0.000 0.003 -0.001
  -0.001 0.001 -0.001 -0.002 0.001 -0.001 -0.001 0.004 -0.004 0.002 0.002
  -0.001 0.000 -0.000 0.003 -0.001 0.003 -0.002 0.081 0.003 -0.010 -0.012
  0.007 0.001 0.005 0.007 -0.003 -0.000 -0.001 -0.000 0.000 -0.000 0.000
  0.000 -0.000 -0.000 -0.000 0.000 0.000 -0.001 0.000 0.000]
 [0.010 -0.008 -0.013 -0.005 -0.002 -0.001 0.001 0.001 -0.004 0.004 0.002
  -0.001 -0.001 0.001 -0.003 -0.003 -0.001 0.002 -0.000 -0.001 -0.006
  0.006 0.003 0.001 0.005 0.002 0.000 -0.000 0.004 -0.006 -0.002 -0.003
  -0.003 -0.000 -0.000 0.000 0.002 0.000 -0.004 0.001 0.003 0.091 0.002
  0.002 0.002 0.000 0.000 0.006 -0.003 -0.001 0.000 -0.001 -0.001 0.001
  -0.001 -0.000 0.000 0.000 -0.000 0.001 0.000 -0.000 0.000 0.000]
 [-0.018 0.008 0.015 0.006 -0.004 0.001 -0.000 -0.001 -0.003 0.005 0.005
  -0.000 0.004 0.002 -0.001 -0.000 -0.001 0.000 0.001 0.002 0.001 -0.001
  0.000 -0.002 -0.000 -0.003 0.002 -0.000 -0.000 -0.002 -0.001 -0.001
  0.002 0.001 -0.002 0.000 0.001 0.001 0.002 0.002 -0.010 0.002 0.087
  -0.010 -0.001 -0.003 0.001 0.004 -0.002 0.001 -0.000 -0.001 -0.001
  0.001 -0.001 0.001 -0.000 -0.000 -0.000 -0.000 0.000 0.000 0.001 0.000]
 [-0.006 0.003 0.008 0.003 0.002 -0.001 -0.000 0.001 -0.002 0.001 0.000
  0.000 -0.000 -0.002 -0.000 0.001 -0.001 -0.001 0.001 -0.001 0.001 0.003
  0.001 -0.000 0.001 -0.000 -0.000 -0.000 0.002 0.000 0.001 0.001 -0.002
  0.001 0.001 0.000 -0.002 -0.001 -0.004 0.000 -0.012 0.002 -0.010 0.052
  0.000 -0.002 -0.000 0.004 -0.002 0.000 -0.000 -0.002 -0.001 -0.000
  -0.001 -0.000 -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 -0.000]
 [-0.011 0.002 -0.001 -0.040 0.007 0.008 -0.011 -0.003 0.004 0.003 0.001
  0.001 -0.000 0.006 -0.005 -0.005 -0.004 0.001 0.001 -0.002 0.001 0.000
  0.001 -0.001 -0.002 -0.000 0.001 0.001 -0.002 0.001 0.001 -0.000 -0.000
  -0.000 -0.000 -0.000 -0.000 -0.001 -0.000 0.003 0.007 0.002 -0.001
  0.000 0.046 -0.011 0.010 -0.022 -0.005 -0.007 -0.001 -0.002 -0.001
  -0.001 -0.001 -0.000 -0.001 -0.000 -0.001 0.000 0.000 -0.001 0.001
  -0.000]
 [-0.011 -0.008 0.019 -0.049 -0.004 -0.001 -0.001 0.001 -0.012 -0.006
  -0.004 -0.003 -0.006 -0.001 -0.000 0.004 0.000 0.002 0.003 0.001 0.002
  0.001 0.002 0.001 -0.004 0.001 0.001 -0.001 0.001 -0.001 -0.001 0.002
  0.001 0.001 0.003 -0.001 0.000 -0.001 -0.000 -0.000 0.001 0.000 -0.003
  -0.002 -0.011 0.020 0.002 0.005 0.000 0.003 -0.002 0.000 0.000 0.000
  0.000 -0.000 -0.000 0.000 0.002 -0.000 -0.000 -0.001 -0.000 -0.000]
 [-0.019 0.021 0.032 0.013 -0.002 0.004 -0.006 0.001 -0.008 -0.009 0.001
  -0.004 -0.007 0.001 0.000 0.004 -0.000 0.002 -0.001 0.002 0.002 -0.002
  0.000 0.002 0.001 -0.001 0.000 0.002 0.000 -0.000 0.003 0.002 -0.000
  0.002 0.002 0.001 0.002 -0.002 -0.001 0.002 0.005 0.000 0.001 -0.000
  0.010 0.002 0.019 -0.004 -0.004 -0.002 0.000 0.000 -0.000 -0.001 -0.000
  -0.001 -0.000 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.000]
 [0.007 -0.005 -0.001 -0.001 -0.003 -0.000 0.004 0.001 -0.001 0.003
  -0.001 0.001 0.001 0.000 0.003 0.001 -0.003 -0.001 -0.001 0.004 -0.003
  0.003 0.002 0.001 -0.000 -0.000 -0.000 0.002 0.000 -0.004 0.000 0.000
  0.003 0.001 -0.002 -0.001 0.002 0.002 0.001 -0.000 0.007 0.006 0.004
  0.004 -0.022 0.005 -0.004 0.037 0.003 0.009 0.003 0.003 0.000 0.001
  -0.001 0.001 0.000 -0.000 -0.000 0.000 -0.000 0.000 0.001 0.000]
 [-0.006 -0.004 0.011 -0.005 0.004 -0.003 -0.004 0.002 -0.007 0.001 0.004
  0.001 0.001 0.002 0.001 0.004 0.000 0.001 -0.002 0.002 -0.004 0.000
  0.001 -0.001 -0.000 0.001 0.000 0.002 -0.000 -0.000 0.001 0.001 0.000
  -0.000 -0.001 0.001 -0.001 0.001 0.000 0.001 -0.003 -0.003 -0.002
  -0.002 -0.005 0.000 -0.004 0.003 0.025 0.002 -0.000 -0.005 0.000 0.000
  0.001 0.002 0.001 0.000 -0.001 -0.001 -0.000 -0.000 0.001 -0.000]
 [-0.002 -0.019 0.017 -0.012 0.002 0.008 -0.003 -0.006 0.001 -0.003 0.001
  0.001 0.000 -0.002 0.001 0.000 -0.003 -0.000 0.001 -0.001 -0.005 0.001
  0.003 0.001 0.001 0.001 0.002 -0.001 0.001 0.001 -0.002 0.000 0.001
  -0.000 -0.001 0.000 -0.001 -0.001 0.001 -0.002 -0.000 -0.001 0.001
  0.000 -0.007 0.003 -0.002 0.009 0.002 0.022 0.004 0.004 -0.000 0.001
  -0.001 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000]
 [0.017 -0.011 -0.023 0.001 0.000 0.005 -0.003 -0.005 -0.010 -0.002
  -0.001 -0.004 -0.004 -0.001 0.003 0.000 0.003 0.002 -0.000 -0.002
  -0.001 -0.002 0.001 0.001 0.002 -0.000 0.001 -0.000 0.000 -0.001 0.001
  -0.001 -0.000 0.000 0.000 0.001 0.000 -0.002 0.000 -0.001 -0.001 0.000
  -0.000 -0.000 -0.001 -0.002 0.000 0.003 -0.000 0.004 0.015 0.001 0.000
  0.001 -0.000 0.000 0.000 0.000 -0.001 -0.000 0.000 0.000 0.000 -0.000]
 [-0.014 0.008 -0.001 0.014 -0.002 0.003 0.001 0.001 0.003 -0.005 -0.004
  -0.004 0.002 -0.002 0.003 -0.001 -0.000 0.002 0.003 -0.002 -0.002 0.001
  0.001 0.002 0.003 -0.001 0.000 -0.000 0.000 -0.001 -0.000 -0.000 -0.001
  -0.000 0.000 -0.000 -0.001 0.001 -0.000 -0.001 -0.000 -0.001 -0.001
  -0.002 -0.002 0.000 0.000 0.003 -0.005 0.004 0.001 0.011 0.000 0.001
  -0.001 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.001 -0.000 -0.000]
 [-0.003 -0.003 0.001 0.006 -0.008 -0.006 0.003 0.004 -0.002 0.000 -0.003
  0.000 -0.003 -0.002 -0.001 0.000 -0.003 0.001 -0.001 0.000 -0.001 0.000
  0.000 0.001 -0.002 -0.001 -0.001 -0.000 -0.000 -0.000 -0.000 0.000
  -0.001 -0.000 0.001 -0.000 0.000 -0.000 0.000 0.000 0.000 -0.001 -0.001
  -0.001 -0.001 0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.003 0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000]
 [0.003 -0.007 0.000 -0.003 0.003 -0.002 0.003 0.004 -0.002 -0.002 -0.002
  -0.003 -0.001 -0.001 0.003 -0.001 -0.000 -0.001 0.001 -0.001 -0.002
  -0.000 0.001 0.000 0.001 -0.001 0.001 -0.000 0.000 -0.000 -0.001 -0.000
  -0.001 -0.000 -0.001 0.000 0.001 0.001 -0.001 0.000 -0.000 0.001 0.001
  -0.000 -0.001 0.000 -0.001 0.001 0.000 0.001 0.001 0.001 0.000 0.003
  -0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000 -0.000 0.000 -0.000]
 [0.001 -0.002 0.006 0.003 0.007 0.005 -0.003 -0.004 -0.003 0.002 0.001
  -0.002 0.000 -0.000 0.001 -0.001 0.002 -0.000 -0.001 -0.000 0.000
  -0.000 -0.000 -0.000 0.001 -0.001 -0.001 0.000 -0.001 -0.001 0.000
  0.001 0.000 0.000 -0.000 0.000 -0.000 -0.000 -0.000 0.001 0.000 -0.001
  -0.001 -0.001 -0.001 0.000 -0.000 -0.001 0.001 -0.001 -0.000 -0.001
  -0.000 -0.000 0.004 -0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.000
  -0.000]
 [0.002 -0.000 -0.003 -0.002 0.001 -0.001 0.001 -0.001 0.000 0.001 -0.002
  0.001 0.000 0.000 0.001 0.000 -0.000 -0.001 0.001 -0.001 0.000 0.001
  -0.000 0.000 -0.000 0.001 0.001 -0.000 -0.000 0.000 0.001 0.000 0.000
  0.000 0.000 -0.000 0.001 0.000 0.000 0.000 0.000 -0.000 0.001 -0.000
  -0.000 -0.000 -0.001 0.001 0.002 -0.001 0.000 -0.001 -0.000 0.000
  -0.000 0.002 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 0.000]
 [-0.000 0.005 -0.002 0.006 -0.006 0.000 0.000 0.001 0.002 -0.002 0.001
  0.001 0.001 0.000 0.000 0.002 0.002 -0.001 -0.002 -0.000 0.001 0.001
  -0.000 0.000 0.001 -0.000 -0.001 0.000 -0.001 -0.001 -0.000 0.000
  -0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.000 -0.000 0.000 -0.000
  -0.000 -0.001 -0.000 -0.000 0.000 0.001 -0.000 0.000 -0.000 0.000
  -0.000 -0.000 0.000 0.002 0.000 -0.000 0.000 -0.000 0.000 0.000 0.000]
 [0.003 -0.001 -0.003 -0.002 -0.001 0.000 -0.001 -0.001 -0.001 -0.003
  -0.002 0.001 0.001 0.001 0.001 -0.000 0.001 -0.002 0.001 -0.000 -0.000
  0.001 0.001 -0.000 0.000 0.001 -0.000 -0.000 0.001 0.001 0.001 0.000
  0.000 -0.000 0.000 -0.000 -0.001 0.000 0.000 -0.000 -0.000 0.000 -0.000
  -0.000 -0.000 0.000 -0.001 -0.000 0.000 -0.000 0.000 -0.000 -0.000
  -0.000 0.000 0.000 0.000 0.001 0.000 -0.000 -0.000 -0.000 -0.000 -0.000]
 [0.001 -0.001 -0.002 0.006 -0.001 0.001 -0.004 0.002 -0.006 0.000 -0.001
  -0.001 -0.000 -0.001 -0.001 0.001 0.001 -0.001 0.003 0.002 0.001 0.000
  0.000 0.000 -0.001 -0.001 -0.000 0.001 0.000 -0.000 -0.001 -0.000
  -0.000 0.000 0.000 0.000 0.000 0.000 -0.000 0.000 -0.000 -0.000 -0.000
  0.000 -0.001 0.002 -0.000 -0.000 -0.001 0.000 -0.001 0.000 -0.000
  -0.000 0.000 -0.000 -0.000 0.000 0.003 -0.000 0.000 0.000 -0.000 -0.000]
 [0.001 -0.000 -0.003 -0.007 -0.004 -0.000 -0.000 -0.000 0.002 0.002
  0.003 0.001 0.003 0.001 0.002 -0.001 -0.000 0.001 -0.002 -0.001 -0.001
  -0.000 -0.001 0.000 0.000 0.000 0.000 0.001 -0.001 -0.000 0.001 -0.000
  0.001 0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 0.000 0.001 -0.000
  -0.000 0.000 -0.000 -0.000 0.000 -0.001 0.000 -0.000 0.000 0.000 -0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.002 -0.000 -0.000 -0.000 0.000]
 [0.001 0.002 0.000 0.004 0.002 0.001 -0.001 -0.001 -0.002 -0.001 0.003
  -0.001 -0.002 -0.001 0.001 0.001 -0.000 -0.002 0.002 0.001 -0.000
  -0.001 -0.001 0.000 -0.000 -0.000 -0.002 -0.000 0.000 -0.001 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 0.000 0.000 0.000
  0.000 -0.000 0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 0.000 -0.000
  -0.000 0.000 -0.000 -0.000 -0.000 0.000 -0.000 0.001 0.000 -0.000
  -0.000]
 [0.001 0.002 -0.005 0.024 0.003 0.001 -0.002 -0.000 -0.004 -0.002 0.000
  -0.001 -0.001 -0.001 -0.001 0.001 -0.001 -0.001 -0.000 0.000 -0.002
  -0.001 -0.001 -0.000 0.001 -0.001 -0.000 0.000 -0.002 -0.001 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 -0.001
  -0.000 0.000 0.000 -0.001 -0.001 0.000 0.000 -0.000 0.000 0.000 0.001
  -0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 0.002 -0.000
  -0.000]
 [-0.000 0.002 0.001 -0.005 -0.001 0.000 -0.001 0.001 -0.000 0.002 -0.001
  0.000 0.001 0.001 -0.000 0.000 0.001 0.000 -0.000 0.001 0.000 0.000
  0.000 -0.000 -0.000 -0.001 -0.001 0.001 -0.000 -0.000 -0.000 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 0.000 0.000 -0.000 0.000 0.000 0.001
  0.000 0.001 -0.000 0.000 0.001 0.001 0.000 0.000 -0.000 0.000 0.000
  0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000 0.001 -0.000]
 [0.004 0.002 -0.000 0.000 -0.002 -0.002 0.002 0.000 0.002 0.001 -0.002
  0.000 0.000 0.000 0.000 -0.000 0.001 0.000 -0.001 0.001 0.000 0.001
  0.000 -0.000 0.000 -0.000 -0.000 0.000 0.001 0.001 -0.001 0.000 -0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000
  -0.000 -0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 -0.000
  -0.000 0.000 0.000 -0.000 -0.000 0.000 -0.000 -0.000 -0.000 0.001]]
