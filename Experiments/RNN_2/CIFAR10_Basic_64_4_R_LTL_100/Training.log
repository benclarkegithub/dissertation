Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            786,688
├─Linear: 1-2                            65,792
=================================================================
Total params: 852,480
Trainable params: 852,480
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Latents to Latents
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToLatents                         --
├─Linear: 1-1                            272
├─Linear: 1-2                            68
├─Linear: 1-3                            68
=================================================================
Total params: 408
Trainable params: 408
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            65,792
├─Linear: 1-2                            789,504
=================================================================
Total params: 855,296
Trainable params: 855,296
Non-trainable params: 0
=================================================================
Training model 1/16...
[Epoch   1 (16.23s)]	ELBO: -17028.629 (-13855.351)	Log prob: -16995.564 (-13818.360)	KLD: 33.064 (36.991)	Grad: 185.608
[Epoch   2 (18.91s)]	ELBO: -12695.804 (-12271.096)	Log prob: -12666.733 (-12244.250)	KLD: 29.069 (26.847)	Grad: 166.220
[Epoch   3 (17.96s)]	ELBO: -12200.914 (-12121.672)	Log prob: -12175.188 (-12096.385)	KLD: 25.726 (25.287)	Grad: 145.312
[Epoch   4 (18.08s)]	ELBO: -11978.433 (-12069.124)	Log prob: -11954.678 (-12047.588)	KLD: 23.756 (21.537)	Grad: 118.977
[Epoch   5 (17.94s)]	ELBO: -11846.503 (-11896.594)	Log prob: -11823.793 (-11874.368)	KLD: 22.710 (22.225)	Grad: 118.840
[Epoch   6 (18.80s)]	ELBO: -11730.053 (-11751.256)	Log prob: -11708.322 (-11729.904)	KLD: 21.729 (21.351)	Grad: 115.135
[Epoch   7 (18.46s)]	ELBO: -11572.353 (-11624.340)	Log prob: -11551.212 (-11604.689)	KLD: 21.138 (19.651)	Grad: 117.705
[Epoch   8 (20.20s)]	ELBO: -11463.907 (-11529.648)	Log prob: -11443.233 (-11509.949)	KLD: 20.673 (19.700)	Grad: 113.624
[Epoch   9 (19.44s)]	ELBO: -11364.570 (-11391.841)	Log prob: -11344.007 (-11372.208)	KLD: 20.564 (19.633)	Grad: 118.027
[Epoch  10 (19.04s)]	ELBO: -11270.130 (-11154.160)	Log prob: -11249.488 (-11133.001)	KLD: 20.642 (21.159)	Grad: 116.052
Training model 2/16...
[Epoch   1 (30.76s)]	ELBO: -9792.182 (-8445.151)	Log prob: -9751.828 (-8403.420)	KLD: 19.498 (41.731)	Grad: 48.754
[Epoch   2 (30.56s)]	ELBO: -8230.428 (-8143.793)	Log prob: -8187.391 (-8101.454)	KLD: 22.730 (42.339)	Grad: 43.983
[Epoch   3 (30.68s)]	ELBO: -8014.881 (-7840.432)	Log prob: -7972.673 (-7798.700)	KLD: 22.067 (41.732)	Grad: 37.613
[Epoch   4 (31.00s)]	ELBO: -7913.166 (-7841.766)	Log prob: -7871.277 (-7799.861)	KLD: 21.759 (41.905)	Grad: 36.966
[Epoch   5 (30.78s)]	ELBO: -7842.250 (-7697.645)	Log prob: -7800.640 (-7656.655)	KLD: 21.446 (40.989)	Grad: 35.239
[Epoch   6 (30.53s)]	ELBO: -7775.999 (-7681.178)	Log prob: -7734.609 (-7639.432)	KLD: 21.281 (41.746)	Grad: 32.617
[Epoch   7 (34.47s)]	ELBO: -7730.261 (-7685.752)	Log prob: -7689.236 (-7644.490)	KLD: 20.945 (41.262)	Grad: 31.604
[Epoch   8 (37.08s)]	ELBO: -7696.321 (-7653.311)	Log prob: -7655.226 (-7611.448)	KLD: 20.962 (41.862)	Grad: 30.764
[Epoch   9 (35.26s)]	ELBO: -7669.239 (-7571.148)	Log prob: -7628.418 (-7529.788)	KLD: 20.801 (41.360)	Grad: 28.268
[Epoch  10 (34.31s)]	ELBO: -7640.742 (-7604.300)	Log prob: -7599.939 (-7563.404)	KLD: 20.663 (40.896)	Grad: 27.394
Training model 3/16...
[Epoch   1 (46.29s)]	ELBO: -7406.662 (-6798.894)	Log prob: -7342.587 (-6738.384)	KLD: 24.398 (60.509)	Grad: 21.867
[Epoch   2 (47.24s)]	ELBO: -6490.064 (-6121.947)	Log prob: -6430.313 (-6061.712)	KLD: 19.892 (60.235)	Grad: 22.043
[Epoch   3 (48.87s)]	ELBO: -6064.149 (-5801.123)	Log prob: -6004.844 (-5741.596)	KLD: 19.442 (59.527)	Grad: 23.285
[Epoch   4 (46.29s)]	ELBO: -5820.911 (-5740.192)	Log prob: -5762.007 (-5682.285)	KLD: 19.017 (57.906)	Grad: 22.898
[Epoch   5 (43.37s)]	ELBO: -5741.586 (-5600.726)	Log prob: -5683.035 (-5541.883)	KLD: 18.678 (58.842)	Grad: 23.229
[Epoch   6 (42.77s)]	ELBO: -5697.586 (-5649.661)	Log prob: -5639.147 (-5592.202)	KLD: 18.752 (57.459)	Grad: 19.545
[Epoch   7 (41.90s)]	ELBO: -5668.856 (-5550.885)	Log prob: -5610.541 (-5492.265)	KLD: 18.616 (58.620)	Grad: 19.357
[Epoch   8 (41.55s)]	ELBO: -5631.095 (-5581.293)	Log prob: -5573.145 (-5522.581)	KLD: 18.415 (58.712)	Grad: 17.882
[Epoch   9 (41.06s)]	ELBO: -5602.834 (-5489.256)	Log prob: -5545.054 (-5431.163)	KLD: 18.314 (58.094)	Grad: 16.323
[Epoch  10 (44.09s)]	ELBO: -5559.411 (-5480.051)	Log prob: -5502.024 (-5424.004)	KLD: 18.272 (56.047)	Grad: 16.180
Training model 4/16...
[Epoch   1 (63.41s)]	ELBO: -5634.001 (-5328.593)	Log prob: -5548.450 (-5251.924)	KLD: 29.512 (76.669)	Grad: 17.559
[Epoch   2 (62.33s)]	ELBO: -5142.276 (-4781.631)	Log prob: -5066.484 (-4707.957)	KLD: 19.762 (73.674)	Grad: 13.745
[Epoch   3 (61.28s)]	ELBO: -4717.894 (-4526.089)	Log prob: -4642.988 (-4447.740)	KLD: 18.755 (78.349)	Grad: 14.972
[Epoch   4 (60.02s)]	ELBO: -4488.214 (-4389.471)	Log prob: -4413.396 (-4314.360)	KLD: 18.656 (75.111)	Grad: 16.318
[Epoch   5 (59.93s)]	ELBO: -4399.472 (-4351.977)	Log prob: -4325.000 (-4278.155)	KLD: 18.386 (73.822)	Grad: 12.406
[Epoch   6 (59.47s)]	ELBO: -4329.465 (-4242.286)	Log prob: -4255.436 (-4166.532)	KLD: 18.096 (75.754)	Grad: 12.935
[Epoch   7 (57.92s)]	ELBO: -4266.838 (-4200.920)	Log prob: -4192.900 (-4126.448)	KLD: 18.146 (74.472)	Grad: 13.690
[Epoch   8 (58.86s)]	ELBO: -4245.644 (-4247.137)	Log prob: -4172.039 (-4173.868)	KLD: 18.055 (73.269)	Grad: 12.133
[Epoch   9 (59.62s)]	ELBO: -4225.189 (-4194.311)	Log prob: -4151.897 (-4120.470)	KLD: 17.877 (73.841)	Grad: 12.098
[Epoch  10 (59.03s)]	ELBO: -4184.141 (-4165.543)	Log prob: -4110.840 (-4092.230)	KLD: 17.939 (73.313)	Grad: 11.243
Training model 5/16...
[Epoch   1 (73.64s)]	ELBO: -4335.700 (-4116.150)	Log prob: -4229.126 (-4019.349)	KLD: 34.709 (96.801)	Grad: 13.422
[Epoch   2 (70.15s)]	ELBO: -4048.937 (-3929.098)	Log prob: -3957.128 (-3840.519)	KLD: 19.685 (88.579)	Grad: 7.705
[Epoch   3 (66.37s)]	ELBO: -3901.748 (-3816.033)	Log prob: -3812.029 (-3724.352)	KLD: 17.510 (91.682)	Grad: 7.589
[Epoch   4 (67.24s)]	ELBO: -3769.064 (-3704.465)	Log prob: -3679.586 (-3617.089)	KLD: 17.341 (87.377)	Grad: 7.083
[Epoch   5 (73.01s)]	ELBO: -3684.089 (-3663.280)	Log prob: -3594.913 (-3572.321)	KLD: 17.195 (90.959)	Grad: 6.662
[Epoch   6 (71.47s)]	ELBO: -3616.992 (-3553.119)	Log prob: -3528.143 (-3466.271)	KLD: 17.088 (86.849)	Grad: 6.256
[Epoch   7 (83.52s)]	ELBO: -3562.089 (-3527.134)	Log prob: -3473.655 (-3441.071)	KLD: 16.909 (86.063)	Grad: 6.122
[Epoch   8 (82.40s)]	ELBO: -3519.493 (-3517.734)	Log prob: -3431.072 (-3430.689)	KLD: 16.950 (87.045)	Grad: 5.850
[Epoch   9 (87.23s)]	ELBO: -3492.442 (-3480.581)	Log prob: -3404.399 (-3393.768)	KLD: 16.804 (86.813)	Grad: 5.471
[Epoch  10 (87.50s)]	ELBO: -3451.162 (-3422.157)	Log prob: -3363.372 (-3337.795)	KLD: 16.739 (84.361)	Grad: 5.941
Training model 6/16...
[Epoch   1 (116.81s)]	ELBO: -3485.288 (-3355.206)	Log prob: -3375.307 (-3250.383)	KLD: 22.911 (104.822)	Grad: 7.809
[Epoch   2 (113.19s)]	ELBO: -3263.200 (-3149.247)	Log prob: -3158.533 (-3042.389)	KLD: 17.173 (106.858)	Grad: 5.750
[Epoch   3 (119.03s)]	ELBO: -3160.708 (-3114.049)	Log prob: -3056.114 (-3010.683)	KLD: 17.148 (103.367)	Grad: 5.416
[Epoch   4 (117.58s)]	ELBO: -3058.279 (-3017.495)	Log prob: -2954.013 (-2913.809)	KLD: 17.017 (103.686)	Grad: 5.430
[Epoch   5 (113.40s)]	ELBO: -2996.511 (-2929.034)	Log prob: -2892.412 (-2827.610)	KLD: 16.958 (101.424)	Grad: 5.400
[Epoch   6 (121.90s)]	ELBO: -2925.372 (-2855.429)	Log prob: -2821.543 (-2753.773)	KLD: 16.764 (101.656)	Grad: 4.821
[Epoch   7 (102.74s)]	ELBO: -2853.802 (-2828.716)	Log prob: -2750.311 (-2724.026)	KLD: 16.582 (104.690)	Grad: 4.745
[Epoch   8 (90.77s)]	ELBO: -2826.231 (-2843.546)	Log prob: -2722.809 (-2741.536)	KLD: 16.559 (102.011)	Grad: 4.447
[Epoch   9 (93.47s)]	ELBO: -2784.891 (-2755.484)	Log prob: -2681.741 (-2653.927)	KLD: 16.437 (101.556)	Grad: 4.314
[Epoch  10 (110.60s)]	ELBO: -2740.092 (-2723.878)	Log prob: -2637.207 (-2620.075)	KLD: 16.458 (103.803)	Grad: 4.293
Training model 7/16...
[Epoch   1 (102.76s)]	ELBO: -2926.137 (-2720.332)	Log prob: -2798.187 (-2599.063)	KLD: 24.485 (121.268)	Grad: 39.256
[Epoch   2 (77.82s)]	ELBO: -2679.537 (-2639.554)	Log prob: -2559.477 (-2519.840)	KLD: 17.432 (119.714)	Grad: 10.900
[Epoch   3 (72.10s)]	ELBO: -2590.771 (-2591.542)	Log prob: -2471.973 (-2473.810)	KLD: 16.170 (117.731)	Grad: 10.088
[Epoch   4 (73.11s)]	ELBO: -2528.626 (-2494.257)	Log prob: -2409.456 (-2376.103)	KLD: 16.377 (118.154)	Grad: 10.388
[Epoch   5 (77.31s)]	ELBO: -2485.768 (-2471.743)	Log prob: -2366.577 (-2351.409)	KLD: 16.444 (120.335)	Grad: 9.955
[Epoch   6 (80.13s)]	ELBO: -2444.478 (-2453.001)	Log prob: -2325.415 (-2332.263)	KLD: 16.520 (120.737)	Grad: 9.081
[Epoch   7 (80.43s)]	ELBO: -2415.469 (-2422.236)	Log prob: -2296.314 (-2304.946)	KLD: 16.670 (117.290)	Grad: 8.852
[Epoch   8 (79.03s)]	ELBO: -2395.275 (-2413.502)	Log prob: -2276.220 (-2294.558)	KLD: 16.597 (118.943)	Grad: 9.031
[Epoch   9 (81.34s)]	ELBO: -2361.375 (-2366.301)	Log prob: -2242.433 (-2249.515)	KLD: 16.611 (116.786)	Grad: 8.729
[Epoch  10 (98.66s)]	ELBO: -2322.229 (-2329.193)	Log prob: -2203.400 (-2211.014)	KLD: 16.561 (118.179)	Grad: 7.886
Training model 8/16...
[Epoch   1 (119.07s)]	ELBO: -2438.735 (-2300.857)	Log prob: -2302.637 (-2166.852)	KLD: 18.020 (134.005)	Grad: 5.709
[Epoch   2 (99.38s)]	ELBO: -2250.966 (-2201.467)	Log prob: -2116.355 (-2066.110)	KLD: 16.515 (135.357)	Grad: 4.070
[Epoch   3 (96.64s)]	ELBO: -2176.537 (-2230.452)	Log prob: -2041.799 (-2096.457)	KLD: 16.757 (133.995)	Grad: 3.839
[Epoch   4 (92.51s)]	ELBO: -2121.590 (-2105.910)	Log prob: -1987.160 (-1971.747)	KLD: 16.766 (134.163)	Grad: 4.082
[Epoch   5 (91.58s)]	ELBO: -2057.933 (-2036.555)	Log prob: -1923.220 (-1900.318)	KLD: 16.863 (136.238)	Grad: 3.980
[Epoch   6 (90.37s)]	ELBO: -2018.081 (-2003.483)	Log prob: -1883.379 (-1866.637)	KLD: 16.992 (136.847)	Grad: 3.890
[Epoch   7 (100.35s)]	ELBO: -1992.190 (-2004.470)	Log prob: -1857.799 (-1865.037)	KLD: 16.936 (139.433)	Grad: 3.802
[Epoch   8 (96.81s)]	ELBO: -1960.064 (-2004.949)	Log prob: -1825.694 (-1874.567)	KLD: 16.956 (130.382)	Grad: 3.719
[Epoch   9 (97.96s)]	ELBO: -1940.423 (-1949.606)	Log prob: -1806.291 (-1819.628)	KLD: 16.883 (129.979)	Grad: 3.461
[Epoch  10 (99.62s)]	ELBO: -1911.105 (-1898.516)	Log prob: -1777.258 (-1763.803)	KLD: 16.668 (134.714)	Grad: 3.564
Training model 9/16...
[Epoch   1 (134.72s)]	ELBO: -2057.146 (-1872.130)	Log prob: -1905.153 (-1722.145)	KLD: 18.764 (149.984)	Grad: 5.479
[Epoch   2 (124.42s)]	ELBO: -1838.076 (-1800.320)	Log prob: -1687.745 (-1648.573)	KLD: 16.883 (151.747)	Grad: 3.688
[Epoch   3 (106.78s)]	ELBO: -1791.607 (-1788.381)	Log prob: -1641.415 (-1636.482)	KLD: 16.886 (151.899)	Grad: 3.656
[Epoch   4 (94.95s)]	ELBO: -1766.174 (-1747.018)	Log prob: -1615.971 (-1599.306)	KLD: 16.989 (147.712)	Grad: 3.657
[Epoch   5 (100.79s)]	ELBO: -1740.046 (-1728.620)	Log prob: -1589.955 (-1578.560)	KLD: 16.994 (150.060)	Grad: 3.542
[Epoch   6 (105.01s)]	ELBO: -1715.254 (-1778.210)	Log prob: -1565.401 (-1629.329)	KLD: 16.896 (148.881)	Grad: 3.509
[Epoch   7 (107.66s)]	ELBO: -1682.936 (-1657.676)	Log prob: -1533.487 (-1508.010)	KLD: 16.820 (149.666)	Grad: 3.347
[Epoch   8 (108.37s)]	ELBO: -1638.203 (-1625.392)	Log prob: -1488.904 (-1475.811)	KLD: 16.760 (149.582)	Grad: 3.301
[Epoch   9 (107.34s)]	ELBO: -1606.449 (-1613.013)	Log prob: -1457.484 (-1461.664)	KLD: 16.662 (151.349)	Grad: 3.265
[Epoch  10 (107.98s)]	ELBO: -1575.257 (-1570.953)	Log prob: -1426.400 (-1421.644)	KLD: 16.620 (149.308)	Grad: 3.171
Training model 10/16...
[Epoch   1 (120.39s)]	ELBO: -1763.782 (-1599.959)	Log prob: -1581.723 (-1422.156)	KLD: 34.864 (177.803)	Grad: 12.217
[Epoch   2 (114.46s)]	ELBO: -1532.370 (-1523.658)	Log prob: -1358.473 (-1352.391)	KLD: 26.611 (171.268)	Grad: 3.693
[Epoch   3 (113.29s)]	ELBO: -1488.552 (-1471.094)	Log prob: -1320.517 (-1305.239)	KLD: 20.675 (165.855)	Grad: 3.239
[Epoch   4 (114.65s)]	ELBO: -1455.791 (-1438.210)	Log prob: -1290.267 (-1273.862)	KLD: 18.159 (164.348)	Grad: 3.021
[Epoch   5 (115.89s)]	ELBO: -1423.541 (-1390.818)	Log prob: -1258.833 (-1226.812)	KLD: 17.415 (164.006)	Grad: 2.960
[Epoch   6 (117.34s)]	ELBO: -1379.048 (-1348.544)	Log prob: -1214.922 (-1185.460)	KLD: 16.994 (163.084)	Grad: 2.968
[Epoch   7 (121.30s)]	ELBO: -1352.411 (-1371.873)	Log prob: -1188.402 (-1207.288)	KLD: 16.844 (164.585)	Grad: 2.846
[Epoch   8 (127.43s)]	ELBO: -1335.425 (-1320.338)	Log prob: -1171.514 (-1155.215)	KLD: 16.713 (165.123)	Grad: 2.716
[Epoch   9 (121.43s)]	ELBO: -1319.012 (-1307.196)	Log prob: -1155.452 (-1144.464)	KLD: 16.644 (162.732)	Grad: 2.791
[Epoch  10 (123.82s)]	ELBO: -1305.100 (-1305.654)	Log prob: -1141.658 (-1141.151)	KLD: 16.691 (164.502)	Grad: 2.662
Training model 11/16...
[Epoch   1 (142.60s)]	ELBO: -1608.520 (-1367.629)	Log prob: -1419.538 (-1182.542)	KLD: 26.586 (185.087)	Grad: 33.424
[Epoch   2 (143.69s)]	ELBO: -1348.941 (-1316.153)	Log prob: -1161.495 (-1129.200)	KLD: 25.826 (186.953)	Grad: 3.887
[Epoch   3 (148.13s)]	ELBO: -1305.143 (-1287.882)	Log prob: -1119.463 (-1101.549)	KLD: 24.090 (186.333)	Grad: 3.329
[Epoch   4 (148.83s)]	ELBO: -1269.331 (-1289.196)	Log prob: -1085.376 (-1108.495)	KLD: 22.564 (180.701)	Grad: 3.243
[Epoch   5 (151.43s)]	ELBO: -1234.925 (-1254.887)	Log prob: -1052.671 (-1074.030)	KLD: 20.979 (180.858)	Grad: 3.113
[Epoch   6 (147.83s)]	ELBO: -1211.821 (-1231.734)	Log prob: -1030.771 (-1052.661)	KLD: 19.879 (179.073)	Grad: 3.023
[Epoch   7 (146.45s)]	ELBO: -1184.583 (-1185.984)	Log prob: -1004.773 (-1005.718)	KLD: 18.929 (180.266)	Grad: 2.872
[Epoch   8 (145.37s)]	ELBO: -1165.739 (-1172.316)	Log prob: -986.769 (-992.006)	KLD: 18.266 (180.310)	Grad: 2.919
[Epoch   9 (154.41s)]	ELBO: -1148.820 (-1140.007)	Log prob: -970.400 (-960.684)	KLD: 18.032 (179.324)	Grad: 2.875
[Epoch  10 (156.88s)]	ELBO: -1124.591 (-1133.346)	Log prob: -946.629 (-955.618)	KLD: 17.545 (177.728)	Grad: 2.687
Training model 12/16...
[Epoch   1 (185.72s)]	ELBO: -1397.105 (-1161.237)	Log prob: -1206.971 (-969.668)	KLD: 13.372 (191.569)	Grad: 24.322
[Epoch   2 (188.16s)]	ELBO: -1124.122 (-1105.049)	Log prob: -933.084 (-911.592)	KLD: 14.715 (193.457)	Grad: 10.260
[Epoch   3 (161.90s)]	ELBO: -1081.027 (-1114.697)	Log prob: -890.157 (-922.578)	KLD: 14.914 (192.119)	Grad: 9.540
[Epoch   4 (178.94s)]	ELBO: -1050.237 (-1043.736)	Log prob: -859.466 (-853.164)	KLD: 14.977 (190.572)	Grad: 7.598
[Epoch   5 (195.89s)]	ELBO: -1011.799 (-1014.740)	Log prob: -821.420 (-822.884)	KLD: 14.931 (191.856)	Grad: 6.347
[Epoch   6 (182.95s)]	ELBO: -980.600 (-988.757)	Log prob: -790.567 (-800.614)	KLD: 14.870 (188.143)	Grad: 5.722
[Epoch   7 (168.14s)]	ELBO: -963.657 (-950.331)	Log prob: -773.904 (-760.540)	KLD: 14.854 (189.790)	Grad: 5.085
[Epoch   8 (162.64s)]	ELBO: -933.766 (-927.233)	Log prob: -744.383 (-739.185)	KLD: 14.754 (188.048)	Grad: 5.460
[Epoch   9 (168.76s)]	ELBO: -900.581 (-898.955)	Log prob: -711.458 (-710.696)	KLD: 14.797 (188.259)	Grad: 5.420
[Epoch  10 (158.73s)]	ELBO: -875.517 (-889.318)	Log prob: -686.667 (-699.127)	KLD: 14.834 (190.191)	Grad: 5.461
Training model 13/16...
[Epoch   1 (171.50s)]	ELBO: -1161.500 (-966.544)	Log prob: -956.028 (-761.528)	KLD: 16.868 (205.016)	Grad: 17.076
[Epoch   2 (175.95s)]	ELBO: -888.806 (-862.678)	Log prob: -683.835 (-657.116)	KLD: 16.677 (205.561)	Grad: 7.713
[Epoch   3 (134.19s)]	ELBO: -840.217 (-860.951)	Log prob: -635.639 (-657.126)	KLD: 16.306 (203.825)	Grad: 7.241
[Epoch   4 (138.01s)]	ELBO: -815.136 (-791.677)	Log prob: -610.352 (-587.515)	KLD: 16.102 (204.162)	Grad: 7.089
[Epoch   5 (136.54s)]	ELBO: -771.264 (-787.052)	Log prob: -567.319 (-582.791)	KLD: 15.989 (204.261)	Grad: 6.064
[Epoch   6 (147.19s)]	ELBO: -756.357 (-769.691)	Log prob: -552.590 (-567.071)	KLD: 15.899 (202.620)	Grad: 5.433
[Epoch   7 (143.77s)]	ELBO: -732.465 (-744.809)	Log prob: -529.244 (-542.224)	KLD: 15.790 (202.585)	Grad: 5.236
[Epoch   8 (142.13s)]	ELBO: -725.693 (-746.748)	Log prob: -522.855 (-544.787)	KLD: 15.705 (201.961)	Grad: 4.952
[Epoch   9 (124.99s)]	ELBO: -715.061 (-721.978)	Log prob: -512.491 (-519.607)	KLD: 15.651 (202.372)	Grad: 5.314
[Epoch  10 (130.75s)]	ELBO: -704.894 (-732.088)	Log prob: -502.847 (-529.358)	KLD: 15.567 (202.730)	Grad: 5.137
Training model 14/16...
[Epoch   1 (144.59s)]	ELBO: -990.938 (-774.866)	Log prob: -774.952 (-558.821)	KLD: 13.918 (216.045)	Grad: 18.898
[Epoch   2 (157.57s)]	ELBO: -742.427 (-729.022)	Log prob: -526.629 (-513.792)	KLD: 14.299 (215.230)	Grad: 11.793
[Epoch   3 (160.89s)]	ELBO: -704.244 (-711.549)	Log prob: -488.586 (-495.902)	KLD: 14.425 (215.647)	Grad: 8.912
[Epoch   4 (168.15s)]	ELBO: -669.727 (-675.145)	Log prob: -454.341 (-460.680)	KLD: 14.488 (214.465)	Grad: 7.866
[Epoch   5 (157.49s)]	ELBO: -645.685 (-657.179)	Log prob: -430.702 (-440.927)	KLD: 14.508 (216.252)	Grad: 6.416
[Epoch   6 (147.32s)]	ELBO: -634.771 (-647.710)	Log prob: -420.301 (-433.574)	KLD: 14.488 (214.136)	Grad: 5.599
[Epoch   7 (140.73s)]	ELBO: -619.891 (-620.514)	Log prob: -405.906 (-407.251)	KLD: 14.462 (213.263)	Grad: 5.790
[Epoch   8 (135.56s)]	ELBO: -602.170 (-616.844)	Log prob: -388.541 (-403.201)	KLD: 14.496 (213.642)	Grad: 6.870
[Epoch   9 (137.35s)]	ELBO: -569.375 (-587.478)	Log prob: -356.263 (-374.808)	KLD: 14.499 (212.670)	Grad: 6.696
[Epoch  10 (141.44s)]	ELBO: -541.462 (-546.920)	Log prob: -328.696 (-334.943)	KLD: 14.467 (211.977)	Grad: 6.952
Training model 15/16...
[Epoch   1 (163.01s)]	ELBO: -760.581 (-580.725)	Log prob: -532.448 (-352.567)	KLD: 15.621 (228.157)	Grad: 16.102
[Epoch   2 (169.38s)]	ELBO: -551.126 (-539.859)	Log prob: -323.143 (-311.089)	KLD: 15.780 (228.770)	Grad: 6.469
[Epoch   3 (168.15s)]	ELBO: -514.832 (-558.399)	Log prob: -287.267 (-331.399)	KLD: 15.716 (227.000)	Grad: 6.878
[Epoch   4 (150.34s)]	ELBO: -488.686 (-503.060)	Log prob: -261.816 (-276.870)	KLD: 15.586 (226.190)	Grad: 6.295
[Epoch   5 (154.16s)]	ELBO: -475.418 (-500.368)	Log prob: -249.187 (-274.929)	KLD: 15.479 (225.439)	Grad: 5.719
[Epoch   6 (165.13s)]	ELBO: -460.685 (-524.969)	Log prob: -235.055 (-300.270)	KLD: 15.449 (224.700)	Grad: 5.612
[Epoch   7 (286.99s)]	ELBO: -446.838 (-485.889)	Log prob: -221.752 (-261.982)	KLD: 15.417 (223.908)	Grad: 5.567
[Epoch   8 (356.99s)]	ELBO: -432.197 (-504.495)	Log prob: -207.792 (-278.797)	KLD: 15.370 (225.699)	Grad: 5.614
[Epoch   9 (339.37s)]	ELBO: -421.696 (-474.310)	Log prob: -197.782 (-249.914)	KLD: 15.324 (224.397)	Grad: 5.339
[Epoch  10 (335.38s)]	ELBO: -411.949 (-447.119)	Log prob: -188.584 (-223.316)	KLD: 15.269 (223.803)	Grad: 4.733
Training model 16/16...
[Epoch   1 (366.51s)]	ELBO: -632.006 (-540.620)	Log prob: -393.518 (-303.313)	KLD: 15.959 (237.306)	Grad: 8.338
[Epoch   2 (374.62s)]	ELBO: -422.712 (-424.536)	Log prob: -184.872 (-187.299)	KLD: 15.597 (237.237)	Grad: 3.827
[Epoch   3 (355.09s)]	ELBO: -377.225 (-416.335)	Log prob: -139.678 (-179.260)	KLD: 15.678 (237.076)	Grad: 2.852
[Epoch   4 (368.48s)]	ELBO: -351.196 (-365.580)	Log prob: -114.154 (-127.553)	KLD: 15.627 (238.027)	Grad: 2.626
[Epoch   5 (376.57s)]	ELBO: -329.982 (-357.041)	Log prob: -93.274 (-120.907)	KLD: 15.656 (236.134)	Grad: 2.336
[Epoch   6 (358.72s)]	ELBO: -307.698 (-330.643)	Log prob: -71.358 (-95.496)	KLD: 15.682 (235.147)	Grad: 2.138
[Epoch   7 (357.56s)]	ELBO: -295.707 (-352.156)	Log prob: -59.973 (-115.904)	KLD: 15.659 (236.252)	Grad: 2.086
[Epoch   8 (359.77s)]	ELBO: -276.547 (-301.893)	Log prob: -41.238 (-65.397)	KLD: 15.656 (236.496)	Grad: 1.970
[Epoch   9 (370.14s)]	ELBO: -264.586 (-274.848)	Log prob: -29.771 (-41.053)	KLD: 15.689 (233.795)	Grad: 1.968
[Epoch  10 (357.35s)]	ELBO: -254.126 (-282.357)	Log prob: -19.933 (-49.129)	KLD: 15.698 (233.228)	Grad: 1.947
Best epoch(s): [10, 9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 9, 10, 10, 9]	Training time(s): 185.06s, 325.44s, 443.43s, 601.86s, 762.52s, 1099.50s, 822.69s, 984.29s, 1098.02s, 1190.00s, 1485.63s, 1751.83s, 1445.02s, 1491.09s, 2288.90s, 3644.82s (19620.10s)	Best ELBO: -254.126 (-274.848)	Best log prob: -19.933 (-41.053)
Avg. mu: 0.242, 0.243, -0.064, 0.245, -0.148, 0.218, -0.107, 0.134, -0.068, 0.201, -0.126, 0.156, -0.108, 0.187, -0.073, 0.156, -0.066, 0.202, -0.029, 0.142, -0.062, 0.205, -0.061, 0.192, -0.053, 0.238, -0.036, 0.184, -0.068, 0.263, -0.016, 0.183, -0.108, 0.252, -0.044, 0.075, -0.039, 0.238, -0.096, 0.158, -0.109, 0.278, -0.042, 0.163, 0.093, 0.262, 0.316, 0.184, 0.152, 0.281, 0.188, 0.154, 0.034, 0.203, 0.289, 0.163, 0.180, 0.267, 0.132, 0.198, -0.096, 0.264, -0.107, 0.130
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.000, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.003, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Max. mu: 3.849, 3.421, 4.016, 3.506, 3.361, 3.595, 3.313, 3.584, 4.016, 4.619, 2.957, 3.073, 2.686, 2.665, 2.368, 2.817, 1.841, 2.234, 2.615, 2.888, 2.352, 2.077, 1.959, 2.203, 1.863, 1.953, 2.284, 2.008, 1.563, 1.773, 1.789, 1.889, 1.543, 1.456, 1.949, 1.636, 0.993, 1.217, 1.116, 1.586, 1.007, 1.650, 1.262, 2.392, 1.261, 0.619, 0.993, 0.554, 0.616, 0.594, 1.197, 0.449, 0.461, 0.370, 1.473, 0.716, 0.486, 0.458, 0.493, 0.315, 0.477, 0.453, 0.121, 0.341
Max. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.001, 0.001, 0.002, 0.001, 0.001, 0.001, 0.006, 0.004, 0.004, 0.005, 0.005, 0.003, 0.003, 0.003, 0.002, 0.002, 0.003, 0.003, 0.001, 0.001, 0.001, 0.003, 0.002, 0.003, 0.003, 0.003, 0.003, 0.002, 0.003, 0.005, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.002, 0.003, 0.005, 0.004, 0.021, 0.001, 0.001, 0.002, 0.013, 0.001, 0.000, 0.001, 0.041, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000
Min. mu: -3.056, -2.574, -4.670, -4.899, -3.462, -2.413, -3.484, -3.519, -3.146, -2.710, -2.908, -3.676, -2.147, -2.388, -2.398, -3.417, -2.224, -1.745, -2.504, -2.203, -2.170, -1.619, -2.271, -1.650, -2.863, -1.226, -2.123, -1.694, -2.164, -1.045, -2.010, -2.384, -1.799, -0.965, -2.109, -1.427, -1.163, -0.990, -1.557, -1.112, -1.498, -0.696, -0.835, -0.693, -0.521, -0.214, -0.210, -0.353, -0.289, -0.081, -0.148, -0.083, -0.424, -0.043, -0.129, -0.220, -0.023, 0.023, -0.080, 0.055, -0.365, 0.123, -0.346, -0.115
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.691 0.031 0.023 ... -0.000 0.001 0.003]
 [0.031 0.709 -0.017 ... -0.000 0.000 -0.000]
 [0.023 -0.017 0.763 ... -0.002 0.003 0.001]
 ...
 [-0.000 -0.000 -0.002 ... 0.000 -0.000 0.000]
 [0.001 0.000 0.003 ... -0.000 0.001 -0.000]
 [0.003 -0.000 0.001 ... 0.000 -0.000 0.001]]
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
RNN                                      --
├─Canvas: 1-1                            --
│    └─Linear: 2-1                       3,072
├─Encoder: 1-2                           --
│    └─Linear: 2-2                       786,688
│    └─Linear: 2-3                       65,792
├─ModuleList: 1-3                        --
│    └─EncoderToLatents: 2-4             --
│    │    └─Linear: 3-1                  1,028
│    │    └─Linear: 3-2                  1,028
│    └─EncoderToLatents: 2-5             --
│    │    └─Linear: 3-3                  1,028
│    │    └─Linear: 3-4                  1,028
│    └─EncoderToLatents: 2-6             --
│    │    └─Linear: 3-5                  1,028
│    │    └─Linear: 3-6                  1,028
│    └─EncoderToLatents: 2-7             --
│    │    └─Linear: 3-7                  1,028
│    │    └─Linear: 3-8                  1,028
│    └─EncoderToLatents: 2-8             --
│    │    └─Linear: 3-9                  1,028
│    │    └─Linear: 3-10                 1,028
│    └─EncoderToLatents: 2-9             --
│    │    └─Linear: 3-11                 1,028
│    │    └─Linear: 3-12                 1,028
│    └─EncoderToLatents: 2-10            --
│    │    └─Linear: 3-13                 1,028
│    │    └─Linear: 3-14                 1,028
│    └─EncoderToLatents: 2-11            --
│    │    └─Linear: 3-15                 1,028
│    │    └─Linear: 3-16                 1,028
│    └─EncoderToLatents: 2-12            --
│    │    └─Linear: 3-17                 1,028
│    │    └─Linear: 3-18                 1,028
│    └─EncoderToLatents: 2-13            --
│    │    └─Linear: 3-19                 1,028
│    │    └─Linear: 3-20                 1,028
│    └─EncoderToLatents: 2-14            --
│    │    └─Linear: 3-21                 1,028
│    │    └─Linear: 3-22                 1,028
│    └─EncoderToLatents: 2-15            --
│    │    └─Linear: 3-23                 1,028
│    │    └─Linear: 3-24                 1,028
│    └─EncoderToLatents: 2-16            --
│    │    └─Linear: 3-25                 1,028
│    │    └─Linear: 3-26                 1,028
│    └─EncoderToLatents: 2-17            --
│    │    └─Linear: 3-27                 1,028
│    │    └─Linear: 3-28                 1,028
│    └─EncoderToLatents: 2-18            --
│    │    └─Linear: 3-29                 1,028
│    │    └─Linear: 3-30                 1,028
│    └─EncoderToLatents: 2-19            --
│    │    └─Linear: 3-31                 1,028
│    │    └─Linear: 3-32                 1,028
├─LatentsToLatents: 1-4                  --
│    └─Linear: 2-20                      272
│    └─Linear: 2-21                      68
│    └─Linear: 2-22                      68
├─ModuleList: 1-5                        --
│    └─LatentsToDecoder: 2-23            --
│    │    └─Linear: 3-33                 1,280
│    └─LatentsToDecoder: 2-24            --
│    │    └─Linear: 3-34                 1,280
│    └─LatentsToDecoder: 2-25            --
│    │    └─Linear: 3-35                 1,280
│    └─LatentsToDecoder: 2-26            --
│    │    └─Linear: 3-36                 1,280
│    └─LatentsToDecoder: 2-27            --
│    │    └─Linear: 3-37                 1,280
│    └─LatentsToDecoder: 2-28            --
│    │    └─Linear: 3-38                 1,280
│    └─LatentsToDecoder: 2-29            --
│    │    └─Linear: 3-39                 1,280
│    └─LatentsToDecoder: 2-30            --
│    │    └─Linear: 3-40                 1,280
│    └─LatentsToDecoder: 2-31            --
│    │    └─Linear: 3-41                 1,280
│    └─LatentsToDecoder: 2-32            --
│    │    └─Linear: 3-42                 1,280
│    └─LatentsToDecoder: 2-33            --
│    │    └─Linear: 3-43                 1,280
│    └─LatentsToDecoder: 2-34            --
│    │    └─Linear: 3-44                 1,280
│    └─LatentsToDecoder: 2-35            --
│    │    └─Linear: 3-45                 1,280
│    └─LatentsToDecoder: 2-36            --
│    │    └─Linear: 3-46                 1,280
│    └─LatentsToDecoder: 2-37            --
│    │    └─Linear: 3-47                 1,280
│    └─LatentsToDecoder: 2-38            --
│    │    └─Linear: 3-48                 1,280
├─Decoder: 1-6                           --
│    └─Linear: 2-39                      65,792
│    └─Linear: 2-40                      789,504
=================================================================
Total params: 1,764,632
Trainable params: 1,764,632
Non-trainable params: 0
=================================================================
[Epoch   1 (179.41s)]	ELBO: -10932.946, -7188.409, -5146.556, -3768.569, -2842.075, -2143.805, -1570.421, -1092.313, -695.220, -382.357, -268.336, -243.035, -231.345, -228.134, -233.642, -252.109 (-259.244)	Log prob: -10913.617, -7153.083, -5095.938, -3703.411, -2762.850, -2050.872, -1463.349, -971.531, -560.729, -233.625, -105.593, -67.342, -42.110, -24.940, -15.606, -18.447 (-25.921)	KLD: 19.327, 16.000, 15.293, 14.539, 14.068, 13.708, 14.138, 13.709, 13.711, 14.241, 14.010, 12.951, 13.541, 13.959, 14.842, 15.626 (233.323)	Grad: 196988.094
[Epoch   2 (176.06s)]	ELBO: -10938.683, -7192.312, -5146.612, -3767.895, -2841.561, -2140.609, -1565.748, -1085.932, -687.078, -371.161, -254.251, -228.686, -217.733, -213.560, -219.338, -234.861 (-257.220)	Log prob: -10919.315, -7157.000, -5095.989, -3702.769, -2762.401, -2047.774, -1458.823, -965.308, -552.769, -222.641, -91.772, -53.343, -28.927, -10.805, -1.778, -1.577 (-25.877)	KLD: 19.366, 15.956, 15.304, 14.497, 14.036, 13.676, 14.091, 13.697, 13.685, 14.212, 13.959, 12.864, 13.463, 13.949, 14.804, 15.725 (231.344)	Grad: 202426.531
[Epoch   3 (228.71s)]	ELBO: -10937.454, -7190.066, -5147.071, -3770.019, -2843.500, -2143.317, -1568.374, -1086.763, -686.982, -370.549, -250.368, -224.129, -212.944, -208.692, -214.124, -228.708 (-291.729)	Log prob: -10918.119, -7154.777, -5096.507, -3704.965, -2764.393, -2050.548, -1461.554, -966.258, -552.817, -222.210, -88.155, -49.164, -24.602, -6.459, 2.858, 3.976 (-58.988)	KLD: 19.333, 15.958, 15.270, 14.489, 14.055, 13.663, 14.050, 13.685, 13.661, 14.173, 13.875, 12.752, 13.378, 13.891, 14.749, 15.701 (232.741)	Grad: 202835.391
[Epoch   4 (185.25s)]	ELBO: -10934.684, -7187.690, -5143.058, -3763.791, -2837.354, -2137.413, -1562.901, -1082.032, -683.598, -366.194, -240.777, -212.712, -199.806, -194.584, -198.801, -212.803 (-252.240)	Log prob: -10915.336, -7152.367, -5092.462, -3698.722, -2758.258, -2044.658, -1456.130, -961.585, -549.527, -217.948, -78.698, -37.952, -11.797, 7.276, 17.782, 19.522 (-20.811)	KLD: 19.351, 15.972, 15.273, 14.474, 14.027, 13.656, 14.017, 13.677, 13.624, 14.175, 13.833, 12.681, 13.249, 13.851, 14.722, 15.743 (231.429)	Grad: 212051.312
[Epoch   5 (180.33s)]	ELBO: -10944.099, -7192.552, -5147.126, -3767.696, -2839.093, -2139.068, -1564.246, -1082.642, -681.984, -363.994, -232.111, -202.696, -189.388, -184.115, -187.260, -199.718 (-264.924)	Log prob: -10924.754, -7157.269, -5096.592, -3702.688, -2760.080, -2046.431, -1457.635, -962.404, -548.201, -216.103, -70.376, -28.388, -1.890, 17.223, 28.780, 31.991 (-33.384)	KLD: 19.341, 15.941, 15.251, 14.476, 14.004, 13.624, 13.975, 13.626, 13.545, 14.108, 13.844, 12.572, 13.190, 13.841, 14.703, 15.667 (231.540)	Grad: 211655.422
[Epoch   6 (181.96s)]	ELBO: -10951.204, -7205.336, -5156.165, -3774.324, -2845.077, -2144.346, -1567.768, -1085.905, -686.093, -367.009, -226.500, -195.110, -180.584, -174.079, -177.023, -188.387 (-217.661)	Log prob: -10931.846, -7170.007, -5105.605, -3709.277, -2766.051, -2051.733, -1461.258, -965.794, -552.414, -219.259, -64.937, -21.090, 6.551, 26.862, 38.591, 42.890 (13.355)	KLD: 19.360, 15.969, 15.230, 14.489, 13.981, 13.583, 13.898, 13.601, 13.569, 14.071, 13.813, 12.457, 13.115, 13.806, 14.672, 15.663 (231.015)	Grad: 215110.156
[Epoch   7 (190.25s)]	ELBO: -10925.449, -7179.693, -5133.147, -3755.151, -2829.361, -2130.695, -1554.622, -1073.047, -673.483, -354.307, -205.201, -173.283, -158.761, -152.657, -154.903, -164.938 (-221.875)	Log prob: -10906.072, -7144.395, -5082.610, -3690.144, -2750.405, -2038.176, -1448.194, -953.033, -540.005, -206.753, -43.856, 0.445, 27.999, 47.926, 60.348, 65.918 (9.310)	KLD: 19.370, 15.932, 15.234, 14.471, 13.948, 13.563, 13.911, 13.585, 13.464, 14.076, 13.791, 12.383, 13.033, 13.822, 14.668, 15.606 (231.185)	Grad: 208488.297
[Epoch   8 (235.10s)]	ELBO: -10936.785, -7192.333, -5144.775, -3764.857, -2836.515, -2135.395, -1559.410, -1077.171, -676.820, -357.083, -199.847, -166.463, -150.612, -143.974, -145.385, -154.650 (-182.881)	Log prob: -10917.449, -7157.018, -5094.202, -3699.829, -2757.525, -2042.865, -1453.042, -957.278, -543.448, -209.683, -38.655, 7.025, 35.830, 56.236, 69.456, 75.764 (47.590)	KLD: 19.333, 15.980, 15.263, 14.451, 13.962, 13.540, 13.839, 13.526, 13.479, 14.028, 13.792, 12.296, 12.954, 13.768, 14.631, 15.573 (230.471)	Grad: 217728.359
[Epoch   9 (234.74s)]	ELBO: -10944.022, -7195.891, -5148.371, -3764.969, -2836.191, -2135.030, -1559.666, -1078.827, -678.786, -357.685, -194.261, -158.611, -141.732, -134.951, -135.350, -143.932 (-184.114)	Log prob: -10924.667, -7160.626, -5097.859, -3699.983, -2757.276, -2042.591, -1453.429, -959.085, -545.609, -210.520, -33.331, 14.504, 44.195, 64.710, 78.929, 85.878 (45.910)	KLD: 19.359, 15.904, 15.251, 14.473, 13.928, 13.525, 13.796, 13.506, 13.435, 13.987, 13.765, 12.185, 12.812, 13.734, 14.619, 15.532 (230.025)	Grad: 219364.297
[Epoch  10 (188.00s)]	ELBO: -10927.965, -7184.866, -5143.068, -3759.835, -2829.250, -2129.509, -1553.447, -1072.411, -672.454, -350.622, -182.957, -146.725, -129.363, -122.713, -122.518, -129.704 (-159.619)	Log prob: -10908.606, -7149.572, -5092.524, -3694.817, -2750.299, -2037.037, -1447.216, -952.718, -539.354, -203.591, -22.144, 26.215, 56.275, 76.610, 91.370, 99.618 (70.502)	KLD: 19.363, 15.932, 15.249, 14.473, 13.934, 13.523, 13.757, 13.462, 13.407, 13.932, 13.782, 12.126, 12.698, 13.685, 14.565, 15.434 (230.121)	Grad: 225082.578
[Epoch  11 (186.97s)]	ELBO: -10944.200, -7195.467, -5144.119, -3761.879, -2832.182, -2130.412, -1553.519, -1071.879, -670.584, -348.412, -175.365, -136.621, -121.318, -114.096, -113.379, -120.346 (-149.162)	Log prob: -10924.843, -7160.168, -5093.570, -3696.901, -2753.276, -2038.016, -1447.399, -952.303, -537.613, -201.561, -14.800, 35.952, 63.829, 84.697, 99.939, 108.358 (79.001)	KLD: 19.366, 15.934, 15.251, 14.428, 13.928, 13.489, 13.724, 13.457, 13.394, 13.881, 13.714, 12.007, 12.575, 13.646, 14.525, 15.386 (228.163)	Grad: 199826.672
[Epoch  12 (185.74s)]	ELBO: -10939.337, -7192.204, -5146.858, -3764.734, -2834.323, -2133.789, -1556.951, -1074.429, -673.807, -350.148, -171.951, -130.995, -113.019, -105.561, -103.675, -109.182 (-183.135)	Log prob: -10919.974, -7156.913, -5096.351, -3699.789, -2755.472, -2041.465, -1450.938, -954.998, -541.051, -203.555, -11.650, 41.253, 71.635, 92.705, 109.073, 118.985 (44.342)	KLD: 19.364, 15.928, 15.214, 14.439, 13.906, 13.473, 13.689, 13.418, 13.325, 13.837, 13.708, 11.947, 12.406, 13.613, 14.482, 15.419 (227.477)	Grad: 219261.438
[Epoch  13 (185.45s)]	ELBO: -10945.036, -7195.980, -5150.636, -3766.622, -2835.253, -2133.346, -1555.937, -1074.263, -673.178, -349.774, -167.185, -121.692, -103.568, -95.476, -93.235, -98.621 (-117.847)	Log prob: -10925.710, -7160.709, -5100.116, -3701.661, -2756.394, -2041.021, -1449.956, -954.940, -540.576, -203.372, -7.112, 50.168, 80.617, 102.280, 118.947, 128.927 (109.095)	KLD: 19.327, 15.943, 15.252, 14.436, 13.901, 13.466, 13.654, 13.342, 13.280, 13.799, 13.671, 11.787, 12.325, 13.571, 14.425, 15.367 (226.941)	Grad: 226988.297
[Epoch  14 (184.42s)]	ELBO: -10944.608, -7195.255, -5146.274, -3763.881, -2832.774, -2130.514, -1553.066, -1070.463, -668.503, -343.590, -155.108, -109.211, -92.648, -84.693, -82.152, -86.950 (-132.600)	Log prob: -10925.241, -7159.944, -5095.771, -3698.933, -2753.951, -2038.279, -1447.184, -951.243, -536.037, -197.382, 4.817, 62.363, 91.034, 112.536, 129.441, 139.964 (94.492)	KLD: 19.363, 15.946, 15.195, 14.446, 13.873, 13.413, 13.646, 13.338, 13.246, 13.742, 13.716, 11.650, 12.108, 13.547, 14.364, 15.322 (227.092)	Grad: 203920.812
[Epoch  15 (183.50s)]	ELBO: -10944.209, -7197.566, -5148.361, -3765.592, -2832.049, -2130.891, -1553.330, -1071.163, -669.359, -344.133, -150.354, -101.376, -83.633, -75.552, -72.817, -77.081 (-118.652)	Log prob: -10924.865, -7162.263, -5097.828, -3700.629, -2753.219, -2038.646, -1447.507, -952.071, -537.038, -198.100, 9.353, 69.866, 99.550, 121.142, 138.200, 149.165 (107.538)	KLD: 19.339, 15.966, 15.223, 14.435, 13.869, 13.412, 13.581, 13.268, 13.228, 13.712, 13.674, 11.535, 11.941, 13.511, 14.323, 15.230 (226.191)	Grad: 223245.672
[Epoch  16 (184.78s)]	ELBO: -10957.691, -7196.486, -5149.014, -3767.564, -2836.649, -2134.478, -1556.032, -1072.944, -671.597, -345.000, -144.420, -97.037, -77.955, -69.925, -66.924, -70.462 (-117.849)	Log prob: -10938.347, -7161.210, -5098.470, -3702.612, -2757.854, -2042.281, -1450.242, -953.883, -539.307, -199.052, 15.209, 74.116, 105.004, 126.541, 143.870, 155.623 (108.577)	KLD: 19.344, 15.928, 15.272, 14.407, 13.844, 13.402, 13.593, 13.270, 13.230, 13.658, 13.681, 11.524, 11.807, 13.507, 14.329, 15.290 (226.426)	Grad: 218102.359
[Epoch  17 (184.39s)]	ELBO: -10949.345, -7187.859, -5146.145, -3762.999, -2830.957, -2128.295, -1550.482, -1067.918, -665.932, -339.597, -134.328, -86.293, -67.480, -58.993, -55.727, -59.019 (-123.211)	Log prob: -10929.988, -7152.542, -5095.638, -3698.119, -2752.230, -2036.167, -1444.811, -949.014, -533.818, -193.899, 25.032, 84.367, 114.849, 136.811, 154.383, 166.311 (102.533)	KLD: 19.356, 15.962, 15.188, 14.374, 13.846, 13.402, 13.542, 13.234, 13.209, 13.584, 13.662, 11.300, 11.670, 13.475, 14.305, 15.220 (225.744)	Grad: 222224.906
[Epoch  18 (183.91s)]	ELBO: -10939.889, -7187.654, -5143.677, -3761.266, -2830.965, -2128.473, -1549.927, -1066.548, -664.382, -337.736, -129.153, -81.593, -62.110, -53.612, -50.240, -52.535 (-100.265)	Log prob: -10920.551, -7152.372, -5093.190, -3696.369, -2752.247, -2036.409, -1444.309, -947.713, -532.408, -192.203, 30.068, 88.792, 119.848, 141.806, 159.445, 172.280 (125.213)	KLD: 19.340, 15.942, 15.205, 14.410, 13.820, 13.346, 13.554, 13.218, 13.139, 13.558, 13.688, 11.164, 11.573, 13.460, 14.266, 15.131 (225.479)	Grad: 213450.906
[Epoch  19 (184.13s)]	ELBO: -10937.629, -7190.066, -5143.041, -3760.274, -2829.561, -2126.379, -1548.709, -1065.627, -662.934, -335.178, -122.962, -75.691, -54.412, -45.809, -42.119, -43.977 (-89.434)	Log prob: -10918.281, -7154.813, -5092.566, -3695.419, -2750.907, -2034.364, -1443.188, -946.898, -531.073, -189.827, 36.059, 94.382, 126.969, 149.028, 166.969, 180.149 (133.283)	KLD: 19.352, 15.900, 15.221, 14.379, 13.800, 13.361, 13.507, 13.207, 13.133, 13.489, 13.671, 11.051, 11.309, 13.456, 14.251, 15.038 (222.717)	Grad: 228357.500
[Epoch  20 (183.69s)]	ELBO: -10930.604, -7179.182, -5133.783, -3751.733, -2820.541, -2117.478, -1539.751, -1057.336, -655.003, -326.855, -111.195, -64.109, -45.896, -37.264, -33.916, -34.979 (-79.642)	Log prob: -10911.261, -7143.888, -5083.297, -3686.865, -2741.887, -2025.471, -1434.266, -938.684, -523.285, -181.658, 47.669, 105.843, 135.819, 157.855, 175.405, 189.244 (142.525)	KLD: 19.344, 15.952, 15.190, 14.381, 13.787, 13.352, 13.478, 13.167, 13.066, 13.479, 13.667, 11.087, 11.764, 13.403, 14.203, 14.901 (222.168)	Grad: 199296.609
[Epoch  21 (189.08s)]	ELBO: -10923.676, -7171.323, -5128.712, -3746.321, -2815.251, -2112.756, -1536.276, -1053.822, -650.888, -323.386, -104.247, -54.705, -35.066, -26.197, -22.068, -22.933 (-78.997)	Log prob: -10904.333, -7136.087, -5078.267, -3681.464, -2736.654, -2020.845, -1430.926, -935.335, -519.328, -178.415, 54.373, 114.662, 145.509, 167.769, 186.096, 200.212 (145.141)	KLD: 19.340, 15.900, 15.206, 14.410, 13.740, 13.315, 13.439, 13.136, 13.073, 13.411, 13.648, 10.748, 11.208, 13.391, 14.199, 14.981 (224.138)	Grad: 203298.125
[Epoch  22 (187.82s)]	ELBO: -10939.215, -7187.025, -5137.707, -3751.365, -2819.854, -2117.335, -1538.943, -1056.590, -653.394, -325.331, -100.906, -48.011, -27.047, -17.914, -13.482, -13.653 (-58.863)	Log prob: -10919.863, -7151.736, -5087.236, -3686.545, -2741.281, -2025.441, -1433.608, -938.145, -521.904, -180.501, 57.585, 121.196, 152.988, 175.478, 194.050, 208.795 (165.673)	KLD: 19.356, 15.931, 15.182, 14.353, 13.751, 13.321, 13.441, 13.110, 13.044, 13.340, 13.662, 10.716, 10.827, 13.357, 14.141, 14.917 (224.536)	Grad: 208247.531
[Epoch  23 (159.94s)]	ELBO: -10961.958, -7203.395, -5144.061, -3761.854, -2829.811, -2125.789, -1547.112, -1064.395, -661.368, -333.428, -103.724, -47.405, -25.420, -15.830, -10.579, -8.810 (-96.717)	Log prob: -10942.610, -7168.141, -5093.651, -3697.045, -2751.245, -2033.926, -1441.794, -946.009, -529.989, -188.706, 54.642, 121.787, 154.491, 177.390, 196.748, 213.334 (125.708)	KLD: 19.343, 15.903, 15.160, 14.403, 13.755, 13.299, 13.456, 13.068, 12.993, 13.343, 13.644, 10.826, 10.719, 13.309, 14.107, 14.818 (222.425)	Grad: 215421.375
[Epoch  24 (137.19s)]	ELBO: -10931.438, -7176.054, -5128.417, -3745.260, -2814.695, -2111.990, -1534.067, -1050.050, -646.847, -319.171, -82.697, -25.545, -6.001, 4.246, 9.318, 10.670 (-41.495)	Log prob: -10912.091, -7140.798, -5078.000, -3680.489, -2736.153, -2020.168, -1428.842, -931.751, -515.560, -174.603, 75.502, 143.384, 175.639, 199.127, 218.164, 234.244 (183.785)	KLD: 19.343, 15.914, 15.159, 14.355, 13.769, 13.280, 13.404, 13.074, 12.988, 13.280, 13.632, 10.729, 12.712, 13.241, 13.964, 14.728 (225.280)	Grad: 223514.234
[Epoch  25 (132.76s)]	ELBO: -10951.387, -7192.461, -5140.001, -3756.497, -2823.636, -2120.741, -1540.978, -1056.264, -652.411, -323.708, -79.488, -21.055, -0.212, 10.488, 14.739, 16.909 (-27.992)	Log prob: -10932.079, -7157.264, -5089.655, -3691.795, -2745.217, -2029.044, -1435.839, -938.059, -521.263, -179.267, 78.643, 147.726, 180.239, 204.132, 222.239, 239.294 (195.356)	KLD: 19.307, 15.885, 15.152, 14.359, 13.716, 13.278, 13.441, 13.069, 12.941, 13.294, 13.690, 10.650, 11.671, 13.192, 13.856, 14.885 (223.348)	Grad: 218923.359
[Epoch  26 (133.50s)]	ELBO: -10943.999, -7180.896, -5136.018, -3752.012, -2820.171, -2118.029, -1539.327, -1055.886, -652.763, -323.987, -73.349, -15.701, 5.886, 16.326, 21.114, 23.041 (-17.957)	Log prob: -10924.708, -7145.708, -5085.683, -3687.297, -2741.770, -2026.345, -1434.238, -937.812, -521.760, -179.779, 84.567, 152.451, 184.936, 208.667, 227.504, 244.123 (202.539)	KLD: 19.291, 15.898, 15.146, 14.383, 13.684, 13.284, 13.404, 12.985, 12.929, 13.204, 13.708, 10.236, 10.898, 13.292, 14.049, 14.692 (220.496)	Grad: 215497.812
[Epoch  27 (136.29s)]	ELBO: -10959.216, -7191.546, -5137.292, -3752.469, -2819.512, -2115.017, -1537.185, -1052.951, -649.090, -320.708, -64.520, -7.184, 14.146, 24.467, 29.030, 31.456 (-3.821)	Log prob: -10939.892, -7156.327, -5086.902, -3687.716, -2741.083, -2023.335, -1432.147, -934.956, -518.210, -176.623, 93.223, 160.642, 192.721, 216.280, 234.845, 251.960 (216.380)	KLD: 19.322, 15.901, 15.166, 14.366, 13.675, 13.254, 13.355, 12.957, 12.885, 13.205, 13.658, 10.083, 10.749, 13.239, 14.001, 14.689 (220.201)	Grad: 217061.562
[Epoch  28 (163.71s)]	ELBO: -10940.220, -7176.685, -5131.669, -3748.828, -2815.472, -2112.389, -1533.647, -1049.840, -646.211, -316.693, -55.282, 0.506, 20.902, 31.817, 36.485, 39.487 (3.392)	Log prob: -10920.898, -7141.400, -5081.244, -3684.072, -2737.024, -2020.713, -1428.675, -931.931, -515.443, -172.758, 102.334, 168.320, 199.462, 223.549, 242.144, 259.733 (224.319)	KLD: 19.329, 15.957, 15.139, 14.330, 13.695, 13.227, 13.295, 12.937, 12.859, 13.167, 13.682, 10.198, 10.745, 13.173, 13.926, 14.588 (220.927)	Grad: 214796.125
[Epoch  29 (217.33s)]	ELBO: -10943.615, -7185.794, -5139.051, -3753.460, -2820.022, -2115.544, -1536.204, -1051.744, -647.848, -318.283, -52.273, 1.309, 22.206, 32.086, 36.533, 39.234 (2.941)	Log prob: -10924.247, -7150.546, -5088.710, -3688.806, -2741.653, -2023.909, -1431.286, -933.894, -517.149, -174.466, 105.203, 169.642, 201.591, 224.663, 242.935, 260.245 (224.775)	KLD: 19.366, 15.880, 15.096, 14.313, 13.713, 13.267, 13.283, 12.932, 12.849, 13.118, 13.659, 10.857, 11.053, 13.192, 13.824, 14.610 (221.833)	Grad: 204948.062
[Epoch  30 (200.33s)]	ELBO: -10941.602, -7181.918, -5131.478, -3747.863, -2814.248, -2112.422, -1534.616, -1050.847, -646.907, -317.682, -49.617, 5.584, 27.173, 37.931, 42.989, 46.391 (1.966)	Log prob: -10922.276, -7146.680, -5081.136, -3683.197, -2735.878, -2020.756, -1429.608, -932.908, -516.142, -173.803, 107.977, 173.917, 205.914, 229.766, 248.539, 266.543 (220.767)	KLD: 19.328, 15.904, 15.110, 14.323, 13.705, 13.295, 13.343, 12.930, 12.827, 13.113, 13.716, 10.739, 10.408, 13.094, 13.715, 14.601 (218.801)	Grad: 230023.641
[Epoch  31 (191.47s)]	ELBO: -11016.523, -7240.204, -5158.947, -3771.835, -2835.501, -2129.864, -1550.729, -1063.987, -660.000, -330.341, -59.778, -1.611, 20.928, 32.441, 37.455, 42.201 (-3.741)	Log prob: -10997.259, -7205.038, -5108.598, -3707.158, -2757.162, -2038.326, -1445.922, -946.301, -529.542, -186.814, 97.384, 165.637, 198.564, 223.183, 242.013, 261.218 (218.922)	KLD: 19.266, 15.893, 15.189, 14.328, 13.663, 13.199, 13.268, 12.879, 12.773, 13.068, 13.635, 10.086, 10.387, 13.105, 13.817, 14.459 (222.663)	Grad: 215353.562
[Epoch  32 (186.72s)]	ELBO: -10967.003, -7196.841, -5142.819, -3760.583, -2826.182, -2122.320, -1548.755, -1065.410, -664.034, -334.653, -62.140, -5.498, 14.557, 23.415, 27.994, 34.024 (-45.677)	Log prob: -10947.569, -7161.424, -5092.249, -3695.678, -2747.593, -2030.447, -1443.292, -947.068, -532.937, -190.469, 95.807, 163.095, 195.140, 217.002, 235.420, 255.901 (177.183)	KLD: 19.435, 15.981, 15.150, 14.339, 13.684, 13.284, 13.589, 12.879, 12.756, 13.086, 13.763, 10.646, 11.990, 13.003, 13.839, 14.450 (222.860)	Grad: 263643.969
[Epoch  33 (187.43s)]	ELBO: -10944.731, -7180.228, -5137.041, -3753.184, -2819.169, -2115.124, -1536.518, -1050.392, -646.069, -315.852, -41.145, 17.195, 24.658, 35.359, 40.192, 44.228 (7.739)	Log prob: -10925.390, -7145.112, -5086.650, -3688.413, -2740.756, -2023.458, -1431.641, -932.695, -515.656, -172.302, 116.105, 184.690, 202.695, 226.388, 245.035, 263.276 (225.251)	KLD: 19.348, 15.766, 15.280, 14.373, 13.646, 13.254, 13.211, 12.819, 12.715, 13.138, 13.699, 10.245, 10.542, 12.992, 13.814, 14.205 (217.512)	Grad: 221920.094
[Epoch  34 (188.85s)]	ELBO: -10943.533, -7176.911, -5132.494, -3744.725, -2810.015, -2104.502, -1526.086, -1040.459, -636.406, -305.650, -28.937, 33.270, 50.496, 61.542, 66.880, 71.648 (0.423)	Log prob: -10924.215, -7141.756, -5082.139, -3680.056, -2731.706, -2013.020, -1421.451, -923.061, -506.403, -162.652, 127.656, 199.504, 226.656, 250.825, 269.903, 288.979 (216.775)	KLD: 19.315, 15.840, 15.200, 14.312, 13.641, 13.174, 13.152, 12.763, 12.605, 12.995, 13.595, 9.641, 9.927, 13.122, 13.741, 14.308 (216.353)	Grad: 212733.547
[Epoch  35 (187.56s)]	ELBO: -10958.904, -7185.467, -5140.987, -3754.120, -2817.713, -2112.749, -1532.473, -1047.131, -642.600, -311.568, -33.563, 32.295, 52.795, 63.553, 69.567, 74.111 (35.368)	Log prob: -10939.599, -7150.298, -5090.654, -3689.468, -2739.440, -2021.295, -1427.939, -929.851, -512.701, -168.716, 122.810, 198.294, 228.749, 252.631, 272.412, 291.125 (253.100)	KLD: 19.305, 15.864, 15.163, 14.316, 13.625, 13.182, 13.079, 12.745, 12.619, 12.953, 13.521, 9.626, 9.954, 13.125, 13.766, 14.169 (217.732)	Grad: 208596.016
[Epoch  36 (189.52s)]	ELBO: -10952.818, -7179.048, -5133.323, -3746.549, -2811.991, -2107.093, -1526.561, -1041.068, -635.299, -304.501, -25.210, 42.572, 63.890, 74.974, 81.135, 85.909 (17.251)	Log prob: -10933.492, -7143.829, -5082.972, -3681.869, -2733.742, -2015.708, -1422.103, -923.870, -505.505, -161.820, 130.950, 208.267, 239.459, 263.626, 283.456, 302.421 (234.551)	KLD: 19.325, 15.893, 15.133, 14.328, 13.569, 13.137, 13.073, 12.740, 12.596, 12.887, 13.479, 9.534, 9.875, 13.082, 13.669, 14.192 (217.301)	Grad: 207153.641
[Epoch  37 (191.14s)]	ELBO: -10945.479, -7170.359, -5124.349, -3742.219, -2806.786, -2101.979, -1522.712, -1038.312, -633.437, -302.924, -23.071, 47.976, 71.028, 82.465, 88.988, 93.877 (12.517)	Log prob: -10926.193, -7135.162, -5074.037, -3677.579, -2728.562, -2010.610, -1418.285, -921.181, -503.719, -160.289, 132.979, 213.820, 246.823, 271.296, 291.315, 310.299 (231.603)	KLD: 19.287, 15.907, 15.116, 14.331, 13.582, 13.146, 13.057, 12.705, 12.588, 12.916, 13.416, 9.793, 9.951, 13.036, 13.496, 14.095 (219.087)	Grad: 221087.219
[Epoch  38 (191.42s)]	ELBO: -10950.779, -7174.877, -5129.580, -3744.490, -2809.346, -2103.162, -1523.509, -1038.986, -634.507, -304.228, -22.265, 51.754, 74.246, 86.315, 92.838, 97.734 (-20.922)	Log prob: -10931.466, -7139.622, -5079.191, -3679.818, -2731.095, -2011.758, -1419.059, -921.837, -504.818, -161.696, 133.665, 217.320, 250.077, 275.161, 295.025, 313.988 (194.882)	KLD: 19.318, 15.942, 15.130, 14.286, 13.576, 13.154, 13.046, 12.698, 12.540, 12.843, 13.398, 9.636, 10.265, 13.015, 13.341, 14.067 (215.804)	Grad: 231703.250
[Epoch  39 (190.62s)]	ELBO: -10953.806, -7179.318, -5135.982, -3752.606, -2816.949, -2111.588, -1531.951, -1047.537, -643.975, -313.132, -32.283, 44.845, 68.920, 80.516, 88.062, 92.616 (44.641)	Log prob: -10934.508, -7144.090, -5085.644, -3687.942, -2738.715, -2020.189, -1427.505, -930.383, -514.270, -170.579, 123.666, 210.405, 244.242, 268.867, 289.574, 308.087 (259.027)	KLD: 19.300, 15.928, 15.110, 14.328, 13.570, 13.164, 13.046, 12.708, 12.551, 12.849, 13.395, 9.611, 9.762, 13.029, 13.162, 13.959 (214.386)	Grad: 232411.984
[Epoch  40 (197.10s)]	ELBO: -10940.726, -7170.526, -5124.191, -3736.743, -2801.402, -2096.923, -1516.774, -1031.715, -626.903, -295.797, -12.518, 66.423, 88.590, 99.943, 106.322, 111.247 (15.264)	Log prob: -10921.387, -7135.322, -5073.856, -3672.122, -2723.217, -2005.617, -1412.423, -914.699, -497.336, -153.412, 143.197, 231.720, 263.863, 288.223, 307.666, 326.457 (229.327)	KLD: 19.333, 15.875, 15.128, 14.285, 13.565, 13.121, 13.045, 12.665, 12.550, 12.819, 13.330, 9.582, 9.976, 13.007, 13.063, 13.866 (214.062)	Grad: 203950.750
[Epoch  41 (191.76s)]	ELBO: -10952.816, -7176.843, -5125.875, -3741.847, -2806.605, -2101.357, -1521.440, -1037.520, -632.329, -300.630, -17.169, 64.328, 87.543, 99.427, 106.226, 111.506 (38.366)	Log prob: -10933.471, -7141.571, -5075.517, -3677.176, -2728.365, -2009.987, -1417.070, -920.495, -502.795, -158.337, 138.438, 229.424, 262.283, 287.128, 306.767, 325.953 (252.839)	KLD: 19.347, 15.923, 15.088, 14.311, 13.571, 13.129, 13.000, 12.654, 12.509, 12.759, 13.314, 9.489, 9.644, 12.961, 12.840, 13.906 (214.472)	Grad: 222315.500
[Epoch  42 (185.68s)]	ELBO: -10945.671, -7177.788, -5131.024, -3743.928, -2807.635, -2101.571, -1521.622, -1036.551, -630.188, -298.944, -14.713, 67.991, 91.093, 102.555, 109.603, 114.598 (30.421)	Log prob: -10926.302, -7142.516, -5080.665, -3679.282, -2729.437, -2010.288, -1417.336, -919.634, -500.778, -156.781, 140.714, 232.866, 265.672, 290.050, 309.793, 328.572 (243.226)	KLD: 19.375, 15.900, 15.085, 14.289, 13.552, 13.081, 13.004, 12.632, 12.492, 12.754, 13.264, 9.448, 9.704, 12.916, 12.695, 13.784 (212.805)	Grad: 219224.844
[Epoch  43 (187.20s)]	ELBO: -10944.555, -7173.297, -5131.081, -3746.680, -2809.653, -2102.934, -1522.182, -1038.054, -633.028, -301.240, -16.803, 66.149, 88.537, 99.660, 106.687, 112.019 (70.000)	Log prob: -10925.238, -7138.061, -5080.758, -3682.103, -2731.517, -2011.674, -1417.926, -921.157, -503.681, -159.168, 138.540, 231.333, 263.792, 287.753, 307.565, 326.636 (282.661)	KLD: 19.318, 15.919, 15.084, 14.254, 13.559, 13.125, 12.996, 12.640, 12.451, 12.725, 13.270, 9.840, 10.072, 12.837, 12.785, 13.739 (212.661)	Grad: 216763.328
[Epoch  44 (141.13s)]	ELBO: -10945.272, -7174.209, -5133.571, -3746.435, -2810.404, -2104.724, -1523.344, -1037.694, -632.312, -302.086, -20.029, 66.836, 89.881, 101.695, 109.469, 111.752 (72.246)	Log prob: -10925.892, -7138.825, -5083.129, -3681.673, -2732.059, -2013.258, -1418.867, -920.575, -502.736, -159.787, 135.503, 232.295, 265.006, 289.554, 310.277, 326.316 (284.119)	KLD: 19.378, 16.001, 15.064, 14.322, 13.581, 13.122, 13.011, 12.641, 12.457, 12.724, 13.233, 9.926, 9.666, 12.735, 12.949, 13.756 (211.873)	Grad: 234548.297
[Epoch  45 (138.77s)]	ELBO: -10955.290, -7178.186, -5128.739, -3740.381, -2805.006, -2098.166, -1516.872, -1031.849, -627.129, -294.762, -9.814, 78.337, 101.112, 112.902, 120.259, 123.586 (68.230)	Log prob: -10935.951, -7142.890, -5078.427, -3675.826, -2726.918, -2006.944, -1412.690, -915.065, -497.884, -152.843, 145.328, 242.991, 275.900, 300.620, 320.314, 336.888 (279.480)	KLD: 19.339, 15.955, 15.018, 14.243, 13.534, 13.133, 12.960, 12.601, 12.462, 12.673, 13.224, 9.511, 10.134, 12.931, 12.337, 13.247 (211.250)	Grad: 217488.609
[Epoch  46 (189.03s)]	ELBO: -10950.741, -7181.344, -5136.918, -3750.273, -2813.626, -2105.173, -1523.590, -1038.650, -632.377, -300.425, -14.888, 75.820, 99.889, 112.921, 120.667, 124.952 (66.808)	Log prob: -10931.402, -7146.091, -5086.569, -3685.645, -2735.461, -2013.915, -1419.401, -921.911, -503.225, -158.630, 140.044, 240.119, 273.853, 299.805, 319.774, 337.358 (277.955)	KLD: 19.336, 15.923, 15.092, 14.279, 13.536, 13.092, 12.932, 12.550, 12.412, 12.644, 13.137, 9.367, 9.664, 12.921, 12.223, 13.299 (211.147)	Grad: 221545.250
[Epoch  47 (185.98s)]	ELBO: -10945.670, -7172.135, -5122.813, -3737.118, -2801.961, -2096.098, -1514.916, -1029.596, -623.827, -291.146, -6.470, 85.419, 109.948, 122.596, 131.005, 135.666 (93.237)	Log prob: -10926.349, -7136.900, -5072.469, -3672.528, -2723.872, -2004.925, -1410.831, -912.940, -494.804, -149.508, 148.282, 249.521, 283.332, 308.852, 329.310, 347.302 (305.676)	KLD: 19.319, 15.918, 15.106, 14.248, 13.499, 13.081, 12.913, 12.572, 12.367, 12.614, 13.114, 9.350, 9.282, 12.872, 12.049, 13.330 (212.439)	Grad: 225009.422
[Epoch  48 (187.04s)]	ELBO: -10952.840, -7176.009, -5121.923, -3736.398, -2799.905, -2093.028, -1512.278, -1027.418, -622.103, -289.864, -3.624, 89.233, 113.060, 126.041, 133.428, 138.353 (94.878)	Log prob: -10933.527, -7140.750, -5071.599, -3671.817, -2721.816, -2001.893, -1408.248, -910.828, -493.163, -148.327, 151.031, 253.221, 286.477, 312.273, 331.663, 349.990 (305.024)	KLD: 19.315, 15.946, 15.066, 14.251, 13.510, 13.048, 12.894, 12.559, 12.350, 12.597, 13.117, 9.333, 9.429, 12.816, 12.002, 13.402 (210.146)	Grad: 217881.438
[Epoch  49 (186.02s)]	ELBO: -10950.606, -7172.763, -5127.814, -3739.351, -2803.205, -2097.201, -1516.062, -1030.605, -624.262, -291.243, -4.339, 90.918, 114.279, 128.351, 136.109, 140.907 (72.618)	Log prob: -10931.271, -7137.492, -5077.483, -3674.777, -2725.122, -2006.048, -1412.025, -914.065, -495.372, -149.754, 150.259, 254.868, 287.382, 314.252, 333.817, 351.959 (284.947)	KLD: 19.339, 15.928, 15.061, 14.248, 13.507, 13.069, 12.883, 12.505, 12.349, 12.598, 13.109, 9.352, 9.154, 12.798, 11.806, 13.345 (212.329)	Grad: 218053.328
[Epoch  50 (186.06s)]	ELBO: -10942.246, -7165.386, -5125.726, -3741.363, -2803.824, -2097.042, -1516.018, -1031.138, -625.288, -292.958, -5.935, 91.234, 116.715, 130.731, 138.798, 143.873 (85.249)	Log prob: -10922.899, -7130.089, -5075.352, -3676.744, -2725.708, -2005.873, -1411.954, -914.551, -496.381, -151.504, 148.613, 255.094, 289.857, 316.663, 336.350, 354.769 (294.518)	KLD: 19.355, 15.947, 15.069, 14.247, 13.498, 13.053, 12.895, 12.524, 12.320, 12.546, 13.094, 9.312, 9.282, 12.790, 11.620, 13.344 (209.269)	Grad: 242057.469
[Epoch  51 (143.61s)]	ELBO: -10937.350, -7160.600, -5121.434, -3734.865, -2797.034, -2089.578, -1508.715, -1024.378, -618.376, -285.335, 1.863, 98.584, 119.623, 132.276, 140.522, 145.395 (100.800)	Log prob: -10918.046, -7125.357, -5071.164, -3670.361, -2719.062, -1998.559, -1404.758, -907.902, -489.597, -143.986, 156.340, 262.743, 294.531, 319.960, 339.988, 358.290 (312.207)	KLD: 19.301, 15.938, 15.032, 14.235, 13.464, 13.049, 12.938, 12.519, 12.304, 12.570, 13.128, 9.682, 10.749, 12.776, 11.783, 13.428 (211.408)	Grad: 216280.781
[Epoch  52 (130.60s)]	ELBO: -10945.412, -7166.016, -5123.522, -3736.752, -2798.450, -2091.175, -1510.678, -1025.331, -619.635, -286.495, 0.592, 100.264, 124.012, 138.026, 146.018, 151.065 (105.123)	Log prob: -10926.049, -7130.702, -5073.171, -3672.163, -2720.406, -2000.087, -1406.723, -908.885, -490.921, -145.267, 154.855, 263.728, 296.834, 323.601, 342.989, 361.222 (314.272)	KLD: 19.362, 15.950, 15.041, 14.237, 13.454, 13.043, 12.868, 12.491, 12.269, 12.514, 13.034, 9.201, 9.358, 12.753, 11.396, 13.186 (209.149)	Grad: 227949.078
[Epoch  53 (133.66s)]	ELBO: -10944.732, -7173.543, -5125.758, -3738.436, -2800.990, -2092.625, -1510.990, -1024.907, -618.703, -285.434, 1.802, 102.314, 126.011, 140.319, 148.880, 153.881 (84.632)	Log prob: -10925.413, -7138.296, -5075.448, -3673.852, -2722.927, -2001.540, -1407.036, -908.511, -490.056, -144.288, 155.964, 265.891, 298.912, 325.919, 345.701, 363.947 (293.942)	KLD: 19.319, 15.932, 15.061, 14.273, 13.477, 13.022, 12.869, 12.442, 12.251, 12.499, 13.016, 9.416, 9.324, 12.699, 11.222, 13.244 (209.310)	Grad: 223734.062
[Epoch  54 (132.64s)]	ELBO: -10921.738, -7150.704, -5111.998, -3726.735, -2789.811, -2082.757, -1501.618, -1017.185, -611.048, -278.319, 9.542, 112.002, 136.607, 151.737, 160.647, 166.181 (120.452)	Log prob: -10902.398, -7115.386, -5061.624, -3662.148, -2711.753, -1991.670, -1397.688, -900.790, -482.424, -137.240, 163.607, 275.282, 309.134, 336.979, 356.792, 375.588 (330.508)	KLD: 19.343, 15.970, 15.061, 14.213, 13.471, 13.030, 12.844, 12.464, 12.230, 12.455, 12.986, 9.216, 9.247, 12.715, 10.902, 13.263 (210.056)	Grad: 228113.422
[Epoch  55 (129.18s)]	ELBO: -10920.305, -7149.906, -5108.613, -3722.703, -2785.504, -2078.368, -1497.404, -1012.547, -606.508, -273.575, 13.894, 119.789, 143.840, 158.702, 167.730, 173.181 (112.411)	Log prob: -10900.937, -7114.604, -5058.312, -3658.169, -2707.502, -1987.350, -1393.564, -896.247, -477.978, -132.606, 167.825, 282.913, 316.135, 343.674, 363.427, 381.977 (322.802)	KLD: 19.367, 15.936, 15.000, 14.229, 13.467, 13.018, 12.821, 12.462, 12.228, 12.439, 12.963, 9.193, 9.171, 12.677, 10.725, 13.099 (210.390)	Grad: 204331.641
[Epoch  56 (130.54s)]	ELBO: -10938.119, -7165.239, -5123.583, -3738.806, -2801.140, -2093.149, -1512.091, -1025.617, -619.168, -286.403, 1.705, 104.906, 127.201, 145.265, 154.560, 160.534 (107.643)	Log prob: -10918.771, -7129.907, -5073.227, -3674.228, -2723.086, -2002.076, -1408.196, -909.302, -490.665, -145.500, 155.546, 268.944, 301.460, 332.115, 352.597, 371.628 (325.871)	KLD: 19.351, 15.985, 15.017, 14.225, 13.474, 13.021, 12.822, 12.420, 12.187, 12.401, 12.938, 10.197, 10.221, 12.591, 11.187, 13.057 (218.228)	Grad: 220112.922
[Epoch  57 (131.61s)]	ELBO: -10933.494, -7157.557, -5116.113, -3729.717, -2791.175, -2084.530, -1502.851, -1017.238, -611.367, -278.932, 9.341, 117.362, 139.293, 155.547, 162.889, 168.486 (121.963)	Log prob: -10914.252, -7122.319, -5065.804, -3665.123, -2713.120, -1993.433, -1398.906, -900.916, -482.840, -138.010, 163.232, 283.501, 318.066, 346.504, 366.686, 385.360 (335.193)	KLD: 19.243, 15.990, 15.073, 14.287, 13.462, 13.043, 12.848, 12.377, 12.205, 12.394, 12.970, 12.248, 12.634, 12.183, 12.841, 13.076 (213.231)	Grad: 219824.219
[Epoch  58 (130.33s)]	ELBO: -10946.968, -7165.423, -5121.476, -3735.314, -2797.144, -2088.252, -1507.486, -1020.639, -613.573, -279.707, 9.077, 121.511, 145.803, 162.922, 171.480, 177.343 (95.245)	Log prob: -10927.603, -7130.079, -5071.084, -3670.702, -2719.096, -1997.189, -1403.647, -904.399, -485.146, -138.933, 162.741, 285.529, 320.704, 350.134, 370.330, 389.255 (304.857)	KLD: 19.367, 15.976, 15.049, 14.220, 13.436, 13.016, 12.775, 12.402, 12.186, 12.347, 12.891, 10.354, 10.883, 12.310, 11.638, 13.062 (209.613)	Grad: 216618.578
[Epoch  59 (131.12s)]	ELBO: -10937.439, -7168.457, -5125.986, -3738.077, -2799.300, -2091.510, -1508.592, -1022.482, -615.776, -282.161, 7.276, 124.179, 150.340, 167.409, 177.167, 182.362 (108.154)	Log prob: -10918.123, -7133.143, -5075.663, -3673.522, -2721.295, -2000.508, -1404.829, -906.307, -487.418, -141.430, 160.807, 287.331, 322.575, 352.132, 372.377, 390.519 (324.036)	KLD: 19.320, 15.999, 15.008, 14.227, 13.451, 12.999, 12.759, 12.412, 12.183, 12.373, 12.800, 9.621, 9.083, 12.488, 10.487, 12.947 (215.882)	Grad: 220601.938
[Epoch  60 (130.59s)]	ELBO: -10949.062, -7182.059, -5127.481, -3740.553, -2800.164, -2091.533, -1510.662, -1024.218, -617.317, -283.711, 5.526, 125.589, 153.149, 171.118, 181.309, 186.581 (139.355)	Log prob: -10929.717, -7146.729, -5077.124, -3675.979, -2722.137, -2000.499, -1406.859, -908.021, -488.964, -143.061, 158.976, 288.311, 325.225, 355.632, 376.221, 394.433 (347.250)	KLD: 19.341, 15.990, 15.026, 14.218, 13.451, 13.008, 12.770, 12.393, 12.157, 12.297, 12.800, 9.272, 9.353, 12.438, 10.397, 12.941 (207.895)	Grad: 230604.672
[Epoch  61 (130.28s)]	ELBO: -10949.393, -7168.657, -5125.839, -3737.631, -2797.587, -2088.816, -1507.836, -1022.247, -614.730, -281.455, 8.084, 130.445, 156.351, 173.795, 184.100, 189.550 (124.403)	Log prob: -10930.057, -7133.349, -5075.484, -3673.046, -2719.581, -1997.848, -1404.126, -906.148, -486.503, -140.924, 161.384, 293.236, 328.295, 358.174, 378.624, 396.993 (335.666)	KLD: 19.347, 15.962, 15.042, 14.231, 13.422, 12.965, 12.740, 12.389, 12.129, 12.304, 12.769, 9.491, 9.153, 12.435, 10.145, 12.919 (211.263)	Grad: 226648.406
[Epoch  62 (132.16s)]	ELBO: -10943.710, -7165.771, -5123.298, -3737.877, -2798.559, -2090.437, -1508.864, -1022.111, -614.962, -280.782, 8.503, 134.439, 161.115, 179.548, 189.800, 195.149 (138.277)	Log prob: -10924.371, -7130.418, -5072.946, -3673.333, -2720.602, -1999.503, -1405.158, -906.038, -486.770, -140.284, 161.782, 296.936, 332.957, 363.789, 384.263, 402.520 (344.022)	KLD: 19.336, 16.014, 15.004, 14.189, 13.414, 12.977, 12.771, 12.369, 12.119, 12.306, 12.781, 9.219, 9.344, 12.399, 10.221, 12.908 (205.745)	Grad: 224093.156
[Epoch  63 (130.45s)]	ELBO: -10952.244, -7164.784, -5125.989, -3737.649, -2797.313, -2089.343, -1508.168, -1021.221, -613.780, -279.472, 10.907, 138.889, 165.417, 181.633, 192.099, 197.243 (129.249)	Log prob: -10932.896, -7129.452, -5075.641, -3673.098, -2719.348, -1998.382, -1404.473, -905.139, -485.611, -139.045, 164.062, 301.265, 336.635, 365.265, 385.655, 403.567 (336.528)	KLD: 19.346, 15.984, 15.017, 14.204, 13.413, 12.998, 12.733, 12.387, 12.087, 12.258, 12.727, 9.222, 8.842, 12.413, 9.924, 12.768 (207.280)	Grad: 203713.875
[Epoch  64 (130.51s)]	ELBO: -10955.535, -7177.224, -5129.481, -3739.247, -2798.755, -2090.092, -1509.097, -1022.060, -614.546, -280.674, 9.232, 138.788, 166.768, 182.800, 193.553, 199.187 (134.459)	Log prob: -10936.187, -7141.884, -5079.165, -3674.725, -2720.782, -1999.134, -1405.402, -905.996, -486.378, -140.234, 162.419, 301.364, 338.087, 366.634, 387.228, 405.595 (339.967)	KLD: 19.349, 15.994, 14.975, 14.206, 13.447, 12.985, 12.738, 12.369, 12.104, 12.273, 12.747, 9.389, 8.743, 12.516, 9.841, 12.733 (205.508)	Grad: 219619.812
[Epoch  65 (131.37s)]	ELBO: -10957.801, -7174.762, -5132.126, -3742.543, -2803.213, -2094.335, -1512.961, -1028.229, -620.553, -286.705, 3.943, 136.238, 165.356, 184.444, 191.417, 197.637 (127.546)	Log prob: -10938.467, -7139.426, -5081.716, -3677.898, -2725.128, -2003.216, -1409.123, -912.102, -492.331, -146.261, 157.092, 298.760, 337.016, 368.578, 385.837, 404.757 (344.020)	KLD: 19.336, 16.001, 15.077, 14.230, 13.441, 13.035, 12.719, 12.289, 12.094, 12.222, 12.705, 9.374, 9.138, 12.474, 10.285, 12.700 (216.474)	Grad: 248506.844
[Epoch  66 (132.72s)]	ELBO: -10951.960, -7174.333, -5126.563, -3738.730, -2797.334, -2088.105, -1506.233, -1019.180, -611.984, -278.181, 12.504, 145.968, 175.506, 193.636, 204.395, 209.992 (124.488)	Log prob: -10932.603, -7138.933, -5076.125, -3674.110, -2719.292, -1997.082, -1402.521, -903.189, -483.903, -137.912, 165.449, 308.140, 346.601, 377.129, 397.567, 415.798 (330.917)	KLD: 19.360, 16.038, 15.044, 14.181, 13.418, 12.982, 12.688, 12.281, 12.090, 12.187, 12.677, 9.227, 8.923, 12.397, 9.680, 12.634 (206.429)	Grad: 211795.891
[Epoch  67 (133.21s)]	ELBO: -10929.146, -7157.641, -5112.733, -3724.518, -2785.360, -2077.175, -1496.339, -1010.537, -603.597, -270.264, 17.724, 152.907, 183.200, 203.673, 214.550, 220.217 (166.628)	Log prob: -10909.800, -7122.318, -5062.393, -3659.953, -2707.390, -1986.208, -1392.687, -894.600, -475.586, -130.052, 170.605, 315.030, 354.008, 386.861, 407.395, 425.657 (371.271)	KLD: 19.343, 15.977, 15.025, 14.222, 13.403, 12.999, 12.685, 12.284, 12.073, 12.202, 12.668, 9.243, 8.684, 12.381, 9.657, 12.596 (204.643)	Grad: 235798.812
[Epoch  68 (131.23s)]	ELBO: -10950.119, -7175.663, -5126.353, -3737.795, -2796.456, -2085.964, -1504.077, -1017.627, -610.794, -276.672, 13.329, 150.832, 182.769, 204.295, 215.685, 221.503 (153.411)	Log prob: -10930.756, -7140.312, -5075.995, -3673.239, -2718.531, -1995.046, -1400.462, -901.752, -482.866, -136.562, 166.092, 312.829, 353.526, 387.352, 408.685, 427.096 (360.965)	KLD: 19.355, 15.998, 15.006, 14.196, 13.371, 12.993, 12.696, 12.261, 12.054, 12.182, 12.653, 9.233, 8.760, 12.301, 9.942, 12.594 (207.555)	Grad: 224008.438
[Epoch  69 (133.44s)]	ELBO: -10938.070, -7162.721, -5117.457, -3728.089, -2788.722, -2078.763, -1496.545, -1009.798, -602.861, -269.758, 20.839, 160.759, 193.077, 215.057, 225.955, 231.769 (157.209)	Log prob: -10918.692, -7127.320, -5067.061, -3663.494, -2710.737, -1987.829, -1392.933, -893.909, -474.928, -129.639, 173.578, 322.787, 363.763, 398.011, 418.606, 436.931 (362.110)	KLD: 19.381, 16.020, 14.997, 14.196, 13.389, 12.952, 12.677, 12.277, 12.045, 12.185, 12.621, 9.289, 8.659, 12.268, 9.696, 12.511 (204.901)	Grad: 228832.609
[Epoch  70 (133.98s)]	ELBO: -10938.143, -7161.044, -5111.142, -3723.096, -2783.825, -2075.169, -1493.324, -1007.160, -599.525, -265.508, 25.618, 169.046, 202.570, 223.754, 234.271, 239.991 (170.984)	Log prob: -10918.782, -7125.668, -5060.742, -3658.501, -2705.836, -1984.204, -1389.693, -891.270, -471.583, -125.431, 178.268, 331.571, 374.001, 407.388, 427.491, 445.736 (377.944)	KLD: 19.361, 16.011, 15.027, 14.199, 13.389, 12.978, 12.665, 12.260, 12.052, 12.134, 12.574, 9.875, 8.906, 12.203, 9.586, 12.525 (206.960)	Grad: 214254.016
[Epoch  71 (130.82s)]	ELBO: -10933.932, -7156.819, -5115.946, -3726.371, -2786.085, -2076.959, -1494.693, -1007.431, -599.531, -264.973, 26.442, 173.647, 207.124, 227.971, 238.237, 244.223 (168.268)	Log prob: -10914.553, -7121.423, -5065.539, -3661.782, -2708.094, -1985.985, -1391.049, -891.523, -471.610, -124.941, 179.065, 335.630, 377.715, 410.781, 430.440, 448.879 (372.072)	KLD: 19.381, 16.018, 15.007, 14.182, 13.404, 12.983, 12.669, 12.264, 12.013, 12.111, 12.591, 9.360, 8.609, 12.219, 9.393, 12.453 (203.805)	Grad: 205233.156
[Epoch  72 (136.47s)]	ELBO: -10928.106, -7154.240, -5117.750, -3730.215, -2789.747, -2079.917, -1497.931, -1010.583, -602.544, -268.373, 22.195, 173.958, 210.042, 231.851, 243.504, 250.145 (180.009)	Log prob: -10908.747, -7118.868, -5067.333, -3665.616, -2711.751, -1988.946, -1394.309, -894.710, -474.662, -128.416, 174.736, 336.069, 381.139, 415.099, 436.318, 455.470 (383.723)	KLD: 19.358, 16.015, 15.046, 14.182, 13.397, 12.974, 12.651, 12.251, 12.010, 12.074, 12.584, 9.570, 8.986, 12.150, 9.566, 12.512 (203.714)	Grad: 218372.562
[Epoch  73 (133.55s)]	ELBO: -10946.655, -7166.252, -5122.389, -3731.187, -2788.755, -2078.850, -1496.679, -1009.638, -601.736, -266.729, 24.147, 179.329, 216.422, 238.059, 249.254, 255.452 (173.444)	Log prob: -10927.298, -7130.857, -5071.968, -3666.587, -2710.761, -1987.883, -1393.043, -893.741, -473.852, -126.770, 176.659, 341.372, 387.014, 420.806, 441.278, 459.817 (377.062)	KLD: 19.361, 16.039, 15.021, 14.180, 13.394, 12.972, 12.671, 12.260, 11.985, 12.076, 12.553, 9.531, 8.549, 12.156, 9.276, 12.341 (203.618)	Grad: 232263.016
[Epoch  74 (130.49s)]	ELBO: -10950.768, -7165.069, -5123.811, -3735.898, -2794.295, -2083.793, -1500.953, -1013.820, -605.739, -271.101, 20.247, 178.437, 217.600, 239.442, 251.305, 257.842 (173.720)	Log prob: -10931.365, -7129.643, -5073.388, -3671.306, -2716.303, -1992.823, -1397.351, -897.981, -477.901, -131.207, 172.680, 340.323, 388.018, 421.958, 442.971, 461.907 (377.966)	KLD: 19.403, 16.023, 15.000, 14.166, 13.401, 12.979, 12.630, 12.236, 12.001, 12.056, 12.539, 9.452, 8.532, 12.098, 9.151, 12.399 (204.246)	Grad: 235791.656
[Epoch  75 (133.03s)]	ELBO: -10926.204, -7150.306, -5115.256, -3724.094, -2784.025, -2074.379, -1492.203, -1004.647, -597.350, -263.166, 28.593, 189.023, 227.664, 249.187, 260.723, 267.124 (203.314)	Log prob: -10906.818, -7114.896, -5064.804, -3659.483, -2706.049, -1983.451, -1388.645, -888.862, -469.551, -123.312, 180.946, 350.851, 398.000, 431.582, 452.130, 470.791 (407.179)	KLD: 19.387, 16.022, 15.045, 14.154, 13.368, 12.953, 12.631, 12.225, 12.013, 12.056, 12.498, 9.475, 8.509, 12.059, 9.012, 12.261 (203.864)	Grad: 220767.875
[Epoch  76 (134.07s)]	ELBO: -10939.163, -7159.003, -5121.746, -3731.589, -2791.373, -2081.084, -1497.412, -1009.577, -601.141, -266.749, 24.544, 187.156, 226.040, 246.866, 256.416, 263.543 (182.881)	Log prob: -10919.784, -7123.589, -5071.309, -3666.960, -2713.396, -1990.112, -1393.797, -893.729, -473.314, -126.891, 176.931, 348.958, 396.500, 429.228, 450.486, 470.047 (389.731)	KLD: 19.379, 16.036, 15.024, 14.189, 13.348, 12.995, 12.642, 12.235, 11.980, 12.030, 12.530, 9.416, 8.657, 11.902, 11.708, 12.433 (206.851)	Grad: 232121.469
[Epoch  77 (132.29s)]	ELBO: -10948.872, -7162.122, -5123.141, -3732.855, -2790.916, -2081.443, -1498.888, -1011.395, -603.439, -269.955, 21.476, 186.590, 227.035, 249.064, 258.956, 264.999 (160.613)	Log prob: -10929.494, -7126.691, -5072.696, -3668.248, -2712.962, -1990.515, -1395.332, -895.584, -475.659, -130.128, 173.774, 348.351, 397.365, 430.949, 453.383, 471.667 (369.511)	KLD: 19.373, 16.054, 15.018, 14.165, 13.345, 12.974, 12.627, 12.255, 11.968, 12.048, 12.471, 9.463, 8.568, 11.556, 12.541, 12.242 (208.899)	Grad: 242635.312
[Epoch  78 (132.58s)]	ELBO: -10945.026, -7161.283, -5122.274, -3732.249, -2788.437, -2078.452, -1495.589, -1008.027, -600.620, -265.770, 26.638, 194.152, 234.049, 255.640, 265.880, 273.204 (198.754)	Log prob: -10925.592, -7125.800, -5071.817, -3667.625, -2710.458, -1987.499, -1392.032, -892.209, -472.824, -125.976, 178.857, 355.890, 404.212, 437.262, 458.642, 478.191 (403.094)	KLD: 19.429, 16.052, 14.978, 14.164, 13.356, 12.974, 12.603, 12.261, 11.979, 11.998, 12.423, 9.519, 8.426, 11.458, 11.141, 12.225 (204.340)	Grad: 208078.391
[Epoch  79 (133.64s)]	ELBO: -10973.621, -7176.448, -5132.697, -3740.002, -2795.599, -2085.435, -1502.370, -1014.414, -606.368, -271.237, 21.329, 189.767, 231.791, 253.175, 265.095, 272.323 (215.702)	Log prob: -10954.251, -7141.043, -5082.228, -3675.340, -2717.576, -1994.436, -1398.728, -898.564, -478.549, -131.413, 173.602, 351.710, 402.641, 435.671, 458.161, 477.753 (416.908)	KLD: 19.370, 16.032, 15.069, 14.191, 13.362, 12.973, 12.645, 12.207, 11.970, 12.004, 12.450, 9.668, 8.908, 11.646, 10.569, 12.365 (201.207)	Grad: 226690.047
[Epoch  80 (130.60s)]	ELBO: -10956.892, -7164.487, -5116.557, -3725.651, -2784.719, -2075.661, -1493.016, -1005.144, -597.411, -262.263, 30.378, 201.077, 244.363, 265.934, 278.241, 284.834 (218.147)	Log prob: -10937.499, -7129.034, -5066.102, -3661.000, -2706.733, -1984.743, -1389.501, -889.387, -469.691, -122.589, 182.447, 362.679, 414.451, 447.770, 469.334, 488.127 (421.524)	KLD: 19.390, 16.065, 15.002, 14.196, 13.333, 12.933, 12.596, 12.242, 11.963, 11.954, 12.395, 9.533, 8.486, 11.748, 9.257, 12.201 (203.377)	Grad: 223809.984
[Epoch  81 (132.69s)]	ELBO: -10956.161, -7163.273, -5122.838, -3730.109, -2788.003, -2077.835, -1495.074, -1008.077, -599.688, -264.012, 29.067, 201.259, 246.799, 268.571, 280.048, 287.667 (229.459)	Log prob: -10936.769, -7127.829, -5072.344, -3665.458, -2710.008, -1986.884, -1391.516, -892.287, -471.956, -124.342, 181.098, 362.807, 416.815, 450.328, 470.806, 490.544 (431.911)	KLD: 19.393, 16.046, 15.056, 14.154, 13.346, 12.956, 12.606, 12.234, 11.942, 11.937, 12.361, 9.517, 8.469, 11.740, 9.001, 12.119 (202.452)	Grad: 232758.906
[Epoch  82 (136.10s)]	ELBO: -10938.328, -7156.972, -5114.223, -3722.826, -2779.830, -2070.327, -1488.264, -1001.098, -592.973, -257.259, 35.754, 208.983, 255.106, 277.459, 288.976, 294.924 (226.644)	Log prob: -10918.916, -7121.485, -5063.745, -3658.180, -2701.841, -1979.384, -1384.764, -885.379, -465.319, -117.664, 187.711, 370.488, 425.065, 459.166, 479.706, 497.868 (428.450)	KLD: 19.409, 16.070, 15.000, 14.168, 13.343, 12.950, 12.558, 12.219, 11.936, 11.941, 12.362, 9.548, 8.453, 11.749, 9.023, 12.214 (201.806)	Grad: 219609.219
[Epoch  83 (137.48s)]	ELBO: -10990.839, -7210.202, -5171.962, -3782.352, -2842.201, -2132.430, -1549.601, -1061.312, -659.581, -326.715, -38.825, 128.022, -8145682432.000, -8145682432.000, -8145682432.000, -8145682432.000 (147.768)	Log prob: -10971.552, -7174.777, -5121.432, -3717.494, -2763.890, -2041.088, -1445.491, -944.861, -530.614, -185.659, 114.724, 292.393, 345.396, 371.457, 396.771, 416.773 (360.442)	KLD: 19.288, 16.141, 15.099, 14.328, 13.454, 13.032, 12.767, 12.341, 12.516, 12.089, 12.493, 10.822, 8145682432.000, 12.003, 11.523, 12.844 (212.674)	Grad: 311666.281
[Epoch  84 (138.48s)]	ELBO: -10979.719, -7197.032, -5160.603, -3770.121, -2829.104, -2117.499, -1530.428, -1041.342, -633.435, -299.944, -27.037, 141.718, 181.967, 195.075, 209.175, 215.740 (175.354)	Log prob: -10960.231, -7161.456, -5109.806, -3704.931, -2750.291, -2025.385, -1425.435, -923.869, -503.711, -158.023, 129.704, 310.102, 362.084, 386.674, 412.702, 432.649 (387.152)	KLD: 19.480, 16.101, 15.220, 14.390, 13.622, 13.303, 12.877, 12.480, 12.252, 12.196, 14.820, 11.644, 11.732, 11.481, 11.929, 13.382 (211.798)	Grad: 269156.531
[Epoch  85 (139.61s)]	ELBO: -10961.565, -7171.347, -5134.832, -3745.417, -2801.622, -2089.498, -1503.475, -1014.246, -606.309, -270.550, 18.702, 191.593, 236.064, 250.417, 263.722, 269.263 (201.331)	Log prob: -10942.176, -7135.948, -5084.371, -3680.697, -2723.382, -1998.123, -1399.372, -897.817, -477.834, -130.052, 171.576, 355.694, 411.114, 436.992, 461.996, 480.691 (414.964)	KLD: 19.390, 16.005, 15.066, 14.259, 13.521, 13.133, 12.729, 12.326, 12.047, 12.023, 12.376, 11.227, 10.949, 11.525, 11.699, 13.155 (213.632)	Grad: 227567.484
[Epoch  86 (137.72s)]	ELBO: -10965.501, -7171.822, -5128.595, -3737.953, -2795.813, -2084.185, -1496.666, -1008.289, -599.732, -265.006, 25.313, 199.759, 248.741, 267.537, 278.935, 283.203 (211.388)	Log prob: -10946.106, -7136.402, -5078.115, -3673.213, -2717.583, -1992.862, -1392.651, -891.958, -471.351, -124.628, 178.055, 363.380, 422.735, 452.981, 475.842, 492.527 (419.918)	KLD: 19.394, 16.028, 15.060, 14.257, 13.492, 13.091, 12.692, 12.316, 12.051, 11.996, 12.364, 10.880, 10.373, 11.451, 11.462, 12.418 (208.529)	Grad: 237766.844
[Epoch  87 (167.74s)]	ELBO: -10944.846, -7154.600, -5119.051, -3729.641, -2787.242, -2075.359, -1489.819, -1000.273, -591.738, -256.177, 35.785, 210.411, 261.683, 279.655, 289.650, 295.156 (234.747)	Log prob: -10925.446, -7119.188, -5068.572, -3664.918, -2709.086, -1984.132, -1385.927, -884.112, -463.575, -116.046, 188.049, 373.506, 434.783, 464.334, 485.796, 503.271 (442.498)	KLD: 19.400, 16.017, 15.062, 14.243, 13.434, 13.072, 12.664, 12.271, 12.002, 11.967, 12.133, 10.832, 10.005, 11.579, 11.467, 11.969 (207.751)	Grad: 221328.516
[Epoch  88 (273.95s)]	ELBO: -10951.652, -7165.380, -5124.075, -3733.922, -2790.802, -2079.711, -1494.126, -1003.779, -595.816, -259.185, 33.298, 209.343, 262.658, 282.040, 293.288, 298.864 (238.090)	Log prob: -10932.249, -7129.932, -5073.580, -3669.244, -2712.693, -1988.555, -1390.328, -887.741, -467.811, -119.258, 185.260, 371.994, 434.974, 465.929, 488.526, 505.926 (443.409)	KLD: 19.402, 16.051, 15.040, 14.187, 13.430, 13.045, 12.642, 12.241, 11.966, 11.922, 12.035, 10.690, 9.665, 11.572, 11.349, 11.824 (205.319)	Grad: 216827.094
[Epoch  89 (282.25s)]	ELBO: -10951.447, -7162.255, -5124.439, -3735.335, -2792.284, -2079.674, -1493.875, -1005.009, -596.899, -261.612, 30.548, 203.596, 258.200, 278.022, 288.927, 294.485 (232.567)	Log prob: -10932.004, -7126.777, -5073.932, -3670.634, -2714.178, -1988.550, -1390.137, -889.050, -469.001, -121.807, 182.361, 366.343, 430.826, 462.189, 484.460, 501.874 (440.771)	KLD: 19.444, 16.033, 15.029, 14.193, 13.407, 13.016, 12.615, 12.222, 11.939, 11.907, 12.008, 10.935, 9.878, 11.540, 11.367, 11.855 (208.204)	Grad: 221705.328
[Epoch  90 (283.51s)]	ELBO: -10947.608, -7154.193, -5119.534, -3728.314, -2785.159, -2073.555, -1488.935, -1000.126, -591.283, -255.676, 37.907, 214.662, 272.821, 292.864, 303.641, 310.623 (226.071)	Log prob: -10928.174, -7118.697, -5069.008, -3663.593, -2707.056, -1982.438, -1385.223, -884.189, -463.414, -115.943, 189.617, 377.091, 444.906, 476.503, 498.614, 517.387 (432.022)	KLD: 19.434, 16.062, 15.033, 14.193, 13.381, 13.014, 12.596, 12.225, 11.931, 11.864, 11.977, 10.719, 9.656, 11.554, 11.334, 11.792 (205.951)	Grad: 230506.844
[Epoch  91 (248.69s)]	ELBO: -10935.881, -7146.568, -5111.892, -3722.122, -2778.226, -2067.482, -1483.027, -994.978, -586.775, -251.016, 42.372, 219.367, 277.653, 298.363, 309.623, 315.687 (70.828)	Log prob: -10916.471, -7111.068, -5061.375, -3657.427, -2700.153, -1976.421, -1379.406, -879.164, -459.057, -111.431, 193.947, 381.322, 449.176, 481.336, 503.781, 521.690 (281.035)	KLD: 19.409, 16.091, 15.018, 14.177, 13.379, 12.989, 12.560, 12.193, 11.903, 11.868, 11.989, 10.380, 9.569, 11.449, 11.185, 11.845 (210.207)	Grad: 217299.656
[Epoch  92 (151.06s)]	ELBO: -10938.587, -7152.715, -5111.832, -3720.296, -2776.925, -2067.255, -1481.138, -993.845, -585.437, -249.986, 43.425, 219.510, 276.782, 295.267, 305.679, 311.732 (226.381)	Log prob: -10919.201, -7117.227, -5061.327, -3655.626, -2698.885, -1976.227, -1377.522, -878.016, -457.709, -110.367, 195.055, 381.862, 449.559, 479.267, 500.712, 518.425 (432.170)	KLD: 19.388, 16.102, 15.017, 14.164, 13.369, 12.988, 12.589, 12.213, 11.899, 11.891, 12.011, 10.722, 10.426, 11.222, 11.034, 11.660 (205.789)	Grad: 221228.641
[Epoch  93 (134.83s)]	ELBO: -10941.303, -7157.810, -5116.881, -3725.909, -2782.460, -2070.872, -1485.636, -996.202, -587.945, -252.330, 40.731, 219.480, 281.439, 301.656, 312.131, 316.527 (217.620)	Log prob: -10921.901, -7122.307, -5066.359, -3661.216, -2704.385, -1979.795, -1381.982, -880.375, -460.225, -112.758, 192.267, 381.480, 453.408, 484.981, 506.620, 522.811 (425.193)	KLD: 19.402, 16.098, 15.017, 14.178, 13.380, 13.003, 12.576, 12.174, 11.893, 11.853, 11.964, 10.463, 9.969, 11.357, 11.164, 11.795 (207.574)	Grad: 236634.984
[Epoch  94 (134.26s)]	ELBO: -10954.553, -7166.004, -5122.083, -3730.839, -2785.856, -2074.831, -1488.489, -1000.025, -591.774, -255.627, 38.641, 216.850, 280.890, 301.353, 310.560, 317.338 (251.474)	Log prob: -10935.146, -7130.520, -5071.534, -3666.122, -2707.798, -1983.770, -1384.835, -884.192, -464.044, -116.067, 190.162, 378.776, 452.789, 484.595, 505.139, 523.548 (456.336)	KLD: 19.403, 16.078, 15.070, 14.166, 13.342, 13.002, 12.592, 12.180, 11.897, 11.830, 11.960, 10.405, 9.972, 11.344, 11.337, 11.631 (204.863)	Grad: 243827.906
[Epoch  95 (132.99s)]	ELBO: -10947.713, -7160.257, -5116.624, -3726.912, -2782.950, -2071.424, -1484.831, -995.931, -587.169, -251.694, 42.739, 221.939, 286.589, 308.179, 320.158, 326.506 (244.702)	Log prob: -10928.285, -7124.764, -5066.111, -3662.222, -2704.924, -1980.401, -1381.242, -880.138, -459.485, -112.196, 194.151, 383.600, 457.960, 490.848, 513.925, 531.904 (449.663)	KLD: 19.428, 16.064, 15.021, 14.176, 13.337, 12.999, 12.565, 12.203, 11.891, 11.815, 11.914, 10.249, 9.711, 11.298, 11.097, 11.632 (204.961)	Grad: 214185.562
[Epoch  96 (135.09s)]	ELBO: -10933.406, -7147.462, -5107.771, -3718.308, -2775.507, -2065.521, -1480.631, -992.703, -584.237, -248.919, 44.405, 222.863, 286.675, 307.885, 318.599, 323.882 (233.202)	Log prob: -10913.983, -7111.970, -5057.270, -3653.658, -2697.500, -1974.541, -1377.099, -877.008, -456.668, -109.540, 195.708, 384.494, 458.450, 490.905, 512.641, 529.622 (440.606)	KLD: 19.416, 16.074, 15.013, 14.149, 13.355, 12.975, 12.550, 12.162, 11.875, 11.810, 11.925, 10.328, 10.144, 11.245, 11.022, 11.697 (207.404)	Grad: 230832.688
[Epoch  97 (130.16s)]	ELBO: -10934.061, -7147.520, -5110.696, -3720.099, -2775.662, -2065.373, -1480.294, -992.475, -582.991, -247.475, 46.747, 226.928, 293.857, 314.539, 326.206, 332.677 (260.733)	Log prob: -10914.635, -7111.966, -5060.180, -3655.411, -2697.622, -1974.341, -1376.735, -876.775, -455.431, -108.141, 198.009, 388.448, 465.316, 497.225, 519.910, 538.071 (466.132)	KLD: 19.429, 16.122, 14.965, 14.173, 13.351, 12.991, 12.527, 12.142, 11.861, 11.773, 11.928, 10.258, 9.940, 11.225, 11.019, 11.690 (205.399)	Grad: 232407.156
[Epoch  98 (126.30s)]	ELBO: -10951.764, -7163.772, -5116.711, -3725.052, -2780.893, -2070.208, -1485.039, -997.341, -588.240, -252.322, 41.527, 222.080, 289.584, 310.849, 322.189, 328.054 (254.197)	Log prob: -10932.324, -7128.247, -5066.170, -3660.353, -2702.868, -1979.238, -1381.534, -881.691, -460.727, -113.035, 192.739, 383.444, 460.698, 493.223, 515.620, 533.190 (458.987)	KLD: 19.440, 16.087, 15.013, 14.162, 13.323, 12.946, 12.536, 12.145, 11.862, 11.774, 11.925, 10.152, 9.750, 11.260, 11.057, 11.706 (204.790)	Grad: 232848.516
[Epoch  99 (127.18s)]	ELBO: -10944.589, -7152.850, -5114.296, -3722.751, -2779.794, -2068.356, -1483.815, -995.690, -587.250, -251.473, 42.869, 222.877, 291.194, 313.621, 325.932, 332.648 (249.633)	Log prob: -10925.146, -7117.318, -5063.797, -3658.109, -2701.849, -1977.428, -1380.368, -880.069, -459.775, -112.203, 194.048, 384.095, 462.028, 495.650, 518.850, 537.164 (452.979)	KLD: 19.441, 16.086, 14.968, 14.148, 13.301, 12.984, 12.518, 12.174, 11.854, 11.794, 11.910, 10.039, 9.615, 11.196, 10.888, 11.598 (203.346)	Grad: 227243.219
[Epoch 100 (127.01s)]	ELBO: -10943.142, -7151.801, -5113.146, -3721.541, -2777.863, -2066.383, -1480.893, -992.469, -583.418, -247.235, 47.004, 227.366, 295.974, 317.420, 329.429, 336.588 (263.365)	Log prob: -10923.720, -7116.288, -5062.601, -3656.843, -2699.830, -1975.405, -1377.389, -876.816, -455.909, -107.960, 198.176, 388.619, 466.934, 499.481, 522.295, 541.024 (467.680)	KLD: 19.425, 16.089, 15.029, 14.152, 13.336, 12.946, 12.526, 12.149, 11.857, 11.765, 11.898, 10.082, 9.706, 11.101, 10.804, 11.570 (204.316)	Grad: 234239.828
Best epoch(s): [100]	Training time(s): 16413.05s (16413.05s)	Best ELBO: 336.588 (263.365)	Best log prob: 541.024 (467.680)
Avg. mu: 0.167, 0.204, -0.084, 0.082, -0.079, 0.157, -0.140, 0.067, -0.041, 0.138, -0.111, 0.075, -0.068, 0.109, -0.071, 0.081, -0.055, 0.138, -0.057, 0.093, -0.058, 0.162, -0.066, 0.072, -0.041, 0.164, -0.023, 0.165, -0.060, 0.082, 0.007, 0.045, -0.088, 0.012, 0.040, 0.076, -0.021, 0.041, 0.013, 0.089, -0.126, 0.045, -0.032, 0.065, 0.200, 0.654, 0.887, 0.046, -0.070, 0.723, 1.306, -0.069, 0.064, 0.003, 0.671, -0.012, 0.345, 0.398, 0.878, 0.174, -0.236, 0.083, -0.099, 0.017
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 607829.688, 21.556, 462.728, 17218394914816.000, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.004, 0.002, 0.001, 0.002, 154.935, 0.004, 0.001, 0.003, 20338.553, 0.001, 0.001, 0.002, 0.013, 0.001, 0.001, 0.002, 0.037, 0.001, 0.001, 0.001, 0.001
Max. mu: 5.473, 3.028, 3.609, 2.888, 4.061, 3.357, 3.189, 3.647, 4.452, 4.748, 3.602, 3.286, 3.017, 3.827, 2.767, 2.925, 2.501, 2.949, 2.582, 4.534, 2.312, 2.797, 2.830, 3.954, 3.017, 3.109, 3.141, 3.243, 2.887, 3.090, 3.029, 4.466, 2.725, 17.859, 2.864, 3.148, 2.655, 6.290, 2.986, 2.880, 2.221, 2.241, 3.447, 3.181, 2.236, 2.562, 5.132, 0.880, 2.317, 2.142, 7.617, 0.857, 1.042, 0.539, 1.870, 0.411, 1.256, 1.271, 3.090, 0.618, 1.466, 1.086, 1.252, 2.410
Max. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.005, 0.001, 0.002, 0.099, 0.005, 0.004, 0.002, 0.025, 0.004, 0.006, 0.005, 0.007, 0.005, 0.004, 0.004, 0.038, 0.025, 0.017, 0.013, 0.011, 0.068, 0.019, 0.026, 5.606, 6078296576.000, 215545.172, 4627267.000, 172183950406451200.000, 0.120, 0.051, 0.015, 0.010, 0.190, 0.042, 0.083, 25.164, 1.140, 0.016, 0.657, 1546556.125, 2.237, 0.006, 0.786, 203376496.000, 0.074, 0.049, 0.241, 0.446, 0.006, 0.001, 0.017, 23.991, 0.109, 0.009, 0.016, 0.564
Min. mu: -2.781, -2.346, -4.051, -4.458, -3.124, -2.828, -4.677, -3.205, -3.068, -2.970, -3.354, -2.953, -3.937, -3.427, -2.843, -4.033, -2.742, -2.889, -3.205, -2.907, -2.792, -2.315, -2.962, -2.462, -4.080, -2.752, -3.026, -2.977, -3.375, -2.667, -3.115, -6.972, -22.324, -2.325, -2.996, -3.539, -3.867, -2.463, -8.441, -4.103, -2.260, -2.314, -4.138, -2.505, -2.698, -0.533, -0.330, -1.028, -2.348, -0.499, -0.264, -0.931, -1.035, -0.806, -1.361, -1.169, -4.649, -0.270, -0.106, -2.771, -1.499, -0.664, -1.202, -1.315
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.000, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.560 0.040 -0.021 -0.145 -0.005 0.005 -0.004 -0.031 0.001 0.021 -0.008
  0.016 0.009 -0.003 0.012 -0.006 0.005 0.005 -0.001 -0.001 0.002 0.010
  0.011 -0.002 0.001 -0.004 -0.010 -0.009 0.011 0.012 0.004 0.001 0.000
  0.006 -0.001 -0.002 0.007 -0.011 -0.007 -0.000 0.003 -0.005 -0.002
  0.000 -0.010 0.001 -0.003 -0.005 0.057 -0.015 -0.046 0.027 -0.000 0.024
  -0.007 0.013 -0.029 -0.019 -0.040 -0.010 0.029 0.019 -0.019 -0.000]
 [0.040 0.596 -0.021 -0.162 0.012 -0.005 -0.007 -0.002 -0.008 0.004 0.013
  0.013 0.000 -0.022 0.003 -0.010 0.001 -0.008 -0.008 -0.009 0.003 -0.007
  0.014 -0.004 0.004 0.013 -0.000 -0.010 -0.001 -0.006 0.009 -0.015 0.019
  0.001 0.003 -0.000 0.004 -0.021 0.007 0.008 -0.001 0.000 -0.001 0.012
  -0.004 0.000 -0.004 -0.002 0.048 -0.011 -0.058 0.013 0.013 0.009 -0.034
  0.006 0.004 -0.012 -0.004 -0.000 0.012 -0.005 -0.003 0.005]
 [-0.021 -0.021 0.624 0.031 0.006 0.002 0.015 0.002 -0.005 0.004 0.000
  -0.004 -0.006 0.005 0.031 0.002 0.003 -0.009 -0.007 -0.007 0.012 -0.000
  -0.002 -0.004 -0.014 -0.004 -0.004 -0.018 0.003 0.018 0.002 0.005 0.004
  0.011 -0.012 0.008 0.011 0.002 0.002 -0.022 0.005 -0.010 0.014 0.010
  0.014 -0.000 0.004 0.006 0.035 -0.016 -0.044 0.015 0.019 0.006 -0.033
  0.010 0.004 -0.011 -0.038 -0.000 0.015 0.002 -0.001 0.004]
 [-0.145 -0.162 0.031 0.765 0.005 0.013 -0.004 0.039 0.019 0.007 -0.025
  -0.014 0.004 0.019 0.002 0.004 0.004 0.002 0.000 0.008 0.023 0.005
  -0.016 -0.010 -0.011 0.007 0.002 0.021 -0.002 0.006 -0.015 -0.004 0.008
  0.007 -0.026 -0.008 0.006 0.012 0.008 -0.009 -0.003 -0.006 0.008 0.006
  0.017 -0.004 0.009 0.010 0.071 0.043 0.104 0.016 -0.027 0.006 0.048
  -0.004 0.028 -0.014 -0.013 0.010 0.011 -0.002 0.001 -0.001]
 [-0.005 0.012 0.006 0.005 0.411 -0.007 0.025 -0.022 0.017 0.012 -0.008
  0.002 -0.002 -0.002 -0.002 0.001 -0.001 -0.002 0.004 0.001 -0.000 0.007
  0.004 0.005 0.007 -0.004 -0.009 -0.002 -0.004 0.000 0.004 -0.013 -0.005
  0.009 -0.000 -0.002 0.001 0.009 -0.005 -0.003 0.004 0.006 0.003 -0.003
  0.004 -0.004 -0.004 0.001 0.000 0.002 -0.009 -0.002 0.002 0.001 -0.011
  0.001 -0.009 0.001 -0.009 -0.002 0.001 0.002 -0.005 0.000]
 [0.005 -0.005 0.002 0.013 -0.007 0.349 -0.009 -0.004 0.006 0.010 0.001
  -0.010 0.007 -0.001 0.003 -0.000 0.006 0.001 0.015 0.006 0.010 -0.006
  -0.005 0.003 0.003 0.002 -0.007 0.003 0.015 -0.006 0.006 -0.000 0.002
  -0.004 -0.004 -0.001 0.001 -0.004 -0.011 -0.002 -0.000 -0.007 -0.001
  -0.002 -0.002 0.004 0.016 -0.000 0.002 -0.000 0.014 0.002 -0.009 -0.004
  -0.007 -0.008 -0.009 0.002 0.013 -0.002 0.007 0.006 -0.006 0.003]
 [-0.004 -0.007 0.015 -0.004 0.025 -0.009 0.383 -0.027 0.003 0.004 -0.009
  0.006 -0.007 -0.000 -0.001 -0.005 0.013 0.006 -0.008 -0.013 0.007 0.007
  -0.005 0.001 -0.003 -0.012 -0.003 0.001 0.006 0.004 -0.014 0.002 0.004
  0.002 -0.010 -0.006 -0.002 0.004 0.001 -0.006 -0.002 0.001 0.013 0.003
  -0.005 -0.002 -0.005 -0.002 -0.011 0.001 -0.010 -0.003 -0.004 -0.002
  0.005 -0.003 0.005 -0.005 0.018 0.003 0.001 0.002 -0.008 -0.008]
 [-0.031 -0.002 0.002 0.039 -0.022 -0.004 -0.027 0.353 -0.004 0.008
  -0.000 -0.000 -0.012 0.003 -0.004 0.008 0.004 0.003 -0.001 0.013 0.003
  0.005 0.003 0.000 -0.003 -0.004 0.000 0.007 0.002 -0.008 -0.006 -0.014
  -0.002 0.015 0.001 0.004 0.008 0.005 0.006 0.012 -0.005 -0.006 0.007
  -0.001 0.007 0.001 0.004 0.003 -0.012 0.005 0.013 -0.006 -0.007 -0.002
  -0.001 -0.004 -0.001 0.004 -0.020 -0.002 -0.007 0.000 0.001 0.002]
 [0.001 -0.008 -0.005 0.019 0.017 0.006 0.003 -0.004 0.331 0.003 0.006
  -0.008 -0.004 -0.002 -0.005 0.010 0.001 -0.001 0.008 0.002 0.007 -0.005
  0.005 0.003 0.005 -0.008 0.004 0.008 0.007 -0.002 -0.004 0.001 0.000
  -0.000 -0.013 -0.009 0.003 0.005 -0.000 -0.007 -0.009 0.000 -0.004
  0.002 -0.006 -0.003 -0.000 -0.002 -0.006 -0.003 0.001 -0.004 0.008
  0.001 -0.001 0.005 -0.009 0.003 -0.011 -0.003 -0.001 -0.004 -0.002
  0.004]
 [0.021 0.004 0.004 0.007 0.012 0.010 0.004 0.008 0.003 0.342 -0.019
  0.002 0.008 -0.001 0.009 0.008 0.014 -0.002 0.010 0.002 -0.002 0.008
  0.011 -0.002 -0.001 -0.001 0.003 0.004 -0.004 0.005 0.003 -0.009 0.014
  0.002 -0.005 0.011 0.002 0.004 0.008 0.001 0.001 -0.007 -0.000 -0.006
  0.004 0.001 0.000 0.002 -0.008 0.000 -0.006 -0.004 0.009 0.004 -0.001
  0.006 -0.005 0.001 -0.011 -0.002 0.006 0.003 -0.008 -0.009]
 [-0.008 0.013 0.000 -0.025 -0.008 0.001 -0.009 -0.000 0.006 -0.019 0.370
  -0.001 0.001 0.005 0.010 -0.001 0.009 -0.005 0.001 -0.004 -0.002 -0.003
  0.010 0.003 0.006 -0.010 0.000 -0.004 -0.003 -0.013 -0.000 -0.005 0.003
  -0.004 -0.003 -0.001 0.002 0.001 -0.004 -0.001 -0.006 -0.008 -0.002
  0.009 -0.005 -0.001 0.008 -0.002 0.002 0.003 -0.004 0.002 -0.001 -0.002
  0.004 -0.001 -0.003 0.005 0.009 -0.001 0.005 -0.005 -0.010 -0.000]
 [0.016 0.013 -0.004 -0.014 0.002 -0.010 0.006 -0.000 -0.008 0.002 -0.001
  0.313 0.001 -0.000 -0.017 -0.006 0.005 -0.002 0.003 0.002 -0.001 -0.000
  0.001 0.005 -0.004 0.002 -0.006 -0.008 -0.012 -0.002 -0.000 -0.000
  0.001 0.004 -0.002 -0.006 0.002 -0.003 0.001 -0.007 -0.011 -0.003 0.005
  -0.013 0.001 0.001 0.001 0.000 0.003 -0.002 -0.008 0.000 0.003 -0.003
  0.003 -0.000 0.001 0.001 0.014 0.001 0.005 -0.004 -0.019 0.005]
 [0.009 0.000 -0.006 0.004 -0.002 0.007 -0.007 -0.012 -0.004 0.008 0.001
  0.001 0.331 0.005 0.002 0.006 -0.010 0.001 -0.006 -0.005 0.003 -0.003
  -0.002 0.008 -0.005 0.004 0.006 0.002 -0.004 0.005 -0.011 0.004 0.015
  -0.007 -0.007 -0.006 -0.004 -0.008 0.001 -0.001 0.002 0.005 -0.003
  0.000 -0.005 0.002 0.006 -0.002 0.001 0.005 -0.001 -0.000 -0.002 -0.001
  0.008 -0.000 0.001 0.003 -0.008 0.000 0.007 -0.006 0.005 0.006]
 [-0.003 -0.022 0.005 0.019 -0.002 -0.001 -0.000 0.003 -0.002 -0.001
  0.005 -0.000 0.005 0.346 -0.005 0.007 0.002 0.002 0.001 0.010 0.002
  0.004 -0.004 0.003 0.004 0.002 -0.002 -0.002 0.009 0.002 0.004 -0.000
  -0.003 0.008 0.003 0.010 -0.002 0.000 -0.007 0.003 -0.000 -0.002 0.000
  0.002 0.000 0.000 0.004 0.001 0.001 0.002 0.001 0.000 -0.003 0.003
  -0.000 -0.000 -0.004 -0.004 -0.010 -0.002 0.003 0.002 0.000 -0.003]
 [0.012 0.003 0.031 0.002 -0.002 0.003 -0.001 -0.004 -0.005 0.009 0.010
  -0.017 0.002 -0.005 0.354 -0.020 0.012 -0.004 0.003 0.003 0.006 0.006
  0.008 -0.006 0.003 0.000 -0.010 -0.005 0.009 -0.003 0.003 0.003 -0.005
  -0.003 -0.004 -0.000 -0.002 0.002 0.005 -0.001 0.001 -0.007 -0.004
  0.004 0.001 -0.002 -0.004 0.000 0.015 -0.000 -0.004 0.006 -0.002 -0.002
  -0.000 -0.002 0.002 -0.002 -0.001 0.000 0.001 -0.005 -0.008 -0.017]
 [-0.006 -0.010 0.002 0.004 0.001 -0.000 -0.005 0.008 0.010 0.008 -0.001
  -0.006 0.006 0.007 -0.020 0.304 0.001 -0.002 0.005 -0.014 0.002 0.002
  0.007 0.003 0.004 -0.003 0.002 0.004 -0.002 0.002 0.001 0.004 -0.004
  0.009 0.002 0.005 0.004 -0.002 -0.003 -0.003 -0.005 -0.005 -0.000
  -0.003 0.005 -0.002 -0.001 0.002 -0.003 0.001 0.001 -0.001 -0.002
  -0.002 -0.000 -0.001 -0.001 0.001 -0.005 -0.000 -0.002 -0.004 0.006
  0.015]
 [0.005 0.001 0.003 0.004 -0.001 0.006 0.013 0.004 0.001 0.014 0.009
  0.005 -0.010 0.002 0.012 0.001 0.331 -0.008 -0.001 -0.010 -0.001 -0.005
  0.001 -0.005 -0.003 0.001 0.005 -0.004 -0.006 0.001 0.009 0.008 -0.005
  -0.004 -0.002 -0.003 0.002 0.001 0.005 0.001 0.002 -0.005 0.008 0.005
  0.006 -0.000 0.009 0.003 -0.001 0.004 -0.005 -0.001 0.001 0.002 0.000
  0.001 -0.003 0.002 0.002 -0.001 0.001 -0.004 0.001 0.002]
 [0.005 -0.008 -0.009 0.002 -0.002 0.001 0.006 0.003 -0.001 -0.002 -0.005
  -0.002 0.001 0.002 -0.004 -0.002 -0.008 0.340 -0.011 0.008 0.006 -0.001
  0.001 -0.005 -0.007 0.003 -0.002 0.015 -0.007 -0.006 0.003 0.003 0.001
  0.006 0.004 0.000 0.002 0.004 0.001 0.008 0.000 0.010 0.003 -0.004
  -0.004 -0.001 -0.005 -0.001 -0.006 0.002 -0.005 -0.003 -0.002 0.001
  -0.002 -0.001 0.005 -0.000 -0.002 0.002 0.001 -0.000 -0.000 0.003]
 [-0.001 -0.008 -0.007 0.000 0.004 0.015 -0.008 -0.001 0.008 0.010 0.001
  0.003 -0.006 0.001 0.003 0.005 -0.001 -0.011 0.344 -0.001 0.006 0.015
  -0.009 -0.003 -0.000 -0.004 -0.003 -0.001 0.000 0.011 0.003 0.003
  -0.005 0.003 0.002 -0.006 0.000 -0.004 -0.004 0.004 -0.002 -0.007 0.008
  0.005 0.006 -0.001 0.008 0.002 -0.012 0.002 0.008 -0.004 -0.002 -0.002
  -0.003 -0.002 -0.001 -0.001 0.015 0.000 0.001 0.003 -0.005 -0.001]
 [-0.001 -0.009 -0.007 0.008 0.001 0.006 -0.013 0.013 0.002 0.002 -0.004
  0.002 -0.005 0.010 0.003 -0.014 -0.010 0.008 -0.001 0.360 -0.007 -0.003
  -0.002 0.009 -0.004 0.006 -0.011 0.002 0.010 -0.003 -0.003 -0.010
  -0.009 -0.005 -0.002 0.005 -0.006 0.005 0.001 -0.008 -0.006 0.000
  -0.001 -0.005 0.001 -0.000 -0.004 0.001 -0.001 -0.005 0.003 -0.001
  -0.002 -0.000 -0.002 -0.002 -0.002 0.001 0.003 -0.001 -0.001 -0.001
  0.002 -0.001]
 [0.002 0.003 0.012 0.023 -0.000 0.010 0.007 0.003 0.007 -0.002 -0.002
  -0.001 0.003 0.002 0.006 0.002 -0.001 0.006 0.006 -0.007 0.304 -0.007
  -0.002 -0.005 0.005 -0.004 -0.003 0.003 -0.002 0.001 0.002 -0.005
  -0.006 0.009 0.000 0.005 -0.005 0.004 0.004 0.008 -0.004 -0.002 0.007
  0.001 0.001 0.001 0.000 0.001 -0.007 0.006 -0.002 -0.003 -0.002 -0.001
  0.000 -0.002 -0.001 -0.001 0.008 -0.000 0.003 0.003 -0.001 -0.001]
 [0.010 -0.007 -0.000 0.005 0.007 -0.006 0.007 0.005 -0.005 0.008 -0.003
  -0.000 -0.003 0.004 0.006 0.002 -0.005 -0.001 0.015 -0.003 -0.007 0.304
  -0.014 0.003 0.012 -0.001 -0.008 0.009 0.001 0.006 0.000 -0.009 -0.005
  0.013 -0.001 0.002 0.002 0.004 -0.003 0.010 -0.003 -0.000 0.001 -0.008
  -0.001 -0.002 -0.007 -0.000 0.001 0.006 0.001 0.000 0.002 0.003 0.002
  0.002 0.002 -0.002 -0.008 0.000 -0.001 0.002 -0.001 0.005]
 [0.011 0.014 -0.002 -0.016 0.004 -0.005 -0.005 0.003 0.005 0.011 0.010
  0.001 -0.002 -0.004 0.008 0.007 0.001 0.001 -0.009 -0.002 -0.002 -0.014
  0.312 0.010 -0.005 0.005 0.007 -0.004 0.002 -0.001 0.005 0.006 0.007
  0.003 0.002 -0.007 -0.004 0.001 -0.002 0.001 -0.003 0.001 0.001 -0.000
  0.002 0.004 -0.000 0.001 -0.002 -0.006 -0.013 -0.001 0.000 -0.001
  -0.002 -0.001 -0.004 -0.000 -0.006 -0.002 0.005 0.003 -0.000 -0.001]
 [-0.002 -0.004 -0.004 -0.010 0.005 0.003 0.001 0.000 0.003 -0.002 0.003
  0.005 0.008 0.003 -0.006 0.003 -0.005 -0.005 -0.003 0.009 -0.005 0.003
  0.010 0.293 -0.000 0.003 0.003 0.005 0.008 -0.001 0.002 0.002 0.004
  0.007 -0.007 -0.002 -0.002 -0.002 -0.003 0.001 -0.003 0.002 -0.003
  0.003 -0.006 0.004 0.000 -0.003 -0.005 0.001 0.001 -0.002 0.002 -0.002
  0.005 0.000 0.003 -0.000 -0.005 0.001 -0.000 -0.001 0.000 0.002]
 [0.001 0.004 -0.014 -0.011 0.007 0.003 -0.003 -0.003 0.005 -0.001 0.006
  -0.004 -0.005 0.004 0.003 0.004 -0.003 -0.007 -0.000 -0.004 0.005 0.012
  -0.005 -0.000 0.348 0.000 0.002 0.004 0.007 -0.004 -0.002 0.005 -0.002
  0.005 0.007 -0.001 0.004 -0.005 -0.008 0.001 0.002 -0.000 -0.001 -0.004
  -0.002 -0.001 0.003 -0.000 0.011 -0.007 -0.001 0.005 -0.001 0.001
  -0.004 -0.000 -0.006 0.002 0.004 -0.002 0.001 0.004 -0.004 0.001]
 [-0.004 0.013 -0.004 0.007 -0.004 0.002 -0.012 -0.004 -0.008 -0.001
  -0.010 0.002 0.004 0.002 0.000 -0.003 0.001 0.003 -0.004 0.006 -0.004
  -0.001 0.005 0.003 0.000 0.322 -0.006 0.005 0.000 0.007 0.006 -0.003
  0.006 0.003 -0.002 -0.001 0.003 -0.003 -0.003 0.004 0.005 -0.003 -0.004
  0.002 0.006 0.002 0.001 0.002 0.005 -0.003 -0.012 0.002 -0.003 0.002
  -0.003 -0.001 0.002 -0.001 -0.012 0.000 0.001 0.002 0.004 0.002]
 [-0.010 -0.000 -0.004 0.002 -0.009 -0.007 -0.003 0.000 0.004 0.003 0.000
  -0.006 0.006 -0.002 -0.010 0.002 0.005 -0.002 -0.003 -0.011 -0.003
  -0.008 0.007 0.003 0.002 -0.006 0.339 0.005 -0.005 -0.011 -0.004 -0.004
  -0.007 0.005 -0.003 -0.002 0.007 0.004 0.003 0.000 -0.003 0.003 0.003
  -0.001 0.004 0.002 -0.008 0.001 -0.003 0.000 0.011 -0.000 -0.000 -0.004
  0.002 -0.001 -0.000 0.003 0.001 -0.000 -0.004 -0.002 0.006 0.004]
 [-0.009 -0.010 -0.018 0.021 -0.002 0.003 0.001 0.007 0.008 0.004 -0.004
  -0.008 0.002 -0.002 -0.005 0.004 -0.004 0.015 -0.001 0.002 0.003 0.009
  -0.004 0.005 0.004 0.005 0.005 0.345 -0.004 -0.005 -0.006 -0.006 -0.007
  0.004 0.000 -0.001 -0.004 0.007 0.002 -0.002 -0.008 -0.000 0.004 -0.002
  0.001 0.002 0.001 0.000 -0.004 -0.002 0.018 -0.001 -0.002 -0.002 -0.000
  -0.002 0.001 0.001 0.004 0.000 0.001 -0.004 0.001 -0.001]
 [0.011 -0.001 0.003 -0.002 -0.004 0.015 0.006 0.002 0.007 -0.004 -0.003
  -0.012 -0.004 0.009 0.009 -0.002 -0.006 -0.007 0.000 0.010 -0.002 0.001
  0.002 0.008 0.007 0.000 -0.005 -0.004 0.376 -0.005 0.008 0.022 0.015
  0.001 -0.006 0.000 0.008 0.005 -0.010 -0.012 0.006 -0.001 -0.008 0.004
  -0.008 -0.001 0.006 -0.002 0.007 -0.004 -0.014 0.003 -0.004 0.001 0.003
  -0.002 -0.002 -0.003 -0.011 -0.001 -0.003 -0.002 0.000 -0.001]
 [0.012 -0.006 0.018 0.006 0.000 -0.006 0.004 -0.008 -0.002 0.005 -0.013
  -0.002 0.005 0.002 -0.003 0.002 0.001 -0.006 0.011 -0.003 0.001 0.006
  -0.001 -0.001 -0.004 0.007 -0.011 -0.005 -0.005 0.328 0.006 0.002 0.003
  -0.003 0.005 0.008 -0.006 -0.001 0.002 0.008 -0.007 0.002 -0.002 -0.001
  0.008 -0.001 -0.004 0.003 0.008 -0.005 -0.009 0.003 0.001 0.004 -0.001
  0.002 -0.004 0.000 0.002 -0.001 0.004 0.002 -0.002 0.001]
 [0.004 0.009 0.002 -0.015 0.004 0.006 -0.014 -0.006 -0.004 0.003 -0.000
  -0.000 -0.011 0.004 0.003 0.001 0.009 0.003 0.003 -0.003 0.002 0.000
  0.005 0.002 -0.002 0.006 -0.004 -0.006 0.008 0.006 0.339 -0.001 -0.005
  0.007 0.006 -0.006 -0.003 0.002 -0.006 -0.002 -0.001 0.003 0.003 -0.000
  0.002 0.001 0.014 0.001 0.003 -0.004 0.005 0.001 0.000 0.002 0.000
  0.001 -0.004 -0.002 0.005 -0.001 -0.002 -0.004 0.005 0.002]
 [0.001 -0.015 0.005 -0.004 -0.013 -0.000 0.002 -0.014 0.001 -0.009
  -0.005 -0.000 0.004 -0.000 0.003 0.004 0.008 0.003 0.003 -0.010 -0.005
  -0.009 0.006 0.002 0.005 -0.003 -0.004 -0.006 0.022 0.002 -0.001 0.349
  0.021 -0.015 0.001 -0.002 0.002 -0.004 -0.001 -0.007 0.003 0.007 -0.007
  -0.004 -0.009 0.005 -0.000 -0.004 -0.001 -0.003 -0.004 -0.000 -0.001
  -0.001 0.002 -0.001 0.006 -0.001 -0.005 0.003 -0.001 -0.001 0.000
  -0.002]
 [0.000 0.019 0.004 0.008 -0.005 0.002 0.004 -0.002 0.000 0.014 0.003
  0.001 0.015 -0.003 -0.005 -0.004 -0.005 0.001 -0.005 -0.009 -0.006
  -0.005 0.007 0.004 -0.002 0.006 -0.007 -0.007 0.015 0.003 -0.005 0.021
  0.384 -0.052 0.008 0.005 -0.002 -0.018 0.013 -0.003 -0.000 0.005 -0.004
  -0.009 0.005 0.001 -0.006 0.001 -0.002 -0.005 -0.042 -0.003 0.003 0.000
  0.002 0.003 0.009 0.001 -0.009 0.006 0.002 -0.002 -0.002 -0.008]
 [0.006 0.001 0.011 0.007 0.009 -0.004 0.002 0.015 -0.000 0.002 -0.004
  0.004 -0.007 0.008 -0.003 0.009 -0.004 0.006 0.003 -0.005 0.009 0.013
  0.003 0.007 0.005 0.003 0.005 0.004 0.001 -0.003 0.007 -0.015 -0.052
  0.322 -0.004 -0.000 0.002 0.017 -0.018 0.006 -0.002 -0.001 0.012 0.003
  -0.006 0.002 -0.000 -0.001 0.005 0.000 -0.003 0.002 0.004 0.003 -0.008
  0.002 -0.013 -0.002 -0.009 -0.007 0.003 0.001 0.000 0.008]
 [-0.001 0.003 -0.012 -0.026 -0.000 -0.004 -0.010 0.001 -0.013 -0.005
  -0.003 -0.002 -0.007 0.003 -0.004 0.002 -0.002 0.004 0.002 -0.002 0.000
  -0.001 0.002 -0.007 0.007 -0.002 -0.003 0.000 -0.006 0.005 0.006 0.001
  0.008 -0.004 0.300 0.005 -0.008 -0.008 0.011 0.005 0.001 -0.001 0.002
  -0.002 -0.007 0.003 0.004 -0.003 0.002 -0.003 -0.003 0.001 0.001 -0.003
  0.002 -0.001 0.001 0.001 0.003 0.000 0.001 0.002 -0.001 -0.001]
 [-0.002 -0.000 0.008 -0.008 -0.002 -0.001 -0.006 0.004 -0.009 0.011
  -0.001 -0.006 -0.006 0.010 -0.000 0.005 -0.003 0.000 -0.006 0.005 0.005
  0.002 -0.007 -0.002 -0.001 -0.001 -0.002 -0.001 0.000 0.008 -0.006
  -0.002 0.005 -0.000 0.005 0.319 -0.000 0.003 0.009 -0.004 0.001 -0.002
  0.003 0.007 -0.003 0.002 -0.001 -0.001 0.001 -0.003 -0.014 -0.000 0.002
  -0.000 0.005 0.001 -0.000 0.000 -0.001 0.000 -0.003 -0.003 -0.001 0.002]
 [0.007 0.004 0.011 0.006 0.001 0.001 -0.002 0.008 0.003 0.002 0.002
  0.002 -0.004 -0.002 -0.002 0.004 0.002 0.002 0.000 -0.006 -0.005 0.002
  -0.004 -0.002 0.004 0.003 0.007 -0.004 0.008 -0.006 -0.003 0.002 -0.002
  0.002 -0.008 -0.000 0.276 0.006 0.001 0.009 0.006 0.003 0.005 -0.002
  0.010 -0.002 -0.008 0.003 0.000 -0.001 -0.004 -0.000 0.001 0.001 0.001
  0.001 -0.001 -0.002 -0.006 -0.001 0.003 0.000 -0.001 0.005]
 [-0.011 -0.021 0.002 0.012 0.009 -0.004 0.004 0.005 0.005 0.004 0.001
  -0.003 -0.008 0.000 0.002 -0.002 0.001 0.004 -0.004 0.005 0.004 0.004
  0.001 -0.002 -0.005 -0.003 0.004 0.007 0.005 -0.001 0.002 -0.004 -0.018
  0.017 -0.008 0.003 0.006 0.237 -0.001 -0.006 -0.001 0.006 -0.001 0.001
  -0.005 0.000 0.002 -0.001 0.000 -0.000 0.000 0.000 -0.000 -0.001 0.005
  -0.000 -0.003 -0.002 -0.007 -0.002 0.004 0.002 -0.001 0.004]
 [-0.007 0.007 0.002 0.008 -0.005 -0.011 0.001 0.006 -0.000 0.008 -0.004
  0.001 0.001 -0.007 0.005 -0.003 0.005 0.001 -0.004 0.001 0.004 -0.003
  -0.002 -0.003 -0.008 -0.003 0.003 0.002 -0.010 0.002 -0.006 -0.001
  0.013 -0.018 0.011 0.009 0.001 -0.001 0.277 -0.007 -0.001 0.009 0.001
  -0.007 -0.004 0.002 -0.005 -0.002 -0.002 0.003 -0.004 -0.002 0.002
  0.000 0.003 0.002 0.007 0.000 0.004 0.003 -0.001 -0.002 0.003 -0.000]
 [-0.000 0.008 -0.022 -0.009 -0.003 -0.002 -0.006 0.012 -0.007 0.001
  -0.001 -0.007 -0.001 0.003 -0.001 -0.003 0.001 0.008 0.004 -0.008 0.008
  0.010 0.001 0.001 0.001 0.004 0.000 -0.002 -0.012 0.008 -0.002 -0.007
  -0.003 0.006 0.005 -0.004 0.009 -0.006 -0.007 0.290 -0.002 -0.004 0.006
  0.003 -0.001 -0.000 -0.002 -0.001 -0.015 0.003 0.002 -0.006 -0.003
  -0.000 0.004 -0.001 -0.004 -0.000 -0.001 -0.002 -0.001 -0.003 0.001
  -0.000]
 [0.003 -0.001 0.005 -0.003 0.004 -0.000 -0.002 -0.005 -0.009 0.001
  -0.006 -0.011 0.002 -0.000 0.001 -0.005 0.002 0.000 -0.002 -0.006
  -0.004 -0.003 -0.003 -0.003 0.002 0.005 -0.003 -0.008 0.006 -0.007
  -0.001 0.003 -0.000 -0.002 0.001 0.001 0.006 -0.001 -0.001 -0.002 0.222
  0.013 -0.006 0.014 0.002 0.001 -0.002 0.001 -0.001 -0.003 0.002 0.000
  0.001 0.000 0.001 0.000 0.001 0.001 -0.005 0.000 0.000 0.003 0.000
  -0.003]
 [-0.005 0.000 -0.010 -0.006 0.006 -0.007 0.001 -0.006 0.000 -0.007
  -0.008 -0.003 0.005 -0.002 -0.007 -0.005 -0.005 0.010 -0.007 0.000
  -0.002 -0.000 0.001 0.002 -0.000 -0.003 0.003 -0.000 -0.001 0.002 0.003
  0.007 0.005 -0.001 -0.001 -0.002 0.003 0.006 0.009 -0.004 0.013 0.187
  -0.008 -0.015 0.001 -0.001 -0.003 0.001 -0.006 -0.003 -0.014 -0.003
  -0.001 0.001 0.001 0.000 -0.004 -0.002 0.000 -0.001 -0.002 -0.001 0.001
  -0.001]
 [-0.002 -0.001 0.014 0.008 0.003 -0.001 0.013 0.007 -0.004 -0.000 -0.002
  0.005 -0.003 0.000 -0.004 -0.000 0.008 0.003 0.008 -0.001 0.007 0.001
  0.001 -0.003 -0.001 -0.004 0.003 0.004 -0.008 -0.002 0.003 -0.007
  -0.004 0.012 0.002 0.003 0.005 -0.001 0.001 0.006 -0.006 -0.008 0.211
  0.002 -0.002 -0.002 -0.001 -0.001 0.003 0.006 -0.000 0.001 -0.000
  -0.003 0.003 -0.001 -0.002 -0.003 -0.006 -0.001 0.003 0.002 0.000 0.003]
 [0.000 0.012 0.010 0.006 -0.003 -0.002 0.003 -0.001 0.002 -0.006 0.009
  -0.013 0.000 0.002 0.004 -0.003 0.005 -0.004 0.005 -0.005 0.001 -0.008
  -0.000 0.003 -0.004 0.002 -0.001 -0.002 0.004 -0.001 -0.000 -0.004
  -0.009 0.003 -0.002 0.007 -0.002 0.001 -0.007 0.003 0.014 -0.015 0.002
  0.214 0.002 0.001 0.008 0.001 0.005 0.002 0.004 0.002 -0.001 -0.000
  0.000 -0.001 -0.002 0.001 0.001 -0.001 0.002 -0.000 0.002 0.001]
 [-0.010 -0.004 0.014 0.017 0.004 -0.002 -0.005 0.007 -0.006 0.004 -0.005
  0.001 -0.005 0.000 0.001 0.005 0.006 -0.004 0.006 0.001 0.001 -0.001
  0.002 -0.006 -0.002 0.006 0.004 0.001 -0.008 0.008 0.002 -0.009 0.005
  -0.006 -0.007 -0.003 0.010 -0.005 -0.004 -0.001 0.002 0.001 -0.002
  0.002 0.238 -0.016 0.004 0.093 0.004 0.001 0.003 0.001 -0.004 -0.004
  0.002 -0.004 0.006 0.002 0.006 0.002 -0.003 0.003 0.001 0.000]
 [0.001 0.000 -0.000 -0.004 -0.004 0.004 -0.002 0.001 -0.003 0.001 -0.001
  0.001 0.002 0.000 -0.002 -0.002 -0.000 -0.001 -0.001 -0.000 0.001
  -0.002 0.004 0.004 -0.001 0.002 0.002 0.002 -0.001 -0.001 0.001 0.005
  0.001 0.002 0.003 0.002 -0.002 0.000 0.002 -0.000 0.001 -0.001 -0.002
  0.001 -0.016 0.077 0.008 -0.009 -0.004 0.004 0.007 -0.001 0.000 -0.001
  0.001 -0.001 0.000 0.002 0.006 0.000 0.000 -0.001 0.003 0.001]
 [-0.003 -0.004 0.004 0.009 -0.004 0.016 -0.005 0.004 -0.000 0.000 0.008
  0.001 0.006 0.004 -0.004 -0.001 0.009 -0.005 0.008 -0.004 0.000 -0.007
  -0.000 0.000 0.003 0.001 -0.008 0.001 0.006 -0.004 0.014 -0.000 -0.006
  -0.000 0.004 -0.001 -0.008 0.002 -0.005 -0.002 -0.002 -0.003 -0.001
  0.008 0.004 0.008 0.188 0.010 -0.010 0.010 0.015 -0.004 0.000 0.000
  0.002 -0.000 0.000 -0.000 0.011 0.000 0.004 -0.001 -0.001 0.002]
 [-0.005 -0.002 0.006 0.010 0.001 -0.000 -0.002 0.003 -0.002 0.002 -0.002
  0.000 -0.002 0.001 0.000 0.002 0.003 -0.001 0.002 0.001 0.001 -0.000
  0.001 -0.003 -0.000 0.002 0.001 0.000 -0.002 0.003 0.001 -0.004 0.001
  -0.001 -0.003 -0.001 0.003 -0.001 -0.002 -0.001 0.001 0.001 -0.001
  0.001 0.093 -0.009 0.010 0.036 0.001 0.001 0.002 0.000 -0.002 -0.001
  0.001 -0.001 0.002 0.001 0.002 0.001 -0.001 0.001 0.000 0.000]
 [0.057 0.048 0.035 0.071 0.000 0.002 -0.011 -0.012 -0.006 -0.008 0.002
  0.003 0.001 0.001 0.015 -0.003 -0.001 -0.006 -0.012 -0.001 -0.007 0.001
  -0.002 -0.005 0.011 0.005 -0.003 -0.004 0.007 0.008 0.003 -0.001 -0.002
  0.005 0.002 0.001 0.000 0.000 -0.002 -0.015 -0.001 -0.006 0.003 0.005
  0.004 -0.004 -0.010 0.001 0.203 -0.073 0.033 0.080 -0.002 0.010 0.006
  0.005 -0.001 -0.009 -0.013 -0.001 0.010 0.008 -0.008 -0.001]
 [-0.015 -0.011 -0.016 0.043 0.002 -0.000 0.001 0.005 -0.003 0.000 0.003
  -0.002 0.005 0.002 -0.000 0.001 0.004 0.002 0.002 -0.005 0.006 0.006
  -0.006 0.001 -0.007 -0.003 0.000 -0.002 -0.004 -0.005 -0.004 -0.003
  -0.005 0.000 -0.003 -0.003 -0.001 -0.000 0.003 0.003 -0.003 -0.003
  0.006 0.002 0.001 0.004 0.010 0.001 -0.073 0.111 -0.042 -0.034 -0.005
  -0.004 0.005 -0.004 -0.001 0.001 0.002 -0.000 -0.002 -0.001 0.001 0.000]
 [-0.046 -0.058 -0.044 0.104 -0.009 0.014 -0.010 0.013 0.001 -0.006
  -0.004 -0.008 -0.001 0.001 -0.004 0.001 -0.005 -0.005 0.008 0.003
  -0.002 0.001 -0.013 0.001 -0.001 -0.012 0.011 0.018 -0.014 -0.009 0.005
  -0.004 -0.042 -0.003 -0.003 -0.014 -0.004 0.000 -0.004 0.002 0.002
  -0.014 -0.000 0.004 0.003 0.007 0.015 0.002 0.033 -0.042 0.123 0.018
  -0.001 0.002 0.011 0.001 -0.001 0.002 0.009 -0.001 0.000 0.001 0.003
  0.001]
 [0.027 0.013 0.015 0.016 -0.002 0.002 -0.003 -0.006 -0.004 -0.004 0.002
  0.000 -0.000 0.000 0.006 -0.001 -0.001 -0.003 -0.004 -0.001 -0.003
  0.000 -0.001 -0.002 0.005 0.002 -0.000 -0.001 0.003 0.003 0.001 -0.000
  -0.003 0.002 0.001 -0.000 -0.000 0.000 -0.002 -0.006 0.000 -0.003 0.001
  0.002 0.001 -0.001 -0.004 0.000 0.080 -0.034 0.018 0.033 -0.000 0.004
  0.001 0.002 -0.001 -0.003 -0.005 -0.001 0.004 0.003 -0.003 -0.000]
 [-0.000 0.013 0.019 -0.027 0.002 -0.009 -0.004 -0.007 0.008 0.009 -0.001
  0.003 -0.002 -0.003 -0.002 -0.002 0.001 -0.002 -0.002 -0.002 -0.002
  0.002 0.000 0.002 -0.001 -0.003 -0.000 -0.002 -0.004 0.001 0.000 -0.001
  0.003 0.004 0.001 0.002 0.001 -0.000 0.002 -0.003 0.001 -0.001 -0.000
  -0.001 -0.004 0.000 0.000 -0.002 -0.002 -0.005 -0.001 -0.000 0.027
  0.000 -0.006 0.014 -0.001 0.000 -0.005 -0.001 -0.001 -0.000 -0.001
  -0.000]
 [0.024 0.009 0.006 0.006 0.001 -0.004 -0.002 -0.002 0.001 0.004 -0.002
  -0.003 -0.001 0.003 -0.002 -0.002 0.002 0.001 -0.002 -0.000 -0.001
  0.003 -0.001 -0.002 0.001 0.002 -0.004 -0.002 0.001 0.004 0.002 -0.001
  0.000 0.003 -0.003 -0.000 0.001 -0.001 0.000 -0.000 0.000 0.001 -0.003
  -0.000 -0.004 -0.001 0.000 -0.001 0.010 -0.004 0.002 0.004 0.000 0.013
  -0.002 0.006 -0.000 -0.002 -0.003 -0.000 0.004 0.002 -0.001 0.000]
 [-0.007 -0.034 -0.033 0.048 -0.011 -0.007 0.005 -0.001 -0.001 -0.001
  0.004 0.003 0.008 -0.000 -0.000 -0.000 0.000 -0.002 -0.003 -0.002 0.000
  0.002 -0.002 0.005 -0.004 -0.003 0.002 -0.000 0.003 -0.001 0.000 0.002
  0.002 -0.008 0.002 0.005 0.001 0.005 0.003 0.004 0.001 0.001 0.003
  0.000 0.002 0.001 0.002 0.001 0.006 0.005 0.011 0.001 -0.006 -0.002
  0.043 0.000 0.004 -0.003 -0.003 0.001 0.002 0.001 0.002 0.001]
 [0.013 0.006 0.010 -0.004 0.001 -0.008 -0.003 -0.004 0.005 0.006 -0.001
  -0.000 -0.000 -0.000 -0.002 -0.001 0.001 -0.001 -0.002 -0.002 -0.002
  0.002 -0.001 0.000 -0.000 -0.001 -0.001 -0.002 -0.002 0.002 0.001
  -0.001 0.003 0.002 -0.001 0.001 0.001 -0.000 0.002 -0.001 0.000 0.000
  -0.001 -0.001 -0.004 -0.001 -0.000 -0.001 0.005 -0.004 0.001 0.002
  0.014 0.006 0.000 0.010 -0.000 -0.001 -0.005 -0.000 0.002 0.001 -0.001
  0.000]
 [-0.029 0.004 0.004 0.028 -0.009 -0.009 0.005 -0.001 -0.009 -0.005
  -0.003 0.001 0.001 -0.004 0.002 -0.001 -0.003 0.005 -0.001 -0.002
  -0.001 0.002 -0.004 0.003 -0.006 0.002 -0.000 0.001 -0.002 -0.004
  -0.004 0.006 0.009 -0.013 0.001 -0.000 -0.001 -0.003 0.007 -0.004 0.001
  -0.004 -0.002 -0.002 0.006 0.000 0.000 0.002 -0.001 -0.001 -0.001
  -0.001 -0.001 -0.000 0.004 -0.000 0.023 -0.003 -0.006 0.008 -0.000
  -0.001 0.001 -0.001]
 [-0.019 -0.012 -0.011 -0.014 0.001 0.002 -0.005 0.004 0.003 0.001 0.005
  0.001 0.003 -0.004 -0.002 0.001 0.002 -0.000 -0.001 0.001 -0.001 -0.002
  -0.000 -0.000 0.002 -0.001 0.003 0.001 -0.003 0.000 -0.002 -0.001 0.001
  -0.002 0.001 0.000 -0.002 -0.002 0.000 -0.000 0.001 -0.002 -0.003 0.001
  0.002 0.002 -0.000 0.001 -0.009 0.001 0.002 -0.003 0.000 -0.002 -0.003
  -0.001 -0.003 0.014 0.011 -0.001 -0.004 -0.000 -0.001 -0.001]
 [-0.040 -0.004 -0.038 -0.013 -0.009 0.013 0.018 -0.020 -0.011 -0.011
  0.009 0.014 -0.008 -0.010 -0.001 -0.005 0.002 -0.002 0.015 0.003 0.008
  -0.008 -0.006 -0.005 0.004 -0.012 0.001 0.004 -0.011 0.002 0.005 -0.005
  -0.009 -0.009 0.003 -0.001 -0.006 -0.007 0.004 -0.001 -0.005 0.000
  -0.006 0.001 0.006 0.006 0.011 0.002 -0.013 0.002 0.009 -0.005 -0.005
  -0.003 -0.003 -0.005 -0.006 0.011 0.087 0.001 -0.005 -0.003 -0.003
  -0.004]
 [-0.010 -0.000 -0.000 0.010 -0.002 -0.002 0.003 -0.002 -0.003 -0.002
  -0.001 0.001 0.000 -0.002 0.000 -0.000 -0.001 0.002 0.000 -0.001 -0.000
  0.000 -0.002 0.001 -0.002 0.000 -0.000 0.000 -0.001 -0.001 -0.001 0.003
  0.006 -0.007 0.000 0.000 -0.001 -0.002 0.003 -0.002 0.000 -0.001 -0.001
  -0.001 0.002 0.000 0.000 0.001 -0.001 -0.000 -0.001 -0.001 -0.001
  -0.000 0.001 -0.000 0.008 -0.001 0.001 0.003 -0.000 -0.001 0.000 -0.001]
 [0.029 0.012 0.015 0.011 0.001 0.007 0.001 -0.007 -0.001 0.006 0.005
  0.005 0.007 0.003 0.001 -0.002 0.001 0.001 0.001 -0.001 0.003 -0.001
  0.005 -0.000 0.001 0.001 -0.004 0.001 -0.003 0.004 -0.002 -0.001 0.002
  0.003 0.001 -0.003 0.003 0.004 -0.001 -0.001 0.000 -0.002 0.003 0.002
  -0.003 0.000 0.004 -0.001 0.010 -0.002 0.000 0.004 -0.001 0.004 0.002
  0.002 -0.000 -0.004 -0.005 -0.000 0.020 0.007 -0.003 0.003]
 [0.019 -0.005 0.002 -0.002 0.002 0.006 0.002 0.000 -0.004 0.003 -0.005
  -0.004 -0.006 0.002 -0.005 -0.004 -0.004 -0.000 0.003 -0.001 0.003
  0.002 0.003 -0.001 0.004 0.002 -0.002 -0.004 -0.002 0.002 -0.004 -0.001
  -0.002 0.001 0.002 -0.003 0.000 0.002 -0.002 -0.003 0.003 -0.001 0.002
  -0.000 0.003 -0.001 -0.001 0.001 0.008 -0.001 0.001 0.003 -0.000 0.002
  0.001 0.001 -0.001 -0.000 -0.003 -0.001 0.007 0.012 -0.003 -0.001]
 [-0.019 -0.003 -0.001 0.001 -0.005 -0.006 -0.008 0.001 -0.002 -0.008
  -0.010 -0.019 0.005 0.000 -0.008 0.006 0.001 -0.000 -0.005 0.002 -0.001
  -0.001 -0.000 0.000 -0.004 0.004 0.006 0.001 0.000 -0.002 0.005 0.000
  -0.002 0.000 -0.001 -0.001 -0.001 -0.001 0.003 0.001 0.000 0.001 0.000
  0.002 0.001 0.003 -0.001 0.000 -0.008 0.001 0.003 -0.003 -0.001 -0.001
  0.002 -0.001 0.001 -0.001 -0.003 0.000 -0.003 -0.003 0.018 0.003]
 [-0.000 0.005 0.004 -0.001 0.000 0.003 -0.008 0.002 0.004 -0.009 -0.000
  0.005 0.006 -0.003 -0.017 0.015 0.002 0.003 -0.001 -0.001 -0.001 0.005
  -0.001 0.002 0.001 0.002 0.004 -0.001 -0.001 0.001 0.002 -0.002 -0.008
  0.008 -0.001 0.002 0.005 0.004 -0.000 -0.000 -0.003 -0.001 0.003 0.001
  0.000 0.001 0.002 0.000 -0.001 0.000 0.001 -0.000 -0.000 0.000 0.001
  0.000 -0.001 -0.001 -0.004 -0.001 0.003 -0.001 0.003 0.016]]
