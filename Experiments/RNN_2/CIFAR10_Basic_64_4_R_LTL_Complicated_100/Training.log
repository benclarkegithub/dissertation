Encoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Encoder                                  --
├─Linear: 1-1                            786,688
├─Linear: 1-2                            65,792
=================================================================
Total params: 852,480
Trainable params: 852,480
Non-trainable params: 0
=================================================================
Encoder to Latents 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Encoder to Latents 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
EncoderToLatents                         --
├─Linear: 1-1                            1,028
├─Linear: 1-2                            1,028
=================================================================
Total params: 2,056
Trainable params: 2,056
Non-trainable params: 0
=================================================================
Latents to Latents
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToLatentsComplicated              --
├─Linear: 1-1                            1,700
├─Linear: 1-2                            10,100
├─Linear: 1-3                            10,100
├─Linear: 1-4                            404
├─Linear: 1-5                            404
=================================================================
Total params: 22,708
Trainable params: 22,708
Non-trainable params: 0
=================================================================
Latents to Decoder 0
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 1
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 2
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 3
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 4
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 5
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 6
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 7
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 8
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 9
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 10
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 11
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 12
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 13
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 14
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Latents to Decoder 15
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
LatentsToDecoder                         --
├─Linear: 1-1                            1,280
=================================================================
Total params: 1,280
Trainable params: 1,280
Non-trainable params: 0
=================================================================
Decoder
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
Decoder                                  --
├─Linear: 1-1                            65,792
├─Linear: 1-2                            789,504
=================================================================
Total params: 855,296
Trainable params: 855,296
Non-trainable params: 0
=================================================================
Training model 1/16...
[Epoch   1 (12.18s)]	ELBO: -17598.441 (-13818.969)	Log prob: -17581.562 (-13800.587)	KLD: 16.878 (18.382)	Grad: 61.720
[Epoch   2 (12.12s)]	ELBO: -13638.774 (-13576.557)	Log prob: -13621.121 (-13559.471)	KLD: 17.653 (17.086)	Grad: 43.569
[Epoch   3 (11.83s)]	ELBO: -12461.671 (-12191.383)	Log prob: -12443.006 (-12172.842)	KLD: 18.663 (18.541)	Grad: 48.721
[Epoch   4 (13.06s)]	ELBO: -11965.389 (-11857.536)	Log prob: -11946.613 (-11838.536)	KLD: 18.776 (18.999)	Grad: 45.179
[Epoch   5 (12.29s)]	ELBO: -11780.500 (-11681.027)	Log prob: -11761.247 (-11662.343)	KLD: 19.250 (18.684)	Grad: 48.564
[Epoch   6 (12.24s)]	ELBO: -11621.618 (-11488.236)	Log prob: -11602.137 (-11468.674)	KLD: 19.482 (19.561)	Grad: 51.794
[Epoch   7 (12.77s)]	ELBO: -11429.169 (-11344.667)	Log prob: -11409.274 (-11323.771)	KLD: 19.897 (20.896)	Grad: 58.368
[Epoch   8 (11.07s)]	ELBO: -11314.854 (-11285.181)	Log prob: -11294.313 (-11264.373)	KLD: 20.541 (20.808)	Grad: 57.064
[Epoch   9 (10.44s)]	ELBO: -11267.361 (-11468.978)	Log prob: -11246.613 (-11447.159)	KLD: 20.747 (21.819)	Grad: 63.672
[Epoch  10 (10.35s)]	ELBO: -11213.043 (-11526.319)	Log prob: -11191.881 (-11505.612)	KLD: 21.162 (20.708)	Grad: 65.512
Training model 2/16...
[Epoch   1 (15.81s)]	ELBO: -9716.628 (-8602.634)	Log prob: -9676.538 (-8561.191)	KLD: 18.800 (41.444)	Grad: 26.117
[Epoch   2 (15.93s)]	ELBO: -8560.760 (-8344.568)	Log prob: -8520.277 (-8305.905)	KLD: 19.241 (38.663)	Grad: 20.110
[Epoch   3 (17.73s)]	ELBO: -8235.011 (-7935.390)	Log prob: -8194.790 (-7897.016)	KLD: 18.899 (38.375)	Grad: 21.020
[Epoch   4 (19.01s)]	ELBO: -8065.038 (-8044.949)	Log prob: -8025.278 (-8005.079)	KLD: 18.841 (39.870)	Grad: 19.763
[Epoch   5 (19.13s)]	ELBO: -8161.872 (-8260.390)	Log prob: -8122.078 (-8219.377)	KLD: 18.613 (41.013)	Grad: 21.283
[Epoch   6 (19.09s)]	ELBO: -8001.874 (-7831.625)	Log prob: -7962.255 (-7793.013)	KLD: 18.366 (38.612)	Grad: 22.408
[Epoch   7 (19.28s)]	ELBO: -7914.754 (-7976.706)	Log prob: -7875.462 (-7938.422)	KLD: 18.181 (38.284)	Grad: 20.948
[Epoch   8 (19.16s)]	ELBO: -7897.735 (-7970.933)	Log prob: -7858.747 (-7930.906)	KLD: 18.357 (40.027)	Grad: 19.739
[Epoch   9 (19.27s)]	ELBO: -8131.805 (-7829.698)	Log prob: -8091.527 (-7788.896)	KLD: 18.550 (40.802)	Grad: 26.488
[Epoch  10 (19.32s)]	ELBO: -7891.625 (-7872.519)	Log prob: -7852.617 (-7832.557)	KLD: 17.914 (39.962)	Grad: 20.956
Training model 3/16...
[Epoch   1 (26.73s)]	ELBO: -7467.821 (-7122.050)	Log prob: -7404.649 (-7065.771)	KLD: 25.147 (56.279)	Grad: 24.979
[Epoch   2 (26.77s)]	ELBO: -7139.623 (-6889.028)	Log prob: -7083.580 (-6831.515)	KLD: 17.611 (57.513)	Grad: 18.026
[Epoch   3 (27.32s)]	ELBO: -6776.850 (-6517.403)	Log prob: -6719.821 (-6462.909)	KLD: 18.419 (54.494)	Grad: 13.918
[Epoch   4 (27.17s)]	ELBO: -6514.329 (-6290.921)	Log prob: -6457.223 (-6233.417)	KLD: 18.255 (57.503)	Grad: 17.980
[Epoch   5 (27.24s)]	ELBO: -6319.928 (-6063.164)	Log prob: -6262.456 (-6005.433)	KLD: 18.191 (57.731)	Grad: 18.009
[Epoch   6 (27.34s)]	ELBO: -6159.026 (-6049.792)	Log prob: -6101.829 (-5992.335)	KLD: 17.785 (57.456)	Grad: 17.267
[Epoch   7 (27.58s)]	ELBO: -6078.401 (-5932.130)	Log prob: -6020.386 (-5873.516)	KLD: 17.900 (58.614)	Grad: 16.841
[Epoch   8 (27.64s)]	ELBO: -5991.568 (-5895.412)	Log prob: -5934.197 (-5838.966)	KLD: 18.047 (56.445)	Grad: 17.462
[Epoch   9 (27.61s)]	ELBO: -5903.004 (-5777.704)	Log prob: -5845.318 (-5716.864)	KLD: 17.833 (60.840)	Grad: 16.318
[Epoch  10 (28.10s)]	ELBO: -6168.185 (-5927.460)	Log prob: -6107.727 (-5865.609)	KLD: 17.835 (61.852)	Grad: 19.622
Training model 4/16...
[Epoch   1 (36.28s)]	ELBO: -5952.438 (-5674.093)	Log prob: -5876.894 (-5603.030)	KLD: 16.961 (71.064)	Grad: 23.479
[Epoch   2 (36.01s)]	ELBO: -5644.165 (-5450.786)	Log prob: -5572.150 (-5377.916)	KLD: 15.231 (72.870)	Grad: 14.411
[Epoch   3 (35.84s)]	ELBO: -5445.090 (-5266.686)	Log prob: -5373.036 (-5193.839)	KLD: 15.852 (72.848)	Grad: 17.432
[Epoch   4 (28.18s)]	ELBO: -5310.627 (-5123.004)	Log prob: -5238.175 (-5049.991)	KLD: 16.380 (73.013)	Grad: 19.945
[Epoch   5 (33.34s)]	ELBO: -5109.659 (-4853.515)	Log prob: -5037.538 (-4782.400)	KLD: 16.549 (71.115)	Grad: 18.536
[Epoch   6 (61.55s)]	ELBO: -4953.361 (-4939.077)	Log prob: -4880.549 (-4867.968)	KLD: 16.796 (71.109)	Grad: 21.057
[Epoch   7 (61.29s)]	ELBO: -4800.662 (-4792.230)	Log prob: -4728.026 (-4720.099)	KLD: 16.782 (72.131)	Grad: 15.764
[Epoch   8 (64.33s)]	ELBO: -4805.203 (-4646.247)	Log prob: -4731.009 (-4571.440)	KLD: 16.955 (74.808)	Grad: 23.705
[Epoch   9 (64.96s)]	ELBO: -4584.090 (-4578.624)	Log prob: -4511.292 (-4505.970)	KLD: 16.817 (72.654)	Grad: 15.738
[Epoch  10 (64.15s)]	ELBO: -4665.473 (-4528.496)	Log prob: -4593.845 (-4458.609)	KLD: 16.727 (69.887)	Grad: 16.739
Training model 5/16...
[Epoch   1 (83.88s)]	ELBO: -4833.955 (-4404.411)	Log prob: -4740.204 (-4317.910)	KLD: 21.121 (86.501)	Grad: 16.226
[Epoch   2 (83.57s)]	ELBO: -4430.297 (-4182.702)	Log prob: -4342.701 (-4095.208)	KLD: 15.631 (87.494)	Grad: 5.432
[Epoch   3 (81.11s)]	ELBO: -4188.329 (-4633.582)	Log prob: -4101.094 (-4543.853)	KLD: 15.871 (89.729)	Grad: 7.198
[Epoch   4 (78.96s)]	ELBO: -4082.031 (-3865.177)	Log prob: -3994.170 (-3777.538)	KLD: 16.582 (87.639)	Grad: 8.983
[Epoch   5 (77.50s)]	ELBO: -3970.208 (-3908.395)	Log prob: -3882.576 (-3819.627)	KLD: 15.935 (88.767)	Grad: 8.305
[Epoch   6 (77.29s)]	ELBO: -3948.838 (-4013.484)	Log prob: -3861.478 (-3928.515)	KLD: 15.774 (84.969)	Grad: 6.443
[Epoch   7 (79.66s)]	ELBO: -3999.487 (-3843.392)	Log prob: -3911.957 (-3756.447)	KLD: 16.101 (86.945)	Grad: 7.524
[Epoch   8 (78.51s)]	ELBO: -3880.509 (-3772.459)	Log prob: -3794.014 (-3689.349)	KLD: 16.031 (83.109)	Grad: 6.744
[Epoch   9 (76.75s)]	ELBO: -3848.618 (-3860.492)	Log prob: -3761.179 (-3770.679)	KLD: 16.185 (89.813)	Grad: 9.583
[Epoch  10 (76.04s)]	ELBO: -3713.375 (-3821.419)	Log prob: -3625.432 (-3733.087)	KLD: 16.023 (88.332)	Grad: 8.842
Training model 6/16...
[Epoch   1 (92.45s)]	ELBO: -3946.368 (-3581.234)	Log prob: -3841.343 (-3478.257)	KLD: 17.467 (102.977)	Grad: 51.074
[Epoch   2 (96.64s)]	ELBO: -3690.949 (-3542.609)	Log prob: -3589.433 (-3442.669)	KLD: 14.053 (99.939)	Grad: 24.158
[Epoch   3 (95.43s)]	ELBO: -3561.273 (-3431.637)	Log prob: -3459.396 (-3330.489)	KLD: 13.427 (101.148)	Grad: 10.496
[Epoch   4 (93.99s)]	ELBO: -3437.688 (-3337.810)	Log prob: -3335.394 (-3235.701)	KLD: 13.802 (102.110)	Grad: 10.387
[Epoch   5 (93.34s)]	ELBO: -3506.338 (-3331.558)	Log prob: -3405.090 (-3229.622)	KLD: 12.688 (101.936)	Grad: 15.165
[Epoch   6 (93.88s)]	ELBO: -3431.274 (-3454.247)	Log prob: -3330.264 (-3351.119)	KLD: 12.938 (103.128)	Grad: 10.079
[Epoch   7 (96.80s)]	ELBO: -3404.119 (-3366.189)	Log prob: -3300.996 (-3257.489)	KLD: 15.008 (108.700)	Grad: 12.526
[Epoch   8 (94.04s)]	ELBO: -3313.051 (-3250.056)	Log prob: -3209.714 (-3146.615)	KLD: 14.630 (103.442)	Grad: 8.618
[Epoch   9 (93.08s)]	ELBO: -3253.229 (-3214.421)	Log prob: -3150.373 (-3114.635)	KLD: 14.517 (99.786)	Grad: 7.022
[Epoch  10 (94.31s)]	ELBO: -3195.315 (-3164.336)	Log prob: -3092.901 (-3060.958)	KLD: 14.639 (103.378)	Grad: 7.871
Training model 7/16...
[Epoch   1 (113.73s)]	ELBO: -3735.039 (-3293.320)	Log prob: -3613.909 (-3171.547)	KLD: 16.617 (121.773)	Grad: 35.063
[Epoch   2 (113.93s)]	ELBO: -3330.862 (-3191.411)	Log prob: -3213.438 (-3074.974)	KLD: 13.966 (116.437)	Grad: 9.972
[Epoch   3 (116.33s)]	ELBO: -3243.811 (-4269.238)	Log prob: -3125.403 (-4148.292)	KLD: 14.328 (120.947)	Grad: 6.018
[Epoch   4 (111.79s)]	ELBO: -3173.823 (-3071.386)	Log prob: -3054.173 (-2956.561)	KLD: 14.645 (114.825)	Grad: 2.886
[Epoch   5 (112.21s)]	ELBO: -3076.322 (-3077.217)	Log prob: -2957.912 (-2956.385)	KLD: 14.284 (120.831)	Grad: 2.544
[Epoch   6 (112.75s)]	ELBO: -3011.799 (-2920.958)	Log prob: -2893.654 (-2802.075)	KLD: 14.358 (118.883)	Grad: 2.520
[Epoch   7 (115.13s)]	ELBO: -2864.090 (-2808.120)	Log prob: -2746.056 (-2690.789)	KLD: 14.521 (117.331)	Grad: 2.789
[Epoch   8 (114.25s)]	ELBO: -2752.613 (-2681.267)	Log prob: -2633.922 (-2561.718)	KLD: 15.030 (119.549)	Grad: 2.760
[Epoch   9 (113.04s)]	ELBO: -2692.769 (-2632.840)	Log prob: -2573.591 (-2514.421)	KLD: 15.309 (118.419)	Grad: 2.977
[Epoch  10 (111.82s)]	ELBO: -2639.463 (-2580.245)	Log prob: -2520.296 (-2460.810)	KLD: 15.262 (119.435)	Grad: 2.826
Training model 8/16...
[Epoch   1 (137.22s)]	ELBO: -2687.659 (-2557.621)	Log prob: -2552.186 (-2425.116)	KLD: 16.562 (132.505)	Grad: 7.448
[Epoch   2 (131.78s)]	ELBO: -2527.566 (-2440.190)	Log prob: -2393.513 (-2306.403)	KLD: 15.069 (133.787)	Grad: 3.547
[Epoch   3 (131.86s)]	ELBO: -2400.374 (-2316.715)	Log prob: -2265.561 (-2182.296)	KLD: 15.705 (134.419)	Grad: 3.557
[Epoch   4 (136.68s)]	ELBO: -2289.820 (-2214.058)	Log prob: -2154.591 (-2082.231)	KLD: 15.904 (131.827)	Grad: 2.873
[Epoch   5 (132.84s)]	ELBO: -2214.790 (-2131.485)	Log prob: -2079.195 (-1995.933)	KLD: 16.438 (135.552)	Grad: 2.703
[Epoch   6 (133.09s)]	ELBO: -2140.760 (-2051.049)	Log prob: -2004.917 (-1913.350)	KLD: 16.556 (137.699)	Grad: 3.127
[Epoch   7 (134.83s)]	ELBO: -2057.228 (-2007.099)	Log prob: -1921.865 (-1874.053)	KLD: 16.469 (133.046)	Grad: 2.236
[Epoch   8 (133.55s)]	ELBO: -1990.046 (-1906.458)	Log prob: -1854.589 (-1770.634)	KLD: 16.752 (135.824)	Grad: 2.177
[Epoch   9 (134.85s)]	ELBO: -1942.054 (-1883.690)	Log prob: -1806.523 (-1747.402)	KLD: 16.792 (136.288)	Grad: 2.242
[Epoch  10 (132.35s)]	ELBO: -1910.998 (-1894.025)	Log prob: -1775.454 (-1757.201)	KLD: 16.861 (136.824)	Grad: 2.186
Training model 9/16...
[Epoch   1 (152.32s)]	ELBO: -2024.887 (-1839.559)	Log prob: -1875.436 (-1688.400)	KLD: 14.823 (151.159)	Grad: 12.196
[Epoch   2 (156.51s)]	ELBO: -1801.721 (-1708.253)	Log prob: -1652.461 (-1558.735)	KLD: 14.881 (149.518)	Grad: 5.264
[Epoch   3 (154.67s)]	ELBO: -1709.370 (-1676.479)	Log prob: -1559.518 (-1526.761)	KLD: 15.360 (149.718)	Grad: 4.584
[Epoch   4 (152.74s)]	ELBO: -1655.389 (-1602.422)	Log prob: -1505.341 (-1454.559)	KLD: 15.705 (147.863)	Grad: 4.628
[Epoch   5 (155.57s)]	ELBO: -1630.543 (-1557.981)	Log prob: -1479.477 (-1406.998)	KLD: 15.755 (150.983)	Grad: 3.696
[Epoch   6 (155.34s)]	ELBO: -1594.697 (-1559.826)	Log prob: -1444.559 (-1410.248)	KLD: 16.056 (149.577)	Grad: 2.695
[Epoch   7 (154.56s)]	ELBO: -1587.358 (-1562.151)	Log prob: -1437.463 (-1411.104)	KLD: 16.141 (151.048)	Grad: 2.664
[Epoch   8 (153.13s)]	ELBO: -1588.693 (-1546.124)	Log prob: -1439.130 (-1398.238)	KLD: 16.151 (147.886)	Grad: 2.093
[Epoch   9 (156.85s)]	ELBO: -1572.487 (-1535.670)	Log prob: -1423.035 (-1389.481)	KLD: 16.214 (146.189)	Grad: 1.905
[Epoch  10 (157.47s)]	ELBO: -1577.293 (-1512.067)	Log prob: -1427.956 (-1366.940)	KLD: 16.193 (145.128)	Grad: 2.048
Training model 10/16...
[Epoch   1 (164.69s)]	ELBO: -1753.595 (-1499.071)	Log prob: -1583.896 (-1333.687)	KLD: 19.765 (165.384)	Grad: 22.957
[Epoch   2 (143.02s)]	ELBO: -1497.203 (-1457.995)	Log prob: -1333.205 (-1292.726)	KLD: 15.544 (165.268)	Grad: 3.604
[Epoch   3 (146.81s)]	ELBO: -1461.358 (-1446.696)	Log prob: -1298.193 (-1282.195)	KLD: 15.090 (164.501)	Grad: 2.822
[Epoch   4 (145.49s)]	ELBO: -1449.735 (-1476.974)	Log prob: -1286.394 (-1312.560)	KLD: 15.007 (164.414)	Grad: 2.613
[Epoch   5 (144.54s)]	ELBO: -1417.974 (-1392.460)	Log prob: -1255.329 (-1231.391)	KLD: 14.887 (161.069)	Grad: 2.407
[Epoch   6 (148.31s)]	ELBO: -1396.307 (-1355.964)	Log prob: -1233.969 (-1196.834)	KLD: 14.876 (159.130)	Grad: 2.273
[Epoch   7 (146.17s)]	ELBO: -1374.795 (-1315.719)	Log prob: -1212.895 (-1155.453)	KLD: 14.949 (160.266)	Grad: 2.303
[Epoch   8 (144.69s)]	ELBO: -1335.082 (-1288.804)	Log prob: -1173.458 (-1124.928)	KLD: 15.027 (163.875)	Grad: 2.220
[Epoch   9 (148.52s)]	ELBO: -1300.783 (-1285.540)	Log prob: -1139.442 (-1122.758)	KLD: 15.134 (162.782)	Grad: 2.319
[Epoch  10 (143.62s)]	ELBO: -1270.901 (-1252.221)	Log prob: -1109.782 (-1092.187)	KLD: 15.283 (160.034)	Grad: 2.362
Training model 11/16...
[Epoch   1 (164.73s)]	ELBO: -1402.093 (-1275.749)	Log prob: -1225.349 (-1097.847)	KLD: 15.824 (177.902)	Grad: 10.618
[Epoch   2 (166.62s)]	ELBO: -1210.794 (-1184.413)	Log prob: -1034.484 (-1008.262)	KLD: 15.177 (176.152)	Grad: 3.308
[Epoch   3 (165.70s)]	ELBO: -1159.339 (-1104.130)	Log prob: -982.520 (-928.927)	KLD: 15.324 (175.203)	Grad: 2.759
[Epoch   4 (163.77s)]	ELBO: -1110.989 (-1528.933)	Log prob: -934.642 (-1351.533)	KLD: 15.085 (177.400)	Grad: 3.248
[Epoch   5 (139.08s)]	ELBO: -1072.243 (-973.050)	Log prob: -895.367 (-795.645)	KLD: 15.663 (177.406)	Grad: 3.264
[Epoch   6 (116.17s)]	ELBO: -981.004 (-962.504)	Log prob: -804.259 (-785.588)	KLD: 15.617 (176.916)	Grad: 2.359
[Epoch   7 (116.49s)]	ELBO: -963.334 (-946.721)	Log prob: -786.686 (-769.943)	KLD: 15.711 (176.777)	Grad: 2.237
[Epoch   8 (116.27s)]	ELBO: -936.579 (-920.571)	Log prob: -759.978 (-743.488)	KLD: 15.649 (177.083)	Grad: 1.797
[Epoch   9 (116.60s)]	ELBO: -925.287 (-972.722)	Log prob: -748.877 (-798.173)	KLD: 15.670 (174.549)	Grad: 1.578
[Epoch  10 (116.62s)]	ELBO: -907.750 (-890.303)	Log prob: -731.720 (-714.948)	KLD: 15.693 (175.354)	Grad: 1.287
Training model 12/16...
[Epoch   1 (132.97s)]	ELBO: -1105.777 (-987.638)	Log prob: -912.355 (-793.532)	KLD: 16.615 (194.106)	Grad: 11.008
[Epoch   2 (131.00s)]	ELBO: -884.861 (-861.335)	Log prob: -693.741 (-670.899)	KLD: 15.745 (190.436)	Grad: 3.778
[Epoch   3 (130.48s)]	ELBO: -836.119 (-833.821)	Log prob: -645.287 (-645.913)	KLD: 15.647 (187.907)	Grad: 3.024
[Epoch   4 (130.68s)]	ELBO: -807.856 (-801.784)	Log prob: -617.604 (-612.971)	KLD: 15.490 (188.814)	Grad: 2.804
[Epoch   5 (132.25s)]	ELBO: -782.716 (-777.671)	Log prob: -592.934 (-585.607)	KLD: 15.523 (192.065)	Grad: 2.382
[Epoch   6 (131.08s)]	ELBO: -751.088 (-772.416)	Log prob: -561.371 (-581.219)	KLD: 15.631 (191.198)	Grad: 2.196
[Epoch   7 (131.11s)]	ELBO: -734.741 (-717.076)	Log prob: -545.243 (-528.677)	KLD: 15.592 (188.399)	Grad: 1.976
[Epoch   8 (132.11s)]	ELBO: -714.105 (-715.769)	Log prob: -524.967 (-526.042)	KLD: 15.607 (189.727)	Grad: 1.940
[Epoch   9 (131.42s)]	ELBO: -703.303 (-710.101)	Log prob: -514.595 (-520.619)	KLD: 15.546 (189.481)	Grad: 1.851
[Epoch  10 (131.93s)]	ELBO: -693.069 (-692.475)	Log prob: -504.877 (-502.577)	KLD: 15.546 (189.898)	Grad: 1.710
Training model 13/16...
[Epoch   1 (146.90s)]	ELBO: -906.844 (-736.651)	Log prob: -703.527 (-529.806)	KLD: 15.725 (206.845)	Grad: 6.933
[Epoch   2 (147.88s)]	ELBO: -710.216 (-700.224)	Log prob: -507.490 (-498.700)	KLD: 15.647 (201.524)	Grad: 2.667
[Epoch   3 (146.30s)]	ELBO: -654.221 (-651.148)	Log prob: -452.111 (-450.548)	KLD: 15.536 (200.601)	Grad: 1.946
[Epoch   4 (146.40s)]	ELBO: -614.948 (-591.438)	Log prob: -413.350 (-390.855)	KLD: 15.427 (200.583)	Grad: 1.759
[Epoch   5 (146.35s)]	ELBO: -579.723 (-574.461)	Log prob: -378.454 (-374.771)	KLD: 15.472 (199.690)	Grad: 1.558
[Epoch   6 (145.86s)]	ELBO: -564.661 (-551.990)	Log prob: -363.631 (-353.072)	KLD: 15.436 (198.918)	Grad: 1.585
[Epoch   7 (145.89s)]	ELBO: -534.492 (-553.534)	Log prob: -333.877 (-353.464)	KLD: 15.428 (200.069)	Grad: 1.439
[Epoch   8 (145.80s)]	ELBO: -524.690 (-499.834)	Log prob: -324.747 (-302.268)	KLD: 15.445 (197.567)	Grad: 1.363
[Epoch   9 (147.35s)]	ELBO: -505.996 (-511.346)	Log prob: -306.541 (-307.956)	KLD: 15.350 (203.390)	Grad: 1.248
[Epoch  10 (145.66s)]	ELBO: -491.639 (-486.487)	Log prob: -292.658 (-285.492)	KLD: 15.375 (200.995)	Grad: 1.209
Training model 14/16...
[Epoch   1 (161.65s)]	ELBO: -688.945 (-525.412)	Log prob: -473.907 (-308.521)	KLD: 16.804 (216.891)	Grad: 12.414
[Epoch   2 (161.33s)]	ELBO: -492.948 (-445.820)	Log prob: -278.727 (-229.503)	KLD: 16.188 (216.318)	Grad: 3.096
[Epoch   3 (160.33s)]	ELBO: -460.595 (-445.912)	Log prob: -246.959 (-230.584)	KLD: 16.016 (215.328)	Grad: 2.540
[Epoch   4 (161.55s)]	ELBO: -431.258 (-432.551)	Log prob: -218.165 (-216.997)	KLD: 15.843 (215.553)	Grad: 2.475
[Epoch   5 (160.72s)]	ELBO: -412.060 (-406.643)	Log prob: -199.568 (-190.807)	KLD: 15.695 (215.835)	Grad: 2.262
[Epoch   6 (162.31s)]	ELBO: -381.863 (-378.938)	Log prob: -169.991 (-169.647)	KLD: 15.632 (209.291)	Grad: 1.952
[Epoch   7 (162.14s)]	ELBO: -352.936 (-347.411)	Log prob: -141.602 (-133.444)	KLD: 15.570 (213.967)	Grad: 1.845
[Epoch   8 (161.02s)]	ELBO: -328.909 (-340.659)	Log prob: -117.995 (-130.362)	KLD: 15.487 (210.297)	Grad: 1.907
[Epoch   9 (160.29s)]	ELBO: -298.181 (-314.822)	Log prob: -87.551 (-102.705)	KLD: 15.558 (212.117)	Grad: 1.859
[Epoch  10 (159.87s)]	ELBO: -267.630 (-318.376)	Log prob: -57.304 (-109.584)	KLD: 15.628 (208.791)	Grad: 1.787
Training model 15/16...
[Epoch   1 (176.03s)]	ELBO: -428.866 (-313.771)	Log prob: -202.251 (-85.237)	KLD: 16.845 (228.534)	Grad: 8.240
[Epoch   2 (176.50s)]	ELBO: -267.969 (-244.343)	Log prob: -41.958 (-18.632)	KLD: 16.545 (225.711)	Grad: 2.868
[Epoch   3 (177.47s)]	ELBO: -238.129 (-237.902)	Log prob: -12.763 (-12.938)	KLD: 16.280 (224.964)	Grad: 2.494
[Epoch   4 (176.72s)]	ELBO: -203.375 (-197.213)	Log prob: 21.166 (26.480)	KLD: 15.990 (223.693)	Grad: 2.159
[Epoch   5 (176.85s)]	ELBO: -172.035 (-152.943)	Log prob: 52.092 (72.564)	KLD: 15.931 (225.507)	Grad: 1.569
[Epoch   6 (177.64s)]	ELBO: -151.761 (-188.786)	Log prob: 72.142 (35.605)	KLD: 15.973 (224.391)	Grad: 1.474
[Epoch   7 (175.72s)]	ELBO: -140.454 (-208.570)	Log prob: 82.912 (13.417)	KLD: 15.864 (221.987)	Grad: 1.471
[Epoch   8 (176.47s)]	ELBO: -133.342 (-131.197)	Log prob: 89.579 (91.385)	KLD: 15.887 (222.582)	Grad: 1.412
[Epoch   9 (175.89s)]	ELBO: -97.125 (-118.239)	Log prob: 125.471 (104.019)	KLD: 15.967 (222.259)	Grad: 1.360
[Epoch  10 (177.14s)]	ELBO: -76.078 (-78.322)	Log prob: 146.021 (146.992)	KLD: 15.942 (225.313)	Grad: 1.280
Training model 16/16...
[Epoch   1 (192.09s)]	ELBO: -245.412 (-135.868)	Log prob: -7.374 (103.185)	KLD: 16.475 (239.053)	Grad: 9.452
[Epoch   2 (193.14s)]	ELBO: -92.845 (-86.109)	Log prob: 144.212 (150.236)	KLD: 15.711 (236.345)	Grad: 2.807
[Epoch   3 (192.01s)]	ELBO: -56.520 (-83.327)	Log prob: 179.533 (154.758)	KLD: 15.246 (238.085)	Grad: 2.043
[Epoch   4 (191.48s)]	ELBO: -42.137 (-85.741)	Log prob: 192.957 (149.819)	KLD: 14.709 (235.560)	Grad: 1.766
[Epoch   5 (194.02s)]	ELBO: -31.645 (-51.146)	Log prob: 202.761 (184.501)	KLD: 14.467 (235.647)	Grad: 1.396
[Epoch   6 (191.87s)]	ELBO: -13.113 (-19.859)	Log prob: 219.997 (210.102)	KLD: 14.102 (229.961)	Grad: 1.286
[Epoch   7 (191.57s)]	ELBO: -21.728 (-94.606)	Log prob: 211.290 (140.317)	KLD: 14.066 (234.924)	Grad: 1.389
[Epoch   8 (194.10s)]	ELBO: 19.679 (30.446)	Log prob: 251.979 (261.111)	KLD: 13.960 (230.665)	Grad: 1.190
[Epoch   9 (194.78s)]	ELBO: 42.889 (36.728)	Log prob: 274.472 (267.093)	KLD: 14.071 (230.365)	Grad: 1.142
[Epoch  10 (193.80s)]	ELBO: 61.970 (43.629)	Log prob: 293.007 (276.851)	KLD: 14.082 (233.222)	Grad: 1.120
Best epoch(s): [8, 9, 9, 10, 8, 10, 10, 9, 10, 10, 10, 10, 10, 9, 10, 10]	Training time(s): 118.34s, 183.73s, 273.50s, 485.93s, 793.26s, 943.96s, 1134.97s, 1339.06s, 1549.17s, 1475.84s, 1382.04s, 1315.04s, 1464.38s, 1611.22s, 1766.44s, 1928.85s (17765.73s)	Best ELBO: 61.970 (43.629)	Best log prob: 293.007 (276.851)
Avg. mu: 0.840, 0.201, 0.280, 0.172, 0.085, -0.116, 0.109, 0.121, 0.153, -0.033, 0.070, 0.133, 0.104, -0.092, 0.105, 0.099, 0.069, -0.092, 0.093, 0.119, 0.081, -0.021, 0.094, 0.096, 0.061, -0.031, 0.054, 0.125, 0.145, -0.018, 0.051, 0.052, 0.088, -0.029, 0.061, 0.091, 0.071, -0.079, 0.030, 0.116, 0.077, -0.023, 0.068, 0.112, 0.097, -0.159, 0.176, 0.116, 0.110, -0.038, 0.085, 0.128, 0.118, -0.099, 0.103, 0.109, 0.123, -0.081, 0.084, 0.121, 0.101, 0.014, 0.066, 0.112
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.000, 0.000, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Max. mu: 6.485, 5.009, 4.584, 4.919, 3.479, 2.990, 2.842, 2.968, 3.118, 3.118, 2.940, 2.978, 3.530, 2.315, 3.187, 2.006, 2.727, 2.105, 2.462, 2.076, 2.437, 2.645, 2.622, 2.279, 2.765, 2.009, 2.325, 1.831, 2.984, 3.601, 2.087, 2.100, 2.094, 1.990, 1.879, 1.608, 2.141, 1.568, 1.406, 1.390, 2.335, 1.710, 1.595, 1.242, 1.703, 0.676, 1.027, 0.924, 0.895, 0.923, 0.798, 0.693, 0.442, 0.216, 0.434, 0.370, 0.451, 0.233, 0.523, 0.354, 0.307, 0.325, 0.219, 0.299
Max. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.002, 0.001, 0.003, 0.005, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.004, 0.004, 0.003, 0.003, 0.005, 0.006, 0.003, 0.003, 0.002, 0.002, 0.002, 0.001, 0.003, 0.004, 0.002, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000
Min. mu: -3.020, -2.924, -4.554, -4.662, -3.402, -2.778, -3.921, -2.797, -3.044, -3.299, -3.288, -2.437, -2.576, -2.407, -2.668, -1.570, -2.684, -2.696, -3.199, -1.773, -2.442, -3.367, -4.216, -2.069, -2.288, -1.930, -2.156, -2.005, -2.095, -2.409, -2.849, -1.702, -2.011, -2.427, -1.787, -1.102, -1.766, -1.495, -1.492, -1.373, -1.264, -1.345, -1.377, -0.874, -1.111, -1.017, -0.566, -0.782, -0.890, -0.717, -0.583, -0.345, -0.306, -0.469, -0.202, -0.218, -0.143, -0.331, -0.206, -0.125, -0.114, -0.226, -0.085, -0.144
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.795 -0.081 -0.291 0.016 0.009 -0.000 0.005 0.010 -0.036 -0.019 0.000
  -0.007 -0.006 0.000 -0.006 -0.002 0.007 0.003 -0.004 -0.003 -0.012
  0.009 0.004 0.009 -0.003 0.003 -0.011 0.015 -0.006 0.002 -0.003 -0.007
  0.015 0.005 -0.007 -0.001 0.007 -0.003 -0.001 0.003 0.002 0.010 -0.018
  -0.006 -0.011 -0.011 -0.019 0.007 -0.006 -0.002 0.017 -0.014 -0.003
  0.003 0.001 0.002 -0.000 0.003 0.001 0.001 0.001 0.001 -0.000 0.004]
 [-0.081 1.070 -0.058 -0.202 0.004 0.002 -0.015 0.015 0.005 -0.011 0.023
  0.007 0.009 0.004 -0.016 -0.000 -0.002 0.010 -0.011 -0.005 0.006 0.006
  0.011 0.007 -0.013 0.013 0.001 0.005 0.007 -0.003 -0.006 -0.006 -0.013
  0.007 -0.001 -0.010 -0.002 0.008 0.003 0.005 -0.001 -0.008 0.008 0.003
  0.002 -0.008 0.021 -0.005 -0.004 -0.019 -0.011 0.008 -0.003 -0.007
  -0.002 -0.000 0.005 -0.001 -0.001 -0.000 0.002 0.002 0.002 0.002]
 [-0.291 -0.058 0.901 0.181 -0.002 0.014 0.012 -0.011 -0.013 -0.018
  -0.002 -0.011 -0.008 -0.002 -0.002 0.000 -0.001 -0.001 0.006 -0.007
  0.001 -0.008 0.017 0.000 0.009 -0.000 0.007 0.002 0.013 0.007 -0.004
  0.012 0.004 -0.003 0.002 -0.002 -0.009 -0.001 0.008 0.008 -0.012 -0.013
  0.015 0.008 -0.001 0.019 0.032 -0.001 0.011 0.017 -0.023 -0.001 0.001
  0.000 0.006 -0.003 -0.002 -0.003 -0.002 -0.003 0.000 -0.005 0.001
  -0.000]
 [0.016 -0.202 0.181 1.602 0.032 0.018 -0.019 0.009 -0.032 -0.049 -0.013
  -0.012 0.014 0.004 0.013 -0.000 0.000 -0.005 -0.008 -0.005 -0.015
  -0.019 0.001 0.011 0.031 -0.012 -0.016 0.001 -0.003 -0.005 0.014 -0.012
  -0.006 -0.000 0.006 0.007 -0.018 -0.010 -0.006 0.010 -0.010 -0.005
  0.006 0.003 -0.040 -0.049 0.013 -0.001 -0.005 -0.012 0.001 0.014 0.006
  -0.003 0.003 -0.002 0.006 -0.002 0.006 -0.007 0.004 0.024 -0.005 0.000]
 [0.009 0.004 -0.002 0.032 0.437 0.025 -0.017 0.017 0.003 0.004 -0.010
  0.008 -0.011 -0.002 0.012 -0.009 0.002 0.001 -0.001 0.000 -0.016 0.001
  0.001 -0.004 -0.004 0.001 -0.009 -0.004 0.010 0.004 -0.006 0.000 -0.004
  0.006 0.006 0.006 0.005 0.001 -0.001 0.003 0.001 -0.002 -0.004 0.002
  0.007 -0.004 -0.002 -0.003 0.004 0.002 0.000 -0.002 -0.008 0.003 0.007
  0.001 -0.006 -0.001 -0.001 -0.004 0.002 0.003 -0.001 -0.002]
 [-0.000 0.002 0.014 0.018 0.025 0.416 -0.007 -0.016 -0.008 -0.001 0.001
  0.001 0.009 0.005 0.001 -0.012 -0.003 -0.002 0.002 -0.007 -0.009 -0.002
  0.004 0.000 0.002 -0.002 0.000 -0.000 -0.007 -0.005 -0.001 -0.003
  -0.005 -0.003 0.001 0.003 0.004 -0.000 -0.003 -0.003 -0.004 -0.001
  0.001 -0.001 0.008 -0.001 0.004 -0.000 -0.003 0.008 0.005 0.003 -0.006
  -0.002 0.005 -0.001 0.000 0.000 0.001 -0.000 0.001 0.001 0.000 -0.002]
 [0.005 -0.015 0.012 -0.019 -0.017 -0.007 0.459 -0.002 0.000 0.007 0.012
  0.010 -0.003 0.002 0.004 0.003 -0.001 -0.009 0.011 -0.007 0.003 0.010
  0.012 -0.000 0.004 0.002 0.000 -0.001 0.005 -0.005 -0.009 0.000 0.002
  -0.005 0.008 0.001 0.004 -0.003 0.001 -0.000 0.004 0.001 -0.000 -0.000
  -0.011 -0.001 -0.006 0.004 -0.004 -0.003 -0.003 0.001 0.003 0.003
  -0.003 0.001 0.000 -0.001 -0.004 -0.000 -0.001 -0.002 -0.001 0.002]
 [0.010 0.015 -0.011 0.009 0.017 -0.016 -0.002 0.318 0.011 -0.003 0.004
  0.002 0.004 -0.006 0.002 0.005 0.003 0.002 -0.002 0.004 0.003 0.001
  -0.000 -0.001 0.002 -0.000 -0.004 0.003 -0.004 0.004 -0.002 0.004
  -0.003 -0.002 -0.005 -0.004 0.001 0.001 -0.003 0.001 0.000 0.001 -0.001
  0.001 -0.003 0.001 0.001 0.001 0.002 -0.006 -0.005 0.001 0.004 0.004
  -0.004 -0.001 0.001 -0.001 0.002 -0.000 -0.001 -0.000 0.001 0.000]
 [-0.036 0.005 -0.013 -0.032 0.003 -0.008 0.000 0.011 0.435 0.036 -0.002
  0.024 0.004 0.002 -0.001 -0.007 -0.000 -0.006 -0.005 0.003 0.006 -0.004
  -0.009 0.008 0.008 0.002 0.001 0.006 0.002 0.003 0.002 -0.000 0.000
  -0.000 -0.012 0.002 -0.005 0.000 0.004 -0.001 0.000 -0.004 -0.003
  -0.002 0.004 -0.012 -0.008 -0.001 -0.007 0.001 -0.010 0.003 -0.002
  -0.002 -0.003 0.000 0.002 -0.001 -0.006 0.002 -0.002 -0.004 -0.000
  0.002]
 [-0.019 -0.011 -0.018 -0.049 0.004 -0.001 0.007 -0.003 0.036 0.327
  -0.013 0.019 0.001 -0.005 0.000 0.003 0.003 -0.001 -0.008 -0.000 0.004
  0.002 0.002 0.004 0.003 0.001 -0.002 -0.002 -0.017 -0.003 -0.002 0.002
  0.002 0.011 -0.001 0.002 0.000 0.004 -0.002 -0.004 -0.002 0.004 0.005
  0.001 0.003 -0.006 -0.009 0.003 0.001 -0.003 -0.002 -0.005 0.000 -0.002
  0.002 0.001 -0.002 -0.003 0.000 0.002 -0.001 -0.002 0.002 0.001]
 [0.000 0.023 -0.002 -0.013 -0.010 0.001 0.012 0.004 -0.002 -0.013 0.366
  -0.021 0.005 0.004 0.013 0.004 -0.005 -0.006 0.007 0.002 0.002 0.002
  -0.006 -0.000 -0.008 0.001 0.007 0.001 -0.003 -0.004 -0.002 0.001
  -0.004 -0.002 0.006 0.001 -0.003 -0.002 -0.003 0.002 0.001 0.002 0.005
  0.000 0.001 -0.004 0.001 -0.001 0.004 0.001 -0.001 -0.004 -0.003 -0.002
  0.001 -0.002 0.001 -0.002 -0.001 0.003 0.003 0.000 -0.001 -0.002]
 [-0.007 0.007 -0.011 -0.012 0.008 0.001 0.010 0.002 0.024 0.019 -0.021
  0.241 -0.002 -0.005 0.003 -0.007 0.000 0.002 -0.003 0.001 0.001 -0.003
  0.006 -0.004 -0.006 -0.002 -0.002 0.003 0.004 0.000 -0.000 -0.002 0.006
  0.003 0.006 -0.001 0.000 0.002 -0.002 -0.002 -0.004 -0.001 -0.000 0.000
  0.001 -0.003 -0.004 0.001 0.001 0.001 -0.004 -0.004 0.000 -0.003 -0.002
  0.001 0.001 0.001 -0.001 0.001 -0.001 -0.001 0.000 0.000]
 [-0.006 0.009 -0.008 0.014 -0.011 0.009 -0.003 0.004 0.004 0.001 0.005
  -0.002 0.281 0.004 0.003 -0.007 0.017 0.002 -0.006 0.001 -0.002 -0.009
  0.005 0.004 0.002 0.005 -0.008 -0.002 -0.001 0.001 -0.001 -0.003 0.000
  -0.004 -0.002 0.001 0.004 0.001 0.001 0.000 -0.003 -0.001 0.004 -0.000
  -0.000 -0.006 -0.007 0.001 0.001 0.000 -0.004 0.002 -0.003 -0.001 0.000
  0.000 0.001 0.001 -0.000 0.003 -0.002 -0.001 0.001 0.000]
 [0.000 0.004 -0.002 0.004 -0.002 0.005 0.002 -0.006 0.002 -0.005 0.004
  -0.005 0.004 0.235 -0.006 0.004 0.003 0.002 -0.007 0.002 0.004 -0.005
  -0.006 -0.004 0.002 0.000 0.002 -0.002 0.003 0.005 -0.000 0.001 -0.004
  0.004 -0.003 -0.001 -0.000 -0.001 -0.001 -0.003 0.001 0.001 0.002
  -0.002 0.006 -0.001 0.001 0.000 0.002 -0.002 -0.001 -0.002 -0.002
  -0.001 -0.000 0.000 0.000 0.001 -0.001 0.001 -0.001 -0.001 0.001 0.000]
 [-0.006 -0.016 -0.002 0.013 0.012 0.001 0.004 0.002 -0.001 0.000 0.013
  0.003 0.003 -0.006 0.305 -0.004 0.002 0.005 0.016 -0.002 -0.002 0.003
  0.000 -0.006 -0.001 -0.000 -0.006 -0.004 0.001 -0.000 -0.001 0.002
  -0.006 0.004 0.001 0.004 -0.003 -0.001 -0.004 -0.000 -0.001 -0.003
  -0.001 -0.000 -0.005 -0.000 0.000 0.003 0.001 0.001 0.003 0.003 -0.001
  0.003 0.001 0.001 0.000 0.001 -0.001 0.002 0.001 -0.001 -0.000 0.000]
 [-0.002 -0.000 0.000 -0.000 -0.009 -0.012 0.003 0.005 -0.007 0.003 0.004
  -0.007 -0.007 0.004 -0.004 0.190 0.001 -0.003 0.003 0.001 -0.000 0.009
  0.002 -0.001 -0.009 -0.003 -0.003 0.002 0.002 0.002 -0.002 -0.000 0.000
  -0.003 -0.001 -0.001 -0.001 -0.002 -0.000 0.001 -0.002 -0.003 -0.000
  0.001 -0.005 0.004 0.004 0.001 0.004 0.000 0.000 -0.001 0.000 -0.001
  -0.001 0.000 0.002 -0.000 0.001 -0.001 0.001 0.001 0.000 -0.000]
 [0.007 -0.002 -0.001 0.000 0.002 -0.003 -0.001 0.003 -0.000 0.003 -0.005
  0.000 0.017 0.003 0.002 0.001 0.238 -0.002 -0.012 -0.004 0.007 0.013
  0.007 -0.003 0.004 -0.002 -0.007 0.000 0.003 -0.003 -0.004 0.000 0.003
  -0.004 -0.006 -0.001 0.002 -0.003 -0.004 0.001 0.001 -0.001 -0.001
  -0.001 -0.004 0.000 -0.000 -0.003 0.000 -0.003 0.003 -0.000 -0.003
  -0.000 0.002 -0.000 0.002 0.001 0.001 -0.000 -0.000 -0.001 0.001 0.001]
 [0.003 0.010 -0.001 -0.005 0.001 -0.002 -0.009 0.002 -0.006 -0.001
  -0.006 0.002 0.002 0.002 0.005 -0.003 -0.002 0.201 -0.014 0.006 -0.003
  0.002 0.001 -0.004 -0.001 0.002 0.001 -0.002 0.003 -0.001 -0.001 -0.000
  0.005 0.004 0.000 -0.003 -0.001 -0.001 -0.000 0.001 0.001 0.002 0.000
  -0.001 0.001 0.002 0.002 -0.001 0.001 -0.000 0.002 0.002 0.001 -0.001
  -0.000 -0.001 -0.001 -0.002 -0.001 0.001 -0.002 -0.001 0.000 0.000]
 [-0.004 -0.011 0.006 -0.008 -0.001 0.002 0.011 -0.002 -0.005 -0.008
  0.007 -0.003 -0.006 -0.007 0.016 0.003 -0.012 -0.014 0.244 -0.007
  -0.010 0.011 0.002 0.004 -0.001 -0.001 0.003 -0.001 -0.007 -0.005 0.003
  0.002 0.004 0.001 0.001 -0.002 0.001 0.002 -0.000 0.001 0.001 -0.000
  0.001 0.001 0.001 0.003 -0.001 -0.001 -0.002 0.001 -0.000 0.003 -0.001
  0.001 -0.001 0.001 -0.002 0.001 0.003 -0.002 0.002 -0.000 -0.000 -0.001]
 [-0.003 -0.005 -0.007 -0.005 0.000 -0.007 -0.007 0.004 0.003 -0.000
  0.002 0.001 0.001 0.002 -0.002 0.001 -0.004 0.006 -0.007 0.158 -0.006
  0.001 0.005 0.003 -0.001 -0.005 -0.002 0.001 -0.003 -0.004 -0.003 0.000
  -0.001 0.004 0.001 0.001 0.001 -0.001 0.000 0.000 -0.000 -0.001 0.002
  -0.001 -0.002 0.001 0.002 0.004 0.002 -0.001 -0.002 -0.002 0.000 -0.001
  -0.000 -0.001 -0.000 -0.000 0.002 -0.001 0.001 0.000 0.001 0.001]
 [-0.012 0.006 0.001 -0.015 -0.016 -0.009 0.003 0.003 0.006 0.004 0.002
  0.001 -0.002 0.004 -0.002 -0.000 0.007 -0.003 -0.010 -0.006 0.250
  -0.011 -0.011 -0.004 0.001 -0.001 0.003 0.001 -0.001 -0.005 0.003 0.006
  0.001 -0.003 0.002 -0.005 -0.005 -0.001 0.001 -0.001 0.003 -0.006 0.001
  0.001 0.001 0.002 0.002 -0.003 -0.004 -0.005 -0.001 -0.002 -0.001
  -0.002 0.000 0.000 0.001 -0.000 0.001 -0.001 -0.000 -0.002 0.000 0.000]
 [0.009 0.006 -0.008 -0.019 0.001 -0.002 0.010 0.001 -0.004 0.002 0.002
  -0.003 -0.009 -0.005 0.003 0.009 0.013 0.002 0.011 0.001 -0.011 0.269
  0.025 0.002 -0.001 0.002 -0.000 -0.004 0.001 0.004 0.001 -0.000 0.002
  -0.002 -0.002 0.001 0.001 -0.001 0.000 -0.000 -0.001 0.006 -0.001 0.003
  0.000 0.001 -0.002 0.003 0.000 0.001 -0.002 0.001 0.000 -0.000 -0.000
  0.001 0.001 0.001 0.000 -0.000 -0.001 -0.001 0.000 0.001]
 [0.004 0.011 0.017 0.001 0.001 0.004 0.012 -0.000 -0.009 0.002 -0.006
  0.006 0.005 -0.006 0.000 0.002 0.007 0.001 0.002 0.005 -0.011 0.025
  0.264 -0.004 -0.009 0.003 -0.008 0.001 -0.000 0.003 0.001 0.000 0.001
  0.003 -0.002 -0.009 0.003 -0.002 -0.001 -0.001 -0.001 0.003 0.000 0.001
  0.001 0.002 0.000 0.002 0.001 0.003 0.001 0.001 0.000 0.001 -0.000
  -0.000 -0.000 0.001 0.000 -0.001 -0.001 -0.001 0.000 0.000]
 [0.009 0.007 0.000 0.011 -0.004 0.000 -0.000 -0.001 0.008 0.004 -0.000
  -0.004 0.004 -0.004 -0.006 -0.001 -0.003 -0.004 0.004 0.003 -0.004
  0.002 -0.004 0.176 -0.001 0.000 -0.004 0.003 -0.005 -0.004 -0.004 0.003
  0.005 -0.000 0.000 -0.005 -0.000 0.002 -0.001 -0.001 0.001 0.001 -0.002
  -0.000 -0.001 0.001 0.002 0.001 -0.001 0.001 0.001 0.002 0.001 0.000
  -0.000 0.000 0.000 -0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000]
 [-0.003 -0.013 0.009 0.031 -0.004 0.002 0.004 0.002 0.008 0.003 -0.008
  -0.006 0.002 0.002 -0.001 -0.009 0.004 -0.001 -0.001 -0.001 0.001
  -0.001 -0.009 -0.001 0.215 0.003 -0.006 -0.003 -0.004 -0.004 -0.002
  -0.002 -0.005 -0.003 -0.004 0.002 -0.003 0.004 -0.002 0.001 -0.001
  0.005 -0.000 0.001 -0.002 -0.004 0.001 -0.000 -0.000 0.001 0.002 0.003
  -0.002 0.001 0.001 -0.000 0.001 0.000 -0.001 0.000 -0.000 0.001 -0.000
  0.000]
 [0.003 0.013 -0.000 -0.012 0.001 -0.002 0.002 -0.000 0.002 0.001 0.001
  -0.002 0.005 0.000 -0.000 -0.003 -0.002 0.002 -0.001 -0.005 -0.001
  0.002 0.003 0.000 0.003 0.168 -0.007 0.005 0.002 -0.003 -0.002 0.002
  -0.001 0.004 0.000 -0.000 -0.003 0.000 -0.002 0.000 -0.002 0.002 -0.003
  -0.000 -0.000 0.001 -0.001 -0.000 0.001 0.001 -0.000 -0.001 -0.001
  -0.001 -0.001 0.001 -0.000 0.001 -0.001 0.000 -0.000 -0.001 -0.001
  -0.000]
 [-0.011 0.001 0.007 -0.016 -0.009 0.000 0.000 -0.004 0.001 -0.002 0.007
  -0.002 -0.008 0.002 -0.006 -0.003 -0.007 0.001 0.003 -0.002 0.003
  -0.000 -0.008 -0.004 -0.006 -0.007 0.201 0.002 -0.000 0.002 0.003
  -0.003 0.002 0.001 -0.002 -0.001 -0.002 0.002 0.004 -0.002 0.001 0.000
  0.002 -0.000 0.001 0.001 0.000 -0.000 0.000 0.002 0.001 0.000 -0.001
  0.001 -0.001 0.001 -0.001 -0.000 -0.000 0.000 -0.002 -0.000 -0.001
  -0.000]
 [0.015 0.005 0.002 0.001 -0.004 -0.000 -0.001 0.003 0.006 -0.002 0.001
  0.003 -0.002 -0.002 -0.004 0.002 0.000 -0.002 -0.001 0.001 0.001 -0.004
  0.001 0.003 -0.003 0.005 0.002 0.137 0.004 -0.000 -0.001 -0.000 -0.001
  0.001 -0.001 0.000 0.000 -0.000 0.001 0.003 -0.001 -0.000 -0.000 -0.000
  0.001 -0.001 0.002 0.002 0.002 -0.001 -0.000 -0.000 -0.000 -0.000 0.000
  -0.000 0.000 -0.000 0.001 0.001 -0.000 0.000 0.001 0.000]
 [-0.006 0.007 0.013 -0.003 0.010 -0.007 0.005 -0.004 0.002 -0.017 -0.003
  0.004 -0.001 0.003 0.001 0.002 0.003 0.003 -0.007 -0.003 -0.001 0.001
  -0.000 -0.005 -0.004 0.002 -0.000 0.004 0.211 0.009 -0.002 -0.004
  -0.000 0.000 -0.002 0.006 0.001 -0.002 -0.003 0.003 -0.001 0.004 -0.000
  0.002 -0.002 0.001 0.000 0.000 -0.000 0.001 0.000 0.000 -0.000 0.000
  -0.001 -0.000 -0.001 0.001 0.000 -0.001 0.000 -0.002 -0.000 0.001]
 [0.002 -0.003 0.007 -0.005 0.004 -0.005 -0.005 0.004 0.003 -0.003 -0.004
  0.000 0.001 0.005 -0.000 0.002 -0.003 -0.001 -0.005 -0.004 -0.005 0.004
  0.003 -0.004 -0.004 -0.003 0.002 -0.000 0.009 0.210 -0.000 0.006 -0.002
  0.005 0.001 0.001 0.001 0.002 0.003 0.001 0.004 -0.006 -0.002 0.000
  0.001 -0.001 -0.000 -0.004 -0.000 0.001 -0.001 -0.001 -0.000 -0.000
  -0.001 0.000 -0.001 0.001 -0.000 -0.000 -0.001 -0.001 -0.000 0.001]
 [-0.003 -0.006 -0.004 0.014 -0.006 -0.001 -0.009 -0.002 0.002 -0.002
  -0.002 -0.000 -0.001 -0.000 -0.001 -0.002 -0.004 -0.001 0.003 -0.003
  0.003 0.001 0.001 -0.004 -0.002 -0.002 0.003 -0.001 -0.002 -0.000 0.204
  0.004 0.008 -0.003 -0.000 -0.002 -0.002 0.001 -0.001 0.001 -0.004
  -0.002 -0.001 0.001 0.001 -0.001 0.003 0.000 0.001 -0.002 0.001 -0.000
  -0.000 -0.001 0.000 0.001 -0.000 0.001 -0.001 0.001 0.001 0.001 -0.000
  -0.001]
 [-0.007 -0.006 0.012 -0.012 0.000 -0.003 0.000 0.004 -0.000 0.002 0.001
  -0.002 -0.003 0.001 0.002 -0.000 0.000 -0.000 0.002 0.000 0.006 -0.000
  0.000 0.003 -0.002 0.002 -0.003 -0.000 -0.004 0.006 0.004 0.126 -0.000
  -0.002 0.002 0.000 -0.000 -0.001 0.000 -0.001 0.002 -0.003 -0.001 0.001
  -0.000 0.002 0.002 0.000 0.001 0.000 -0.001 -0.000 0.000 -0.000 0.001
  0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.001 0.000]
 [0.015 -0.013 0.004 -0.006 -0.004 -0.005 0.002 -0.003 0.000 0.002 -0.004
  0.006 0.000 -0.004 -0.006 0.000 0.003 0.005 0.004 -0.001 0.001 0.002
  0.001 0.005 -0.005 -0.001 0.002 -0.001 -0.000 -0.002 0.008 -0.000 0.149
  0.013 -0.006 -0.013 -0.003 -0.000 -0.000 -0.001 0.002 -0.003 0.002
  -0.002 -0.000 0.001 -0.000 0.003 0.000 0.001 -0.000 -0.001 -0.001
  -0.001 0.000 0.000 -0.000 0.000 -0.000 0.001 -0.000 -0.000 -0.000
  -0.000]
 [0.005 0.007 -0.003 -0.000 0.006 -0.003 -0.005 -0.002 -0.000 0.011
  -0.002 0.003 -0.004 0.004 0.004 -0.003 -0.004 0.004 0.001 0.004 -0.003
  -0.002 0.003 -0.000 -0.003 0.004 0.001 0.001 0.000 0.005 -0.003 -0.002
  0.013 0.151 0.010 0.005 -0.001 -0.002 -0.002 0.000 -0.001 -0.000 0.001
  0.001 -0.000 0.001 0.002 0.001 -0.000 -0.000 0.000 -0.000 -0.000 -0.000
  0.000 0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000]
 [-0.007 -0.001 0.002 0.006 0.006 0.001 0.008 -0.005 -0.012 -0.001 0.006
  0.006 -0.002 -0.003 0.001 -0.001 -0.006 0.000 0.001 0.001 0.002 -0.002
  -0.002 0.000 -0.004 0.000 -0.002 -0.001 -0.002 0.001 -0.000 0.002
  -0.006 0.010 0.145 0.001 0.001 -0.000 0.002 0.002 0.000 -0.000 -0.002
  0.001 -0.000 0.003 0.002 -0.002 -0.001 -0.001 0.000 0.000 0.001 -0.001
  -0.000 0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000]
 [-0.001 -0.010 -0.002 0.007 0.006 0.003 0.001 -0.004 0.002 0.002 0.001
  -0.001 0.001 -0.001 0.004 -0.001 -0.001 -0.003 -0.002 0.001 -0.005
  0.001 -0.009 -0.005 0.002 -0.000 -0.001 0.000 0.006 0.001 -0.002 0.000
  -0.013 0.005 0.001 0.091 0.001 0.000 0.003 0.003 -0.000 0.000 0.000
  0.000 -0.000 -0.001 0.001 -0.001 0.001 0.000 0.001 -0.000 -0.000 0.000
  0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 0.000 -0.000 0.000]
 [0.007 -0.002 -0.009 -0.018 0.005 0.004 0.004 0.001 -0.005 0.000 -0.003
  0.000 0.004 -0.000 -0.003 -0.001 0.002 -0.001 0.001 0.001 -0.005 0.001
  0.003 -0.000 -0.003 -0.003 -0.002 0.000 0.001 0.001 -0.002 -0.000
  -0.003 -0.001 0.001 0.001 0.112 0.005 0.008 0.004 0.003 0.002 0.001
  -0.002 -0.000 0.000 0.002 0.002 -0.001 -0.001 0.000 -0.001 0.000 0.001
  -0.000 0.001 -0.000 -0.001 0.000 0.000 0.000 -0.000 0.000 -0.000]
 [-0.003 0.008 -0.001 -0.010 0.001 -0.000 -0.003 0.001 0.000 0.004 -0.002
  0.002 0.001 -0.001 -0.001 -0.002 -0.003 -0.001 0.002 -0.001 -0.001
  -0.001 -0.002 0.002 0.004 0.000 0.002 -0.000 -0.002 0.002 0.001 -0.001
  -0.000 -0.002 -0.000 0.000 0.005 0.087 0.000 0.012 -0.001 0.000 0.001
  -0.001 -0.001 -0.001 -0.002 0.002 0.001 -0.001 -0.002 0.001 -0.000
  0.001 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000 0.000 0.000 -0.000]
 [-0.001 0.003 0.008 -0.006 -0.001 -0.003 0.001 -0.003 0.004 -0.002
  -0.003 -0.002 0.001 -0.001 -0.004 -0.000 -0.004 -0.000 -0.000 0.000
  0.001 0.000 -0.001 -0.001 -0.002 -0.002 0.004 0.001 -0.003 0.003 -0.001
  0.000 -0.000 -0.002 0.002 0.003 0.008 0.000 0.107 -0.004 0.003 -0.004
  0.002 -0.004 -0.000 -0.000 -0.001 0.001 0.000 0.001 0.000 -0.000 0.000
  -0.001 -0.000 0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 0.000]
 [0.003 0.005 0.008 0.010 0.003 -0.003 -0.000 0.001 -0.001 -0.004 0.002
  -0.002 0.000 -0.003 -0.000 0.001 0.001 0.001 0.001 0.000 -0.001 -0.000
  -0.001 -0.001 0.001 0.000 -0.002 0.003 0.003 0.001 0.001 -0.001 -0.001
  0.000 0.002 0.003 0.004 0.012 -0.004 0.065 -0.002 0.001 0.002 0.000
  0.003 -0.000 0.002 -0.000 0.001 -0.002 -0.001 -0.001 0.000 0.000 0.001
  0.000 0.000 -0.000 0.000 -0.000 0.000 0.000 -0.000 0.000]
 [0.002 -0.001 -0.012 -0.010 0.001 -0.004 0.004 0.000 0.000 -0.002 0.001
  -0.004 -0.003 0.001 -0.001 -0.002 0.001 0.001 0.001 -0.000 0.003 -0.001
  -0.001 0.001 -0.001 -0.002 0.001 -0.001 -0.001 0.004 -0.004 0.002 0.002
  -0.001 0.000 -0.000 0.003 -0.001 0.003 -0.002 0.081 0.003 -0.010 -0.012
  0.007 0.001 0.005 0.007 -0.003 -0.000 -0.001 -0.000 0.000 -0.000 0.000
  0.000 -0.000 -0.000 -0.000 0.000 0.000 -0.001 0.000 0.000]
 [0.010 -0.008 -0.013 -0.005 -0.002 -0.001 0.001 0.001 -0.004 0.004 0.002
  -0.001 -0.001 0.001 -0.003 -0.003 -0.001 0.002 -0.000 -0.001 -0.006
  0.006 0.003 0.001 0.005 0.002 0.000 -0.000 0.004 -0.006 -0.002 -0.003
  -0.003 -0.000 -0.000 0.000 0.002 0.000 -0.004 0.001 0.003 0.091 0.002
  0.002 0.002 0.000 0.000 0.006 -0.003 -0.001 0.000 -0.001 -0.001 0.001
  -0.001 -0.000 0.000 0.000 -0.000 0.001 0.000 -0.000 0.000 0.000]
 [-0.018 0.008 0.015 0.006 -0.004 0.001 -0.000 -0.001 -0.003 0.005 0.005
  -0.000 0.004 0.002 -0.001 -0.000 -0.001 0.000 0.001 0.002 0.001 -0.001
  0.000 -0.002 -0.000 -0.003 0.002 -0.000 -0.000 -0.002 -0.001 -0.001
  0.002 0.001 -0.002 0.000 0.001 0.001 0.002 0.002 -0.010 0.002 0.087
  -0.010 -0.001 -0.003 0.001 0.004 -0.002 0.001 -0.000 -0.001 -0.001
  0.001 -0.001 0.001 -0.000 -0.000 -0.000 -0.000 0.000 0.000 0.001 0.000]
 [-0.006 0.003 0.008 0.003 0.002 -0.001 -0.000 0.001 -0.002 0.001 0.000
  0.000 -0.000 -0.002 -0.000 0.001 -0.001 -0.001 0.001 -0.001 0.001 0.003
  0.001 -0.000 0.001 -0.000 -0.000 -0.000 0.002 0.000 0.001 0.001 -0.002
  0.001 0.001 0.000 -0.002 -0.001 -0.004 0.000 -0.012 0.002 -0.010 0.052
  0.000 -0.002 -0.000 0.004 -0.002 0.000 -0.000 -0.002 -0.001 -0.000
  -0.001 -0.000 -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 -0.000]
 [-0.011 0.002 -0.001 -0.040 0.007 0.008 -0.011 -0.003 0.004 0.003 0.001
  0.001 -0.000 0.006 -0.005 -0.005 -0.004 0.001 0.001 -0.002 0.001 0.000
  0.001 -0.001 -0.002 -0.000 0.001 0.001 -0.002 0.001 0.001 -0.000 -0.000
  -0.000 -0.000 -0.000 -0.000 -0.001 -0.000 0.003 0.007 0.002 -0.001
  0.000 0.046 -0.011 0.010 -0.022 -0.005 -0.007 -0.001 -0.002 -0.001
  -0.001 -0.001 -0.000 -0.001 -0.000 -0.001 0.000 0.000 -0.001 0.001
  -0.000]
 [-0.011 -0.008 0.019 -0.049 -0.004 -0.001 -0.001 0.001 -0.012 -0.006
  -0.004 -0.003 -0.006 -0.001 -0.000 0.004 0.000 0.002 0.003 0.001 0.002
  0.001 0.002 0.001 -0.004 0.001 0.001 -0.001 0.001 -0.001 -0.001 0.002
  0.001 0.001 0.003 -0.001 0.000 -0.001 -0.000 -0.000 0.001 0.000 -0.003
  -0.002 -0.011 0.020 0.002 0.005 0.000 0.003 -0.002 0.000 0.000 0.000
  0.000 -0.000 -0.000 0.000 0.002 -0.000 -0.000 -0.001 -0.000 -0.000]
 [-0.019 0.021 0.032 0.013 -0.002 0.004 -0.006 0.001 -0.008 -0.009 0.001
  -0.004 -0.007 0.001 0.000 0.004 -0.000 0.002 -0.001 0.002 0.002 -0.002
  0.000 0.002 0.001 -0.001 0.000 0.002 0.000 -0.000 0.003 0.002 -0.000
  0.002 0.002 0.001 0.002 -0.002 -0.001 0.002 0.005 0.000 0.001 -0.000
  0.010 0.002 0.019 -0.004 -0.004 -0.002 0.000 0.000 -0.000 -0.001 -0.000
  -0.001 -0.000 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.000]
 [0.007 -0.005 -0.001 -0.001 -0.003 -0.000 0.004 0.001 -0.001 0.003
  -0.001 0.001 0.001 0.000 0.003 0.001 -0.003 -0.001 -0.001 0.004 -0.003
  0.003 0.002 0.001 -0.000 -0.000 -0.000 0.002 0.000 -0.004 0.000 0.000
  0.003 0.001 -0.002 -0.001 0.002 0.002 0.001 -0.000 0.007 0.006 0.004
  0.004 -0.022 0.005 -0.004 0.037 0.003 0.009 0.003 0.003 0.000 0.001
  -0.001 0.001 0.000 -0.000 -0.000 0.000 -0.000 0.000 0.001 0.000]
 [-0.006 -0.004 0.011 -0.005 0.004 -0.003 -0.004 0.002 -0.007 0.001 0.004
  0.001 0.001 0.002 0.001 0.004 0.000 0.001 -0.002 0.002 -0.004 0.000
  0.001 -0.001 -0.000 0.001 0.000 0.002 -0.000 -0.000 0.001 0.001 0.000
  -0.000 -0.001 0.001 -0.001 0.001 0.000 0.001 -0.003 -0.003 -0.002
  -0.002 -0.005 0.000 -0.004 0.003 0.025 0.002 -0.000 -0.005 0.000 0.000
  0.001 0.002 0.001 0.000 -0.001 -0.001 -0.000 -0.000 0.001 -0.000]
 [-0.002 -0.019 0.017 -0.012 0.002 0.008 -0.003 -0.006 0.001 -0.003 0.001
  0.001 0.000 -0.002 0.001 0.000 -0.003 -0.000 0.001 -0.001 -0.005 0.001
  0.003 0.001 0.001 0.001 0.002 -0.001 0.001 0.001 -0.002 0.000 0.001
  -0.000 -0.001 0.000 -0.001 -0.001 0.001 -0.002 -0.000 -0.001 0.001
  0.000 -0.007 0.003 -0.002 0.009 0.002 0.022 0.004 0.004 -0.000 0.001
  -0.001 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000]
 [0.017 -0.011 -0.023 0.001 0.000 0.005 -0.003 -0.005 -0.010 -0.002
  -0.001 -0.004 -0.004 -0.001 0.003 0.000 0.003 0.002 -0.000 -0.002
  -0.001 -0.002 0.001 0.001 0.002 -0.000 0.001 -0.000 0.000 -0.001 0.001
  -0.001 -0.000 0.000 0.000 0.001 0.000 -0.002 0.000 -0.001 -0.001 0.000
  -0.000 -0.000 -0.001 -0.002 0.000 0.003 -0.000 0.004 0.015 0.001 0.000
  0.001 -0.000 0.000 0.000 0.000 -0.001 -0.000 0.000 0.000 0.000 -0.000]
 [-0.014 0.008 -0.001 0.014 -0.002 0.003 0.001 0.001 0.003 -0.005 -0.004
  -0.004 0.002 -0.002 0.003 -0.001 -0.000 0.002 0.003 -0.002 -0.002 0.001
  0.001 0.002 0.003 -0.001 0.000 -0.000 0.000 -0.001 -0.000 -0.000 -0.001
  -0.000 0.000 -0.000 -0.001 0.001 -0.000 -0.001 -0.000 -0.001 -0.001
  -0.002 -0.002 0.000 0.000 0.003 -0.005 0.004 0.001 0.011 0.000 0.001
  -0.001 -0.001 -0.000 -0.000 0.000 0.000 0.000 0.001 -0.000 -0.000]
 [-0.003 -0.003 0.001 0.006 -0.008 -0.006 0.003 0.004 -0.002 0.000 -0.003
  0.000 -0.003 -0.002 -0.001 0.000 -0.003 0.001 -0.001 0.000 -0.001 0.000
  0.000 0.001 -0.002 -0.001 -0.001 -0.000 -0.000 -0.000 -0.000 0.000
  -0.001 -0.000 0.001 -0.000 0.000 -0.000 0.000 0.000 0.000 -0.001 -0.001
  -0.001 -0.001 0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.003 0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000]
 [0.003 -0.007 0.000 -0.003 0.003 -0.002 0.003 0.004 -0.002 -0.002 -0.002
  -0.003 -0.001 -0.001 0.003 -0.001 -0.000 -0.001 0.001 -0.001 -0.002
  -0.000 0.001 0.000 0.001 -0.001 0.001 -0.000 0.000 -0.000 -0.001 -0.000
  -0.001 -0.000 -0.001 0.000 0.001 0.001 -0.001 0.000 -0.000 0.001 0.001
  -0.000 -0.001 0.000 -0.001 0.001 0.000 0.001 0.001 0.001 0.000 0.003
  -0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000 -0.000 0.000 -0.000]
 [0.001 -0.002 0.006 0.003 0.007 0.005 -0.003 -0.004 -0.003 0.002 0.001
  -0.002 0.000 -0.000 0.001 -0.001 0.002 -0.000 -0.001 -0.000 0.000
  -0.000 -0.000 -0.000 0.001 -0.001 -0.001 0.000 -0.001 -0.001 0.000
  0.001 0.000 0.000 -0.000 0.000 -0.000 -0.000 -0.000 0.001 0.000 -0.001
  -0.001 -0.001 -0.001 0.000 -0.000 -0.001 0.001 -0.001 -0.000 -0.001
  -0.000 -0.000 0.004 -0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.000
  -0.000]
 [0.002 -0.000 -0.003 -0.002 0.001 -0.001 0.001 -0.001 0.000 0.001 -0.002
  0.001 0.000 0.000 0.001 0.000 -0.000 -0.001 0.001 -0.001 0.000 0.001
  -0.000 0.000 -0.000 0.001 0.001 -0.000 -0.000 0.000 0.001 0.000 0.000
  0.000 0.000 -0.000 0.001 0.000 0.000 0.000 0.000 -0.000 0.001 -0.000
  -0.000 -0.000 -0.001 0.001 0.002 -0.001 0.000 -0.001 -0.000 0.000
  -0.000 0.002 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 0.000]
 [-0.000 0.005 -0.002 0.006 -0.006 0.000 0.000 0.001 0.002 -0.002 0.001
  0.001 0.001 0.000 0.000 0.002 0.002 -0.001 -0.002 -0.000 0.001 0.001
  -0.000 0.000 0.001 -0.000 -0.001 0.000 -0.001 -0.001 -0.000 0.000
  -0.000 -0.000 0.000 0.000 -0.000 0.000 0.000 0.000 -0.000 0.000 -0.000
  -0.000 -0.001 -0.000 -0.000 0.000 0.001 -0.000 0.000 -0.000 0.000
  -0.000 -0.000 0.000 0.002 0.000 -0.000 0.000 -0.000 0.000 0.000 0.000]
 [0.003 -0.001 -0.003 -0.002 -0.001 0.000 -0.001 -0.001 -0.001 -0.003
  -0.002 0.001 0.001 0.001 0.001 -0.000 0.001 -0.002 0.001 -0.000 -0.000
  0.001 0.001 -0.000 0.000 0.001 -0.000 -0.000 0.001 0.001 0.001 0.000
  0.000 -0.000 0.000 -0.000 -0.001 0.000 0.000 -0.000 -0.000 0.000 -0.000
  -0.000 -0.000 0.000 -0.001 -0.000 0.000 -0.000 0.000 -0.000 -0.000
  -0.000 0.000 0.000 0.000 0.001 0.000 -0.000 -0.000 -0.000 -0.000 -0.000]
 [0.001 -0.001 -0.002 0.006 -0.001 0.001 -0.004 0.002 -0.006 0.000 -0.001
  -0.001 -0.000 -0.001 -0.001 0.001 0.001 -0.001 0.003 0.002 0.001 0.000
  0.000 0.000 -0.001 -0.001 -0.000 0.001 0.000 -0.000 -0.001 -0.000
  -0.000 0.000 0.000 0.000 0.000 0.000 -0.000 0.000 -0.000 -0.000 -0.000
  0.000 -0.001 0.002 -0.000 -0.000 -0.001 0.000 -0.001 0.000 -0.000
  -0.000 0.000 -0.000 -0.000 0.000 0.003 -0.000 0.000 0.000 -0.000 -0.000]
 [0.001 -0.000 -0.003 -0.007 -0.004 -0.000 -0.000 -0.000 0.002 0.002
  0.003 0.001 0.003 0.001 0.002 -0.001 -0.000 0.001 -0.002 -0.001 -0.001
  -0.000 -0.001 0.000 0.000 0.000 0.000 0.001 -0.001 -0.000 0.001 -0.000
  0.001 0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 0.000 0.001 -0.000
  -0.000 0.000 -0.000 -0.000 0.000 -0.001 0.000 -0.000 0.000 0.000 -0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.002 -0.000 -0.000 -0.000 0.000]
 [0.001 0.002 0.000 0.004 0.002 0.001 -0.001 -0.001 -0.002 -0.001 0.003
  -0.001 -0.002 -0.001 0.001 0.001 -0.000 -0.002 0.002 0.001 -0.000
  -0.001 -0.001 0.000 -0.000 -0.000 -0.002 -0.000 0.000 -0.001 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 0.000 -0.000 -0.000 0.000 0.000 0.000
  0.000 -0.000 0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 0.000 -0.000
  -0.000 0.000 -0.000 -0.000 -0.000 0.000 -0.000 0.001 0.000 -0.000
  -0.000]
 [0.001 0.002 -0.005 0.024 0.003 0.001 -0.002 -0.000 -0.004 -0.002 0.000
  -0.001 -0.001 -0.001 -0.001 0.001 -0.001 -0.001 -0.000 0.000 -0.002
  -0.001 -0.001 -0.000 0.001 -0.001 -0.000 0.000 -0.002 -0.001 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 -0.001
  -0.000 0.000 0.000 -0.001 -0.001 0.000 0.000 -0.000 0.000 0.000 0.001
  -0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 -0.000 0.000 0.002 -0.000
  -0.000]
 [-0.000 0.002 0.001 -0.005 -0.001 0.000 -0.001 0.001 -0.000 0.002 -0.001
  0.000 0.001 0.001 -0.000 0.000 0.001 0.000 -0.000 0.001 0.000 0.000
  0.000 -0.000 -0.000 -0.001 -0.001 0.001 -0.000 -0.000 -0.000 0.001
  -0.000 -0.000 -0.000 -0.000 0.000 0.000 0.000 -0.000 0.000 0.000 0.001
  0.000 0.001 -0.000 0.000 0.001 0.001 0.000 0.000 -0.000 0.000 0.000
  0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 -0.000 0.001 -0.000]
 [0.004 0.002 -0.000 0.000 -0.002 -0.002 0.002 0.000 0.002 0.001 -0.002
  0.000 0.000 0.000 0.000 -0.000 0.001 0.000 -0.001 0.001 0.000 0.001
  0.000 -0.000 0.000 -0.000 -0.000 0.000 0.001 0.001 -0.001 0.000 -0.000
  -0.000 -0.000 0.000 -0.000 -0.000 0.000 0.000 0.000 0.000 0.000 -0.000
  -0.000 -0.000 0.000 0.000 -0.000 -0.000 -0.000 -0.000 0.000 -0.000
  -0.000 0.000 0.000 -0.000 -0.000 0.000 -0.000 -0.000 -0.000 0.001]]
=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
RNN                                      --
├─Canvas: 1-1                            --
│    └─Linear: 2-1                       3,072
├─Encoder: 1-2                           --
│    └─Linear: 2-2                       786,688
│    └─Linear: 2-3                       65,792
├─ModuleList: 1-3                        --
│    └─EncoderToLatents: 2-4             --
│    │    └─Linear: 3-1                  1,028
│    │    └─Linear: 3-2                  1,028
│    └─EncoderToLatents: 2-5             --
│    │    └─Linear: 3-3                  1,028
│    │    └─Linear: 3-4                  1,028
│    └─EncoderToLatents: 2-6             --
│    │    └─Linear: 3-5                  1,028
│    │    └─Linear: 3-6                  1,028
│    └─EncoderToLatents: 2-7             --
│    │    └─Linear: 3-7                  1,028
│    │    └─Linear: 3-8                  1,028
│    └─EncoderToLatents: 2-8             --
│    │    └─Linear: 3-9                  1,028
│    │    └─Linear: 3-10                 1,028
│    └─EncoderToLatents: 2-9             --
│    │    └─Linear: 3-11                 1,028
│    │    └─Linear: 3-12                 1,028
│    └─EncoderToLatents: 2-10            --
│    │    └─Linear: 3-13                 1,028
│    │    └─Linear: 3-14                 1,028
│    └─EncoderToLatents: 2-11            --
│    │    └─Linear: 3-15                 1,028
│    │    └─Linear: 3-16                 1,028
│    └─EncoderToLatents: 2-12            --
│    │    └─Linear: 3-17                 1,028
│    │    └─Linear: 3-18                 1,028
│    └─EncoderToLatents: 2-13            --
│    │    └─Linear: 3-19                 1,028
│    │    └─Linear: 3-20                 1,028
│    └─EncoderToLatents: 2-14            --
│    │    └─Linear: 3-21                 1,028
│    │    └─Linear: 3-22                 1,028
│    └─EncoderToLatents: 2-15            --
│    │    └─Linear: 3-23                 1,028
│    │    └─Linear: 3-24                 1,028
│    └─EncoderToLatents: 2-16            --
│    │    └─Linear: 3-25                 1,028
│    │    └─Linear: 3-26                 1,028
│    └─EncoderToLatents: 2-17            --
│    │    └─Linear: 3-27                 1,028
│    │    └─Linear: 3-28                 1,028
│    └─EncoderToLatents: 2-18            --
│    │    └─Linear: 3-29                 1,028
│    │    └─Linear: 3-30                 1,028
│    └─EncoderToLatents: 2-19            --
│    │    └─Linear: 3-31                 1,028
│    │    └─Linear: 3-32                 1,028
├─LatentsToLatentsComplicated: 1-4       --
│    └─Linear: 2-20                      1,700
│    └─Linear: 2-21                      10,100
│    └─Linear: 2-22                      10,100
│    └─Linear: 2-23                      404
│    └─Linear: 2-24                      404
├─ModuleList: 1-5                        --
│    └─LatentsToDecoder: 2-25            --
│    │    └─Linear: 3-33                 1,280
│    └─LatentsToDecoder: 2-26            --
│    │    └─Linear: 3-34                 1,280
│    └─LatentsToDecoder: 2-27            --
│    │    └─Linear: 3-35                 1,280
│    └─LatentsToDecoder: 2-28            --
│    │    └─Linear: 3-36                 1,280
│    └─LatentsToDecoder: 2-29            --
│    │    └─Linear: 3-37                 1,280
│    └─LatentsToDecoder: 2-30            --
│    │    └─Linear: 3-38                 1,280
│    └─LatentsToDecoder: 2-31            --
│    │    └─Linear: 3-39                 1,280
│    └─LatentsToDecoder: 2-32            --
│    │    └─Linear: 3-40                 1,280
│    └─LatentsToDecoder: 2-33            --
│    │    └─Linear: 3-41                 1,280
│    └─LatentsToDecoder: 2-34            --
│    │    └─Linear: 3-42                 1,280
│    └─LatentsToDecoder: 2-35            --
│    │    └─Linear: 3-43                 1,280
│    └─LatentsToDecoder: 2-36            --
│    │    └─Linear: 3-44                 1,280
│    └─LatentsToDecoder: 2-37            --
│    │    └─Linear: 3-45                 1,280
│    └─LatentsToDecoder: 2-38            --
│    │    └─Linear: 3-46                 1,280
│    └─LatentsToDecoder: 2-39            --
│    │    └─Linear: 3-47                 1,280
│    └─LatentsToDecoder: 2-40            --
│    │    └─Linear: 3-48                 1,280
├─Decoder: 1-6                           --
│    └─Linear: 2-41                      65,792
│    └─Linear: 2-42                      789,504
=================================================================
Total params: 1,786,932
Trainable params: 1,786,932
Non-trainable params: 0
=================================================================
[Epoch   1 (270.34s)]	ELBO: -11670.951, -7489.736, -5202.048, -3776.753, -2829.863, -2124.421, -1540.971, -1057.734, -657.105, -336.973, -76.898, -21.993, 27.955, 41.123, 46.586, 29.200 (79.002)	Log prob: -11651.940, -7455.011, -5152.083, -3712.200, -2751.146, -2032.340, -1435.149, -938.622, -524.758, -191.064, 82.354, 151.801, 215.145, 243.712, 264.514, 261.143 (310.965)	KLD: 19.009, 15.716, 15.241, 14.589, 14.162, 13.368, 13.740, 13.289, 13.235, 13.562, 13.343, 14.542, 13.396, 15.400, 15.338, 14.015 (231.963)	Grad: 294019.219
[Epoch   2 (275.51s)]	ELBO: -11620.851, -7446.492, -5177.736, -3754.681, -2805.416, -2098.919, -1511.309, -1025.696, -622.373, -295.805, -26.145, 51.278, 98.592, 109.013, 114.247, 100.245 (97.446)	Log prob: -11601.777, -7411.564, -5127.559, -3689.920, -2726.501, -2006.636, -1405.431, -906.573, -489.997, -149.962, 132.983, 223.040, 283.634, 309.392, 329.932, 330.229 (324.612)	KLD: 19.077, 15.852, 15.248, 14.587, 14.152, 13.370, 13.593, 13.245, 13.254, 13.467, 13.285, 12.634, 13.280, 15.337, 15.307, 14.298 (227.166)	Grad: 240473.125
[Epoch   3 (270.73s)]	ELBO: -11696.332, -7533.912, -5274.595, -3854.271, -2905.229, -2200.984, -1617.120, -1133.605, -731.785, -410.330, -151.773, -51.178, 19.407, 35.694, 43.139, 30.231 (73.657)	Log prob: -11676.621, -7498.415, -5223.892, -3789.029, -2825.861, -2108.188, -1510.648, -1013.833, -598.714, -263.836, 8.193, 122.700, 206.819, 238.350, 261.067, 262.551 (311.577)	KLD: 19.713, 15.790, 15.199, 14.539, 14.126, 13.428, 13.676, 13.300, 13.298, 13.423, 13.472, 13.912, 13.535, 15.244, 15.272, 14.391 (237.919)	Grad: 443177.500
[Epoch   4 (268.04s)]	ELBO: -11576.567, -7437.840, -5193.103, -3768.201, -2818.615, -2111.908, -1523.153, -1036.489, -632.832, -305.118, -30.681, 51.460, 114.020, 125.500, 132.156, 121.417 (108.949)	Log prob: -11556.571, -7402.012, -5142.157, -3702.723, -2739.059, -2018.844, -1416.369, -916.520, -499.899, -158.826, 128.832, 226.058, 302.129, 328.805, 350.522, 354.401 (340.991)	KLD: 19.994, 15.834, 15.117, 14.533, 14.080, 13.505, 13.722, 13.185, 12.964, 13.360, 13.220, 15.086, 13.510, 15.196, 15.061, 14.617 (232.041)	Grad: 243982.328
[Epoch   5 (275.17s)]	ELBO: -11583.486, -7443.022, -5194.750, -3767.013, -2818.488, -2110.797, -1521.233, -1034.565, -629.627, -301.326, -23.533, 64.585, 134.618, 147.657, 154.248, 143.575 (129.447)	Log prob: -11564.103, -7407.825, -5144.348, -3702.089, -2739.554, -2018.531, -1415.407, -915.672, -497.712, -156.093, 134.919, 236.580, 319.907, 348.046, 369.832, 373.876 (359.651)	KLD: 19.380, 15.812, 15.208, 14.523, 14.010, 13.334, 13.559, 13.066, 13.022, 13.319, 13.219, 13.543, 13.294, 15.100, 15.195, 14.717 (230.204)	Grad: 284865.844
[Epoch   6 (267.68s)]	ELBO: -11637.512, -7467.664, -5196.292, -3767.457, -2818.515, -2110.601, -1520.201, -1033.458, -628.945, -300.493, -20.844, 70.699, 147.622, 161.220, 168.191, 158.104 (156.370)	Log prob: -11618.301, -7432.622, -5146.033, -3702.672, -2739.745, -2018.535, -1414.591, -914.811, -497.320, -155.615, 137.239, 242.239, 332.393, 361.029, 383.062, 387.638 (387.147)	KLD: 19.223, 15.820, 15.217, 14.524, 13.988, 13.294, 13.546, 13.035, 12.979, 13.253, 13.205, 13.458, 13.231, 15.037, 15.062, 14.663 (230.778)	Grad: 280477.000
[Epoch   7 (277.99s)]	ELBO: -11645.979, -7458.147, -5181.690, -3755.303, -2808.789, -2101.554, -1512.782, -1026.695, -622.858, -294.898, -14.147, 82.543, 166.153, 180.244, 187.443, 178.804 (164.953)	Log prob: -11626.803, -7423.161, -5131.489, -3690.580, -2730.053, -2009.561, -1407.283, -908.158, -491.337, -150.110, 143.763, 253.830, 350.579, 379.629, 401.845, 407.698 (395.084)	KLD: 19.179, 15.804, 15.213, 14.526, 14.014, 13.256, 13.506, 13.037, 12.984, 13.267, 13.123, 13.376, 13.139, 14.960, 15.017, 14.492 (230.130)	Grad: 298242.719
[Epoch   8 (290.01s)]	ELBO: -11650.108, -7468.677, -5180.325, -3754.083, -2805.765, -2098.118, -1509.559, -1023.371, -619.278, -290.928, -7.607, 91.579, 180.434, 194.479, 201.407, 194.163 (174.743)	Log prob: -11631.021, -7433.778, -5130.171, -3689.424, -2727.096, -2006.224, -1404.185, -904.988, -487.942, -146.418, 150.016, 262.465, 364.413, 393.344, 415.251, 422.306 (399.902)	KLD: 19.085, 15.813, 15.259, 14.500, 14.009, 13.226, 13.481, 13.008, 12.953, 13.174, 13.113, 13.263, 13.092, 14.887, 14.978, 14.299 (225.159)	Grad: 295934.594
[Epoch   9 (291.99s)]	ELBO: -11616.807, -7457.287, -5186.938, -3758.739, -2811.083, -2101.990, -1512.887, -1026.303, -622.280, -293.604, -10.791, 93.302, 188.231, 202.907, 209.989, 203.970 (187.402)	Log prob: -11597.714, -7422.408, -5136.828, -3694.146, -2732.467, -2010.209, -1407.671, -908.130, -491.228, -149.384, 146.481, 263.663, 371.737, 401.188, 423.176, 431.439 (415.229)	KLD: 19.102, 15.776, 15.230, 14.487, 14.022, 13.164, 13.436, 12.956, 12.880, 13.167, 13.053, 13.089, 13.146, 14.775, 14.905, 14.283 (227.828)	Grad: 298214.781
[Epoch  10 (287.29s)]	ELBO: -11677.021, -7497.522, -5197.611, -3765.675, -2816.169, -2107.256, -1518.569, -1032.570, -628.143, -299.448, -15.190, 95.449, 196.685, 212.824, 221.099, 217.315 (190.661)	Log prob: -11657.935, -7462.628, -5147.482, -3701.037, -2737.525, -2015.451, -1413.314, -914.379, -497.099, -155.250, 142.031, 265.791, 380.117, 411.006, 434.092, 444.419 (419.936)	KLD: 19.089, 15.806, 15.234, 14.511, 14.004, 13.158, 13.451, 12.936, 12.854, 13.153, 13.023, 13.122, 13.090, 14.749, 14.811, 14.111 (229.276)	Grad: 341212.562
[Epoch  11 (290.69s)]	ELBO: -11674.890, -7505.382, -5192.021, -3758.661, -2811.932, -2102.334, -1512.953, -1027.227, -622.508, -293.345, -7.494, 105.118, 210.801, 226.277, 234.218, 231.740 (224.495)	Log prob: -11655.796, -7470.455, -5141.900, -3694.029, -2733.334, -2010.648, -1407.880, -909.253, -491.674, -149.420, 149.416, 275.018, 393.798, 423.949, 446.638, 458.294 (450.036)	KLD: 19.095, 15.831, 15.197, 14.511, 13.963, 13.090, 13.387, 12.902, 12.859, 13.091, 12.984, 12.991, 13.096, 14.676, 14.747, 14.134 (225.541)	Grad: 304079.719
[Epoch  12 (285.13s)]	ELBO: -11685.233, -7501.218, -5195.339, -3761.605, -2814.326, -2105.023, -1514.267, -1027.344, -622.611, -293.277, -7.148, 111.237, 220.347, 236.505, 244.442, 242.594 (241.780)	Log prob: -11666.124, -7466.234, -5145.167, -3696.951, -2735.730, -2013.365, -1409.259, -909.444, -491.912, -149.528, 149.551, 280.908, 403.112, 433.781, 456.383, 468.582 (466.266)	KLD: 19.113, 15.872, 15.188, 14.476, 13.946, 13.063, 13.351, 12.891, 12.799, 13.049, 12.950, 12.972, 13.094, 14.511, 14.665, 14.047 (224.487)	Grad: 310517.969
[Epoch  13 (285.90s)]	ELBO: -11682.501, -7513.830, -5215.758, -3783.778, -2834.596, -2123.691, -1531.069, -1043.775, -638.468, -308.951, -22.508, 102.539, 216.339, 234.678, 244.684, 244.124 (234.601)	Log prob: -11663.482, -7478.959, -5165.685, -3719.198, -2756.077, -2032.066, -1426.067, -925.892, -507.762, -165.193, 134.191, 272.147, 399.107, 431.867, 456.481, 469.809 (460.223)	KLD: 19.022, 15.848, 15.206, 14.502, 13.941, 13.105, 13.377, 12.881, 12.822, 13.052, 12.942, 12.909, 13.160, 14.420, 14.608, 13.889 (225.622)	Grad: 327838.938
[Epoch  14 (286.79s)]	ELBO: -11671.003, -7510.357, -5204.278, -3771.180, -2822.914, -2111.061, -1520.037, -1032.783, -626.780, -296.899, -9.587, 118.664, 235.552, 252.362, 261.004, 261.018 (241.638)	Log prob: -11651.914, -7475.453, -5154.179, -3706.614, -2744.398, -2019.502, -1415.137, -915.040, -496.259, -153.374, 146.837, 287.937, 418.021, 449.212, 472.415, 486.270 (463.726)	KLD: 19.087, 15.821, 15.193, 14.467, 13.951, 13.040, 13.343, 12.843, 12.778, 13.003, 12.899, 12.849, 13.196, 14.380, 14.561, 13.842 (222.088)	Grad: 297761.250
[Epoch  15 (262.14s)]	ELBO: -11619.376, -7498.061, -5218.653, -3782.476, -2832.454, -2120.510, -1529.959, -1041.925, -635.424, -305.832, -18.702, 117.816, 238.117, 257.257, 267.791, 268.037 (244.934)	Log prob: -11600.329, -7463.190, -5168.597, -3717.961, -2754.018, -2028.994, -1425.093, -924.204, -504.938, -162.352, 137.653, 286.974, 420.462, 453.879, 478.876, 492.923 (471.976)	KLD: 19.048, 15.821, 15.184, 14.461, 13.924, 13.077, 13.350, 12.855, 12.766, 12.993, 12.876, 12.803, 13.188, 14.277, 14.463, 13.801 (227.043)	Grad: 325708.094
[Epoch  16 (265.67s)]	ELBO: -11601.826, -7490.995, -5208.262, -3774.829, -2826.817, -2113.313, -1522.595, -1033.855, -627.628, -296.471, -7.762, 130.211, 251.048, 268.877, 278.815, 279.876 (214.955)	Log prob: -11582.654, -7455.996, -5158.057, -3710.242, -2748.319, -2021.876, -1417.855, -916.330, -497.410, -153.289, 148.274, 298.970, 432.895, 464.863, 489.166, 503.915 (439.731)	KLD: 19.170, 15.825, 15.212, 14.379, 13.910, 12.942, 13.301, 12.787, 12.691, 12.965, 12.853, 12.723, 13.089, 14.139, 14.365, 13.688 (224.775)	Grad: 297685.156
[Epoch  17 (267.46s)]	ELBO: -11621.389, -7487.771, -5218.297, -3786.286, -2835.828, -2123.247, -1530.117, -1042.888, -636.775, -306.388, -18.307, 127.483, 249.688, 270.034, 281.516, 283.453 (225.162)	Log prob: -11602.217, -7452.820, -5168.150, -3721.761, -2757.374, -2031.832, -1425.420, -925.375, -506.566, -163.227, 137.667, 296.221, 431.472, 465.916, 491.655, 507.239 (449.842)	KLD: 19.176, 15.781, 15.192, 14.378, 13.929, 12.961, 13.282, 12.816, 12.697, 12.951, 12.814, 12.764, 13.046, 14.098, 14.257, 13.647 (224.680)	Grad: 324026.375
[Epoch  18 (260.43s)]	ELBO: -11624.971, -7476.661, -5207.338, -3774.282, -2823.386, -2110.906, -1518.873, -1030.636, -624.259, -293.375, -4.596, 142.142, 263.070, 282.483, 293.284, 295.366 (286.189)	Log prob: -11605.829, -7441.722, -5157.201, -3709.673, -2744.904, -2019.490, -1414.168, -913.137, -494.115, -150.366, 151.170, 310.627, 444.557, 477.946, 502.938, 518.508 (509.067)	KLD: 19.147, 15.797, 15.194, 14.472, 13.875, 12.932, 13.289, 12.793, 12.646, 12.864, 12.757, 12.719, 13.002, 13.976, 14.191, 13.489 (222.879)	Grad: 294674.938
[Epoch  19 (261.58s)]	ELBO: -11666.328, -7471.847, -5205.962, -3776.066, -2824.448, -2110.198, -1516.590, -1028.871, -621.827, -290.589, 0.067, 148.971, 270.835, 289.993, 301.143, 303.762 (296.889)	Log prob: -11647.180, -7436.854, -5155.793, -3711.477, -2746.015, -2018.882, -1412.034, -911.544, -491.835, -147.765, 155.641, 317.175, 452.015, 484.985, 510.174, 526.101 (519.808)	KLD: 19.148, 15.840, 15.179, 14.422, 13.844, 12.884, 13.240, 12.771, 12.664, 12.832, 12.750, 12.631, 12.975, 13.813, 14.039, 13.308 (222.919)	Grad: 303502.094
[Epoch  20 (260.00s)]	ELBO: -11687.847, -7495.270, -5207.026, -3773.295, -2824.279, -2111.996, -1520.430, -1032.580, -626.201, -295.444, -5.353, 148.053, 268.611, 288.789, 301.110, 304.819 (253.218)	Log prob: -11668.783, -7460.392, -5156.951, -3708.803, -2745.913, -2020.708, -1415.899, -915.285, -496.218, -152.691, 150.123, 316.293, 449.800, 483.700, 509.962, 527.026 (474.634)	KLD: 19.057, 15.822, 15.198, 14.416, 13.873, 12.923, 13.243, 12.764, 12.687, 12.771, 12.723, 12.764, 12.950, 13.721, 13.941, 13.356 (221.416)	Grad: 295198.438
[Epoch  21 (257.55s)]	ELBO: -11620.086, -7497.652, -5217.465, -3781.533, -2830.964, -2119.461, -1527.686, -1039.296, -633.127, -301.468, -11.977, 149.774, 272.505, 294.303, 307.632, 312.328 (300.546)	Log prob: -11601.021, -7462.791, -5167.416, -3717.123, -2752.660, -2028.242, -1423.252, -922.148, -503.378, -158.863, 143.267, 317.790, 453.468, 488.942, 516.083, 533.979 (523.009)	KLD: 19.071, 15.787, 15.191, 14.363, 13.892, 12.917, 13.214, 12.714, 12.601, 12.856, 12.639, 12.771, 12.947, 13.676, 13.812, 13.200 (222.463)	Grad: 317630.188
[Epoch  22 (262.11s)]	ELBO: -11625.029, -7494.082, -5204.088, -3765.484, -2814.657, -2102.978, -1510.872, -1023.301, -616.663, -285.404, 5.415, 168.207, 287.296, 307.601, 320.780, 324.713 (283.927)	Log prob: -11605.925, -7459.201, -5153.995, -3701.005, -2736.368, -2011.880, -1406.613, -906.342, -487.119, -143.101, 160.292, 335.768, 467.735, 501.551, 528.382, 545.369 (505.686)	KLD: 19.103, 15.776, 15.214, 14.386, 13.810, 12.811, 13.160, 12.700, 12.586, 12.758, 12.574, 12.684, 12.878, 13.511, 13.652, 13.055 (221.759)	Grad: 305303.344
[Epoch  23 (279.72s)]	ELBO: -11667.700, -7543.853, -5249.476, -3816.370, -2864.308, -2147.799, -1552.371, -1063.443, -656.360, -326.092, -35.905, 135.355, 258.431, 284.358, 300.212, 306.041 (287.330)	Log prob: -11648.628, -7508.975, -5199.418, -3751.927, -2786.071, -2056.711, -1448.102, -946.470, -526.804, -183.709, 119.096, 303.079, 439.055, 478.376, 507.802, 526.690 (508.933)	KLD: 19.079, 15.799, 15.177, 14.386, 13.796, 12.850, 13.180, 12.706, 12.583, 12.827, 12.617, 12.722, 12.900, 13.395, 13.572, 13.059 (221.603)	Grad: 359975.625
[Epoch  24 (282.67s)]	ELBO: -11611.499, -7472.577, -5203.853, -3773.354, -2822.779, -2110.029, -1515.889, -1027.926, -621.006, -289.700, 1.385, 171.321, 288.093, 311.828, 325.598, 331.543 (291.821)	Log prob: -11592.422, -7437.751, -5153.872, -3708.977, -2744.578, -2019.041, -1411.721, -911.073, -491.589, -147.481, 156.176, 338.742, 468.342, 505.352, 532.561, 551.515 (510.485)	KLD: 19.075, 15.755, 15.154, 14.391, 13.825, 12.787, 13.180, 12.686, 12.565, 12.801, 12.572, 12.630, 12.827, 13.276, 13.438, 13.009 (218.663)	Grad: 306518.438
[Epoch  25 (277.11s)]	ELBO: -11622.835, -7470.755, -5214.966, -3781.629, -2830.820, -2117.390, -1523.624, -1035.172, -627.553, -295.732, -4.182, 170.717, 289.051, 312.801, 327.096, 332.639 (315.372)	Log prob: -11603.649, -7435.738, -5164.790, -3717.085, -2752.499, -2026.295, -1419.412, -918.290, -498.156, -153.614, 150.482, 337.887, 469.042, 505.948, 533.490, 551.979 (531.673)	KLD: 19.188, 15.829, 15.162, 14.369, 13.773, 12.774, 13.118, 12.669, 12.516, 12.720, 12.546, 12.507, 12.820, 13.156, 13.248, 12.945 (216.301)	Grad: 306341.938
[Epoch  26 (272.47s)]	ELBO: -11651.643, -7492.057, -5220.859, -3784.658, -2831.723, -2117.768, -1521.770, -1032.783, -625.167, -292.994, -0.923, 178.063, 293.024, 316.750, 330.046, 335.623 (324.878)	Log prob: -11632.479, -7457.071, -5170.697, -3720.129, -2753.376, -2026.650, -1417.553, -915.919, -495.802, -150.927, 153.651, 345.223, 472.945, 509.751, 536.128, 554.591 (541.381)	KLD: 19.165, 15.824, 15.175, 14.364, 13.818, 12.771, 13.101, 12.645, 12.501, 12.702, 12.508, 12.585, 12.761, 13.081, 13.080, 12.887 (216.502)	Grad: 331955.562
[Epoch  27 (269.14s)]	ELBO: -11616.165, -7482.242, -5221.064, -3784.693, -2831.136, -2115.115, -1519.571, -1030.335, -622.869, -291.414, 0.410, 183.070, 295.577, 320.009, 333.825, 340.042 (274.265)	Log prob: -11597.012, -7447.271, -5170.894, -3720.191, -2752.835, -2024.050, -1415.410, -913.543, -493.590, -149.423, 154.911, 350.092, 475.268, 512.656, 539.440, 558.410 (496.922)	KLD: 19.154, 15.819, 15.196, 14.332, 13.800, 12.763, 13.097, 12.630, 12.488, 12.711, 12.510, 12.521, 12.670, 12.955, 12.969, 12.753 (222.656)	Grad: 342556.250
[Epoch  28 (269.68s)]	ELBO: -11669.119, -7491.499, -5213.811, -3778.424, -2826.633, -2112.449, -1516.966, -1029.026, -621.301, -288.934, 3.518, 189.728, 300.655, 324.318, 338.336, 344.608 (319.510)	Log prob: -11649.978, -7456.534, -5163.663, -3713.909, -2748.282, -2021.376, -1412.754, -912.268, -492.100, -147.025, 157.869, 356.605, 480.156, 516.690, 543.469, 562.411 (537.635)	KLD: 19.142, 15.826, 15.179, 14.369, 13.835, 12.721, 13.140, 12.546, 12.443, 12.709, 12.441, 12.527, 12.624, 12.870, 12.763, 12.669 (218.125)	Grad: 331423.625
[Epoch  29 (272.72s)]	ELBO: -11788.636, -7636.717, -5336.177, -3891.394, -2932.417, -2215.719, -1618.277, -1129.186, -720.972, -390.502, -100.594, 109.274, 242.617, 277.104, 294.789, 304.102 (325.846)	Log prob: -11769.456, -7601.792, -5286.018, -3826.894, -2854.083, -2124.556, -1513.942, -1012.227, -591.477, -248.233, 54.084, 276.914, 423.005, 470.387, 500.869, 522.859 (545.042)	KLD: 19.168, 15.757, 15.234, 14.339, 13.836, 12.830, 13.171, 12.624, 12.535, 12.775, 12.408, 12.962, 12.749, 12.894, 12.797, 12.678 (219.196)	Grad: 432854.281
[Epoch  30 (271.50s)]	ELBO: -11670.828, -7479.714, -5211.991, -3776.965, -2819.947, -2103.354, -1506.115, -1018.100, -610.051, -277.691, 15.383, 201.818, 312.666, 337.370, 351.035, 357.446 (349.616)	Log prob: -11651.708, -7444.730, -5161.871, -3712.451, -2741.615, -2012.332, -1401.998, -901.411, -480.906, -135.851, 169.604, 368.878, 492.339, 529.813, 556.179, 575.254 (566.385)	KLD: 19.127, 15.860, 15.137, 14.391, 13.817, 12.690, 13.094, 12.572, 12.456, 12.696, 12.382, 12.838, 12.613, 12.770, 12.701, 12.665 (216.769)	Grad: 301167.906
[Epoch  31 (267.21s)]	ELBO: -11692.433, -7487.341, -5220.705, -3782.805, -2825.069, -2109.942, -1514.701, -1024.901, -617.078, -283.705, 10.151, 204.194, 309.453, 333.969, 348.009, 355.008 (340.025)	Log prob: -11673.375, -7452.408, -5170.606, -3718.352, -2746.857, -2019.007, -1410.694, -908.343, -488.046, -142.046, 164.173, 370.873, 488.709, 525.870, 552.400, 571.976 (557.861)	KLD: 19.054, 15.878, 15.166, 14.355, 13.757, 12.727, 13.070, 12.551, 12.474, 12.627, 12.362, 12.657, 12.577, 12.644, 12.491, 12.576 (217.836)	Grad: 288737.906
[Epoch  32 (268.49s)]	ELBO: -11783.432, -7524.464, -5203.320, -3764.620, -2809.958, -2096.849, -1501.816, -1013.342, -605.687, -272.663, 20.670, 218.865, 320.630, 344.721, 359.327, 366.591 (333.219)	Log prob: -11764.408, -7489.533, -5153.252, -3700.213, -2731.761, -2005.968, -1397.880, -896.908, -476.851, -131.227, 174.430, 385.246, 499.504, 536.143, 563.100, 582.869 (547.452)	KLD: 19.020, 15.909, 15.136, 14.339, 13.794, 12.683, 13.054, 12.499, 12.402, 12.600, 12.324, 12.621, 12.493, 12.548, 12.350, 12.506 (214.233)	Grad: 306573.594
[Epoch  33 (272.54s)]	ELBO: -11768.179, -7517.263, -5195.320, -3756.804, -2803.728, -2089.223, -1493.933, -1005.849, -598.288, -265.308, 28.328, 228.859, 327.760, 352.306, 366.635, 373.911 (346.329)	Log prob: -11749.183, -7482.371, -5145.269, -3692.408, -2725.596, -1998.424, -1390.157, -889.615, -469.670, -124.117, 181.816, 394.834, 506.167, 543.125, 569.691, 589.407 (561.864)	KLD: 18.991, 15.900, 15.157, 14.347, 13.738, 12.667, 12.975, 12.458, 12.386, 12.572, 12.297, 12.487, 12.432, 12.412, 12.238, 12.440 (215.535)	Grad: 299679.594
[Epoch  34 (277.06s)]	ELBO: -11679.514, -7483.321, -5197.400, -3761.315, -2807.377, -2093.969, -1499.285, -1011.182, -603.554, -270.424, 22.392, 227.015, 327.011, 354.003, 369.181, 376.797 (336.367)	Log prob: -11660.499, -7448.433, -5147.350, -3696.921, -2729.237, -2003.185, -1395.472, -894.931, -474.959, -129.273, 175.790, 392.893, 505.258, 544.596, 571.943, 591.964 (550.264)	KLD: 19.015, 15.870, 15.166, 14.345, 13.745, 12.643, 13.029, 12.439, 12.344, 12.556, 12.247, 12.480, 12.369, 12.345, 12.170, 12.405 (213.897)	Grad: 303190.188
[Epoch  35 (272.32s)]	ELBO: -11691.472, -7501.355, -5212.287, -3772.985, -2818.979, -2103.797, -1508.310, -1020.052, -612.246, -279.832, 13.928, 222.285, 321.853, 349.716, 364.882, 372.595 (329.691)	Log prob: -11672.476, -7466.471, -5162.264, -3708.625, -2740.867, -2013.057, -1404.578, -903.889, -483.770, -138.799, 167.165, 387.977, 499.808, 539.969, 567.136, 587.137 (541.650)	KLD: 18.998, 15.884, 15.138, 14.337, 13.752, 12.631, 12.993, 12.430, 12.313, 12.557, 12.204, 12.454, 12.264, 12.298, 12.001, 12.288 (211.959)	Grad: 323574.781
[Epoch  36 (272.33s)]	ELBO: -11654.800, -7501.179, -5238.578, -3798.427, -2844.521, -2128.141, -1531.188, -1042.253, -632.508, -299.932, -6.964, 208.530, 313.307, 346.068, 362.711, 371.767 (341.934)	Log prob: -11635.735, -7466.230, -5188.483, -3734.025, -2766.409, -2037.413, -1427.470, -926.122, -504.105, -158.959, 146.195, 374.065, 491.080, 536.040, 564.696, 585.949 (558.172)	KLD: 19.070, 15.876, 15.148, 14.308, 13.709, 12.614, 12.993, 12.414, 12.271, 12.570, 12.186, 12.376, 12.238, 12.199, 12.013, 12.196 (216.238)	Grad: 369833.344
[Epoch  37 (268.22s)]	ELBO: -11711.976, -7570.863, -5273.814, -3828.168, -2872.417, -2153.265, -1554.326, -1063.761, -654.000, -321.353, -29.439, 191.189, 301.659, 339.768, 358.922, 368.156 (348.410)	Log prob: -11692.697, -7535.766, -5223.545, -3763.574, -2794.055, -2062.277, -1450.271, -947.301, -525.250, -179.980, 124.076, 357.116, 479.806, 530.015, 561.201, 582.475 (563.225)	KLD: 19.284, 15.815, 15.167, 14.326, 13.771, 12.625, 13.066, 12.405, 12.290, 12.623, 12.142, 12.413, 12.220, 12.100, 12.031, 12.040 (214.815)	Grad: 405725.562
[Epoch  38 (269.58s)]	ELBO: -11749.271, -7615.902, -5278.937, -3829.001, -2869.620, -2145.739, -1542.475, -1049.522, -639.108, -304.745, -10.431, 210.773, 318.711, 353.809, 371.523, 380.294 (351.364)	Log prob: -11729.945, -7580.768, -5228.576, -3764.287, -2791.157, -2054.684, -1438.364, -933.016, -510.351, -163.359, 143.123, 376.685, 496.785, 544.018, 573.542, 594.219 (564.160)	KLD: 19.328, 15.807, 15.225, 14.353, 13.749, 12.593, 13.056, 12.396, 12.250, 12.628, 12.168, 12.358, 12.161, 12.136, 11.810, 11.906 (212.796)	Grad: 420767.062
[Epoch  39 (270.02s)]	ELBO: -11645.168, -7507.696, -5238.437, -3802.073, -2847.423, -2128.323, -1530.594, -1039.698, -630.775, -297.406, -3.522, 221.758, 328.160, 359.449, 377.264, 385.970 (250.319)	Log prob: -11626.090, -7472.823, -5188.449, -3737.718, -2769.265, -2037.579, -1426.793, -923.510, -502.328, -156.357, 149.676, 387.292, 505.831, 549.157, 578.798, 599.534 (469.261)	KLD: 19.081, 15.790, 15.116, 14.369, 13.802, 12.585, 13.056, 12.389, 12.258, 12.601, 12.149, 12.335, 12.138, 12.037, 11.826, 12.029 (218.941)	Grad: 343182.062
[Epoch  40 (272.57s)]	ELBO: -11627.876, -7481.891, -5223.114, -3786.673, -2832.327, -2116.071, -1519.388, -1030.094, -621.459, -288.189, 5.248, 231.890, 340.351, 372.325, 389.873, 398.606 (395.815)	Log prob: -11608.818, -7446.992, -5173.083, -3722.308, -2754.267, -2025.446, -1415.741, -914.094, -493.281, -147.441, 158.117, 397.121, 517.767, 561.930, 591.426, 612.184 (609.590)	KLD: 19.067, 15.833, 15.130, 14.333, 13.697, 12.567, 13.021, 12.353, 12.178, 12.570, 12.121, 12.363, 12.184, 12.188, 11.948, 12.025 (213.775)	Grad: 341979.000
[Epoch  41 (268.86s)]	ELBO: -11653.935, -7540.628, -5252.690, -3810.990, -2857.348, -2142.906, -1546.294, -1056.955, -647.822, -314.622, -21.429, 211.767, 325.036, 359.907, 382.114, 390.934 (380.491)	Log prob: -11634.851, -7505.807, -5202.648, -3746.674, -2779.281, -2052.134, -1442.491, -940.675, -519.171, -173.332, 132.067, 377.695, 502.999, 549.770, 583.748, 604.335 (592.857)	KLD: 19.085, 15.737, 15.220, 14.275, 13.746, 12.709, 13.030, 12.478, 12.370, 12.639, 12.207, 12.432, 12.034, 11.901, 11.770, 11.768 (212.366)	Grad: 356747.219
[Epoch  42 (275.04s)]	ELBO: -11594.269, -7465.507, -5215.062, -3775.159, -2819.288, -2100.552, -1503.676, -1013.668, -603.360, -269.502, 25.226, 258.671, 361.443, 389.433, 406.330, 414.450 (378.530)	Log prob: -11575.189, -7430.597, -5164.999, -3710.731, -2741.161, -2009.900, -1400.083, -897.767, -475.234, -128.841, 177.953, 423.686, 538.428, 578.148, 606.667, 626.448 (590.846)	KLD: 19.078, 15.838, 15.147, 14.365, 13.699, 12.524, 12.941, 12.309, 12.225, 12.535, 12.066, 12.288, 11.971, 11.729, 11.623, 11.660 (212.315)	Grad: 313123.625
[Epoch  43 (277.93s)]	ELBO: -11661.807, -7469.602, -5214.934, -3778.135, -2820.358, -2100.676, -1503.177, -1014.522, -605.130, -271.445, 22.708, 259.214, 363.479, 391.299, 407.984, 416.401 (398.476)	Log prob: -11642.846, -7434.821, -5164.978, -3713.844, -2742.354, -2010.190, -1399.780, -898.838, -477.250, -131.071, 175.122, 423.888, 540.191, 579.611, 607.831, 627.886 (610.456)	KLD: 18.967, 15.815, 15.172, 14.336, 13.714, 12.483, 12.909, 12.288, 12.196, 12.494, 12.040, 12.260, 12.038, 11.600, 11.535, 11.637 (211.980)	Grad: 325476.812
[Epoch  44 (283.18s)]	ELBO: -11615.983, -7446.286, -5200.063, -3763.731, -2808.655, -2092.228, -1494.821, -1006.763, -597.844, -263.564, 30.543, 269.401, 373.304, 400.898, 418.185, 426.868 (389.507)	Log prob: -11596.972, -7411.425, -5150.114, -3699.414, -2730.599, -2001.666, -1391.329, -890.972, -469.868, -123.140, 183.004, 434.170, 549.993, 589.127, 617.899, 638.176 (599.560)	KLD: 19.011, 15.853, 15.085, 14.368, 13.737, 12.507, 12.930, 12.300, 12.185, 12.448, 12.036, 12.309, 11.920, 11.540, 11.486, 11.594 (210.053)	Grad: 312321.594
[Epoch  45 (285.99s)]	ELBO: -11624.551, -7512.938, -5241.116, -3797.347, -2840.789, -2123.307, -1526.203, -1036.524, -627.643, -293.669, -0.426, 239.514, 352.420, 386.789, 406.195, 416.318 (397.286)	Log prob: -11605.353, -7477.937, -5190.996, -3732.895, -2762.614, -2032.593, -1422.571, -920.608, -499.576, -153.126, 152.120, 404.378, 529.143, 574.958, 605.831, 627.522 (607.889)	KLD: 19.197, 15.804, 15.120, 14.330, 13.724, 12.538, 12.919, 12.283, 12.151, 12.476, 12.004, 12.316, 11.860, 11.446, 11.466, 11.570 (210.603)	Grad: 369588.906
[Epoch  46 (283.32s)]	ELBO: -11597.110, -7509.805, -5237.431, -3794.194, -2836.166, -2117.005, -1518.100, -1027.983, -618.143, -283.153, 11.014, 252.487, 363.259, 394.841, 413.561, 423.042 (398.588)	Log prob: -11577.846, -7474.744, -5187.270, -3729.689, -2757.978, -2026.342, -1414.491, -912.103, -490.136, -142.669, 163.498, 417.253, 539.872, 582.822, 612.956, 634.015 (610.841)	KLD: 19.266, 15.797, 15.098, 14.346, 13.682, 12.475, 12.947, 12.270, 12.126, 12.477, 12.000, 12.282, 11.847, 11.368, 11.414, 11.578 (212.253)	Grad: 349344.625
[Epoch  47 (270.65s)]	ELBO: -11571.744, -7458.129, -5207.887, -3769.377, -2811.320, -2095.564, -1499.153, -1009.917, -600.361, -265.905, 28.707, 271.809, 379.459, 408.893, 427.280, 436.997 (418.697)	Log prob: -11552.624, -7423.186, -5157.819, -3704.997, -2733.257, -2005.044, -1395.732, -894.264, -472.603, -125.724, 180.821, 436.227, 555.673, 596.430, 626.179, 647.358 (628.474)	KLD: 19.124, 15.822, 15.125, 14.309, 13.683, 12.457, 12.901, 12.232, 12.104, 12.423, 11.934, 12.304, 11.795, 11.324, 11.362, 11.463 (209.777)	Grad: 292728.562
[Epoch  48 (263.58s)]	ELBO: -11590.510, -7476.810, -5234.660, -3794.398, -2835.768, -2119.253, -1520.904, -1030.939, -620.962, -286.318, 8.091, 250.839, 365.024, 398.264, 420.025, 430.727 (394.715)	Log prob: -11571.397, -7441.915, -5184.643, -3730.094, -2757.781, -2028.814, -1417.531, -915.311, -493.219, -146.138, 160.227, 415.274, 541.238, 585.781, 618.927, 641.064 (607.590)	KLD: 19.113, 15.782, 15.120, 14.292, 13.681, 12.453, 12.932, 12.255, 12.115, 12.437, 11.955, 12.299, 11.780, 11.303, 11.386, 11.433 (212.875)	Grad: 364882.719
[Epoch  49 (262.19s)]	ELBO: -11634.667, -7474.257, -5222.029, -3783.836, -2825.174, -2111.734, -1514.658, -1024.908, -615.505, -280.743, 14.047, 258.669, 373.400, 406.239, 426.556, 436.278 (428.414)	Log prob: -11615.472, -7439.193, -5171.832, -3719.363, -2747.012, -2021.087, -1411.098, -909.107, -487.626, -140.423, 166.296, 423.223, 549.713, 593.792, 625.435, 646.541 (635.466)	KLD: 19.200, 15.865, 15.133, 14.275, 13.688, 12.485, 12.914, 12.240, 12.078, 12.441, 11.929, 12.305, 11.759, 11.240, 11.325, 11.385 (207.052)	Grad: 368599.344
[Epoch  50 (261.61s)]	ELBO: -11605.317, -7468.896, -5222.754, -3785.351, -2828.112, -2113.107, -1515.784, -1026.538, -617.107, -283.414, 10.619, 254.869, 372.428, 406.565, 429.372, 440.390 (425.437)	Log prob: -11586.223, -7433.967, -5172.702, -3721.032, -2750.122, -2022.674, -1412.429, -910.958, -489.452, -143.332, 162.649, 419.175, 548.443, 593.877, 628.008, 650.327 (635.927)	KLD: 19.090, 15.836, 15.123, 14.269, 13.673, 12.443, 12.921, 12.225, 12.074, 12.427, 11.947, 12.277, 11.709, 11.297, 11.324, 11.301 (210.491)	Grad: 369360.688
[Epoch  51 (259.71s)]	ELBO: -11652.408, -7459.104, -5204.493, -3767.101, -2809.138, -2093.451, -1495.223, -1005.891, -596.669, -260.886, 34.126, 280.894, 396.676, 426.783, 446.365, 456.967 (425.627)	Log prob: -11633.419, -7424.264, -5154.533, -3702.805, -2731.185, -2003.074, -1391.994, -890.428, -469.112, -120.982, 185.935, 444.971, 572.419, 613.665, 644.438, 666.313 (634.593)	KLD: 18.984, 15.861, 15.115, 14.337, 13.656, 12.426, 12.851, 12.233, 12.095, 12.346, 11.906, 12.267, 11.666, 11.139, 11.192, 11.273 (208.966)	Grad: 320364.094
[Epoch  52 (263.31s)]	ELBO: -11692.858, -7531.360, -5254.014, -3814.638, -2854.155, -2136.609, -1538.429, -1048.067, -638.426, -303.287, -9.204, 237.351, 370.105, 407.322, 430.246, 442.113 (373.251)	Log prob: -11673.930, -7496.602, -5204.141, -3750.518, -2776.377, -2046.392, -1435.306, -932.710, -510.984, -163.447, 142.549, 401.401, 545.798, 594.043, 628.137, 651.193 (582.207)	KLD: 18.927, 15.834, 15.111, 14.245, 13.659, 12.440, 12.907, 12.234, 12.084, 12.399, 11.913, 12.297, 11.643, 11.029, 11.169, 11.189 (208.956)	Grad: 366299.531
[Epoch  53 (265.11s)]	ELBO: -11766.261, -7664.830, -5334.689, -3887.593, -2923.239, -2203.520, -1600.232, -1108.807, -698.032, -364.157, -71.724, 174.961, 325.096, 371.544, 402.043, 416.033 (331.428)	Log prob: -11746.950, -7629.722, -5284.459, -3823.131, -2845.135, -2112.945, -1496.696, -992.968, -570.039, -223.704, 80.719, 339.717, 501.703, 559.467, 601.367, 626.747 (551.728)	KLD: 19.315, 15.791, 15.122, 14.233, 13.642, 12.471, 12.960, 12.304, 12.154, 12.460, 11.990, 12.314, 11.852, 11.316, 11.401, 11.389 (220.300)	Grad: 458830.312
[Epoch  54 (264.97s)]	ELBO: -11695.035, -7535.852, -5225.117, -3786.416, -2825.527, -2107.967, -1508.567, -1018.239, -608.238, -272.959, 22.951, 269.873, 396.507, 432.377, 455.029, 467.021 (450.339)	Log prob: -11676.012, -7500.970, -5175.071, -3722.146, -2747.560, -2017.491, -1405.155, -902.580, -480.490, -132.799, 175.091, 434.241, 572.661, 619.800, 653.672, 676.967 (660.173)	KLD: 19.024, 15.858, 15.163, 14.226, 13.698, 12.508, 12.935, 12.247, 12.090, 12.412, 11.980, 12.229, 11.786, 11.269, 11.219, 11.303 (209.834)	Grad: 426159.344
[Epoch  55 (266.10s)]	ELBO: -11576.492, -7426.912, -5190.226, -3756.503, -2799.521, -2083.363, -1484.899, -995.143, -585.099, -249.557, 45.797, 294.286, 418.932, 451.799, 472.496, 484.382 (442.829)	Log prob: -11557.507, -7392.077, -5140.282, -3692.300, -2721.664, -1993.053, -1381.773, -879.816, -457.700, -109.838, 197.448, 458.116, 594.423, 638.287, 669.981, 692.953 (652.542)	KLD: 18.993, 15.840, 15.110, 14.261, 13.654, 12.452, 12.816, 12.202, 12.070, 12.320, 11.933, 12.179, 11.661, 10.997, 10.998, 11.085 (209.712)	Grad: 296223.531
[Epoch  56 (259.52s)]	ELBO: -11581.199, -7438.747, -5208.842, -3774.254, -2817.109, -2101.126, -1502.490, -1011.209, -600.746, -265.243, 30.476, 279.110, 408.271, 443.919, 465.319, 478.077 (460.389)	Log prob: -11562.161, -7403.825, -5158.830, -3709.962, -2739.245, -2010.803, -1399.347, -895.900, -473.407, -125.562, 181.988, 442.757, 583.549, 630.198, 662.513, 686.350 (668.453)	KLD: 19.034, 15.888, 15.090, 14.281, 13.572, 12.458, 12.820, 12.167, 12.030, 12.341, 11.832, 12.135, 11.630, 11.002, 10.915, 11.079 (208.064)	Grad: 334653.250
[Epoch  57 (259.82s)]	ELBO: -11558.473, -7430.650, -5215.706, -3777.085, -2817.988, -2099.717, -1499.375, -1009.481, -600.529, -264.906, 29.958, 278.266, 413.346, 451.952, 475.195, 488.188 (448.641)	Log prob: -11539.331, -7395.685, -5165.652, -3712.768, -2740.041, -2009.328, -1396.148, -894.112, -473.175, -125.235, 181.471, 441.866, 588.517, 638.050, 672.318, 696.449 (656.386)	KLD: 19.138, 15.822, 15.093, 14.265, 13.627, 12.445, 12.837, 12.143, 11.984, 12.318, 11.842, 12.086, 11.572, 10.927, 11.025, 11.137 (207.745)	Grad: 347559.000
[Epoch  58 (214.03s)]	ELBO: -11592.218, -7437.535, -5219.958, -3782.005, -2822.007, -2103.337, -1503.446, -1013.246, -603.758, -267.757, 27.302, 275.399, 411.562, 453.997, 477.402, 490.537 (398.947)	Log prob: -11573.207, -7402.666, -5169.989, -3717.778, -2744.167, -2013.052, -1400.375, -898.019, -476.522, -128.238, 178.659, 438.831, 586.664, 640.035, 674.391, 698.623 (608.185)	KLD: 19.013, 15.853, 15.102, 14.259, 13.613, 12.446, 12.784, 12.157, 12.009, 12.282, 11.838, 12.076, 11.670, 10.935, 10.952, 11.097 (209.238)	Grad: 366525.594
[Epoch  59 (197.74s)]	ELBO: -11567.144, -7431.269, -5215.317, -3779.082, -2820.208, -2105.033, -1505.714, -1016.232, -606.961, -270.804, 25.080, 273.696, 412.960, 456.354, 479.201, 492.417 (467.622)	Log prob: -11548.051, -7396.339, -5165.233, -3714.774, -2742.286, -2014.643, -1402.563, -900.953, -479.642, -131.182, 176.532, 437.253, 588.198, 642.609, 676.379, 700.619 (672.745)	KLD: 19.095, 15.832, 15.154, 14.229, 13.611, 12.467, 12.762, 12.128, 12.040, 12.304, 11.829, 12.105, 11.682, 11.016, 10.924, 11.024 (205.123)	Grad: 340670.312
[Epoch  60 (199.17s)]	ELBO: -11572.161, -7440.978, -5213.231, -3774.974, -2816.982, -2102.203, -1504.152, -1013.227, -603.887, -267.367, 27.755, 276.842, 419.437, 463.612, 485.393, 498.957 (480.348)	Log prob: -11553.089, -7406.056, -5163.184, -3710.706, -2739.136, -2011.933, -1401.088, -898.035, -476.688, -127.870, 179.073, 440.217, 594.388, 649.521, 682.119, 706.667 (687.461)	KLD: 19.067, 15.855, 15.127, 14.220, 13.579, 12.423, 12.793, 12.129, 12.006, 12.298, 11.821, 12.057, 11.576, 10.959, 10.817, 10.982 (207.113)	Grad: 344055.344
[Epoch  61 (197.29s)]	ELBO: -11571.864, -7441.438, -5232.043, -3793.750, -2831.673, -2108.383, -1504.447, -1013.731, -603.336, -267.224, 27.890, 277.551, 422.138, 467.543, 489.220, 502.508 (486.989)	Log prob: -11552.826, -7406.540, -5182.072, -3729.557, -2753.896, -2018.178, -1401.497, -898.649, -476.267, -127.934, 178.966, 440.673, 596.892, 653.167, 685.566, 709.820 (694.317)	KLD: 19.045, 15.854, 15.074, 14.221, 13.582, 12.430, 12.743, 12.133, 11.988, 12.220, 11.786, 12.046, 11.633, 10.870, 10.721, 10.966 (207.328)	Grad: 329913.594
[Epoch  62 (199.13s)]	ELBO: -11593.479, -7429.730, -5208.154, -3775.038, -2817.218, -2097.093, -1497.139, -1006.084, -596.618, -260.971, 34.120, 283.273, 429.603, 478.029, 499.256, 512.535 (491.235)	Log prob: -11574.476, -7394.830, -5158.198, -3710.839, -2739.472, -2006.948, -1394.234, -891.041, -469.607, -121.765, 185.097, 446.277, 604.214, 663.532, 695.440, 719.688 (699.401)	KLD: 19.010, 15.890, 15.057, 14.242, 13.547, 12.398, 12.762, 12.137, 11.968, 12.195, 11.771, 12.027, 11.608, 10.892, 10.680, 10.969 (208.167)	Grad: 335427.062
[Epoch  63 (198.28s)]	ELBO: -11633.738, -7446.031, -5204.729, -3769.619, -2810.341, -2092.291, -1494.285, -1003.798, -593.385, -257.806, 37.970, 288.119, 435.683, 485.650, 506.773, 520.007 (499.701)	Log prob: -11614.736, -7411.205, -5154.779, -3705.374, -2732.513, -2002.082, -1391.371, -888.771, -466.399, -118.662, 188.896, 451.037, 610.249, 671.159, 702.891, 727.059 (704.935)	KLD: 18.993, 15.835, 15.121, 14.297, 13.582, 12.381, 12.704, 12.112, 11.960, 12.158, 11.781, 11.993, 11.647, 10.943, 10.610, 10.933 (205.233)	Grad: 320134.062
[Epoch  64 (199.11s)]	ELBO: -11762.837, -7547.829, -5291.857, -3854.052, -2896.100, -2178.991, -1580.574, -1089.677, -681.852, -350.977, -58.537, 184.857, 332.827, 405.879, 434.659, 448.976 (489.536)	Log prob: -11742.840, -7512.389, -5241.212, -3789.160, -2817.399, -2087.774, -1476.303, -973.118, -553.117, -209.713, 94.667, 350.377, 510.279, 595.762, 636.789, 662.434 (703.203)	KLD: 19.996, 15.442, 15.206, 14.249, 13.807, 12.515, 13.054, 12.289, 12.177, 12.529, 11.939, 12.317, 11.932, 12.430, 12.247, 11.328 (213.667)	Grad: 535500.562
[Epoch  65 (198.69s)]	ELBO: -11706.347, -7488.448, -5207.410, -3768.713, -2809.840, -2090.393, -1490.380, -999.631, -589.513, -252.819, 43.272, 294.668, 445.498, 494.220, 513.518, 526.700 (492.051)	Log prob: -11687.197, -7453.869, -5157.640, -3704.676, -2732.029, -2000.197, -1387.258, -884.272, -462.108, -113.058, 194.921, 458.478, 621.074, 681.836, 712.594, 736.900 (702.444)	KLD: 19.151, 15.421, 15.199, 14.266, 13.773, 12.385, 12.926, 12.237, 12.046, 12.357, 11.887, 12.161, 11.766, 12.039, 11.461, 11.124 (210.392)	Grad: 348380.219
[Epoch  66 (197.73s)]	ELBO: -11738.774, -7494.918, -5192.356, -3753.912, -2793.339, -2077.081, -1480.031, -990.135, -580.765, -244.761, 50.702, 301.572, 452.706, 505.511, 525.633, 538.622 (512.266)	Log prob: -11719.761, -7460.394, -5142.517, -3689.789, -2715.552, -1986.950, -1377.097, -874.997, -453.616, -105.382, 201.963, 464.887, 627.649, 692.142, 723.266, 747.162 (720.725)	KLD: 19.013, 15.512, 15.313, 14.285, 13.665, 12.344, 12.803, 12.203, 12.011, 12.230, 11.882, 12.054, 11.627, 11.688, 11.003, 10.907 (208.459)	Grad: 378861.750
[Epoch  67 (197.42s)]	ELBO: -11645.243, -7449.791, -5181.189, -3747.494, -2791.740, -2076.729, -1482.552, -993.307, -583.615, -247.961, 46.963, 297.406, 452.072, 508.594, 529.814, 542.818 (514.651)	Log prob: -11626.272, -7415.304, -5131.404, -3683.422, -2714.020, -1986.665, -1379.696, -878.253, -456.568, -108.741, 198.047, 460.506, 626.741, 694.739, 726.692, 750.486 (722.413)	KLD: 18.961, 15.527, 15.296, 14.288, 13.648, 12.345, 12.791, 12.198, 11.993, 12.173, 11.865, 12.016, 11.569, 11.475, 10.734, 10.790 (207.762)	Grad: 337104.938
[Epoch  68 (198.97s)]	ELBO: -11686.762, -7477.973, -5189.696, -3751.891, -2794.265, -2078.388, -1482.774, -993.215, -584.011, -248.510, 46.877, 297.682, 454.215, 512.971, 533.939, 546.259 (519.720)	Log prob: -11667.771, -7443.373, -5139.829, -3687.764, -2716.526, -1988.300, -1379.953, -878.250, -457.120, -109.476, 197.744, 460.478, 628.592, 698.741, 730.282, 753.355 (728.102)	KLD: 18.996, 15.604, 15.265, 14.265, 13.609, 12.349, 12.732, 12.145, 11.925, 12.144, 11.833, 11.929, 11.581, 11.393, 10.572, 10.754 (208.381)	Grad: 353889.094
[Epoch  69 (199.59s)]	ELBO: -11683.493, -7476.197, -5185.559, -3747.296, -2791.167, -2075.010, -1478.910, -990.029, -580.880, -244.790, 50.471, 301.513, 460.676, 520.620, 540.141, 552.991 (515.202)	Log prob: -11664.532, -7441.618, -5135.741, -3683.196, -2713.444, -1984.901, -1376.078, -875.069, -453.969, -105.743, 201.325, 464.335, 635.035, 706.192, 736.082, 759.672 (720.649)	KLD: 18.961, 15.617, 15.240, 14.285, 13.622, 12.385, 12.722, 12.128, 11.951, 12.135, 11.808, 11.968, 11.537, 11.212, 10.369, 10.739 (205.446)	Grad: 316935.688
[Epoch  70 (199.75s)]	ELBO: -11727.324, -7530.329, -5201.378, -3761.673, -2805.684, -2089.878, -1492.924, -1003.675, -594.544, -258.493, 36.537, 287.376, 450.479, 514.066, 536.449, 549.272 (517.714)	Log prob: -11708.358, -7495.730, -5151.488, -3697.534, -2727.992, -1999.799, -1390.186, -888.804, -467.750, -119.605, 187.212, 450.002, 624.622, 699.397, 732.221, 755.716 (723.513)	KLD: 18.957, 15.638, 15.294, 14.251, 13.553, 12.386, 12.659, 12.132, 11.923, 12.094, 11.787, 11.950, 11.517, 11.188, 10.443, 10.670 (205.799)	Grad: 360821.469
[Epoch  71 (199.69s)]	ELBO: -11770.604, -7558.093, -5213.153, -3770.612, -2812.109, -2095.271, -1497.592, -1007.802, -598.617, -262.406, 32.776, 283.970, 449.663, 514.584, 537.070, 550.273 (523.070)	Log prob: -11751.672, -7523.548, -5163.333, -3706.547, -2734.467, -2005.218, -1394.834, -892.934, -471.847, -123.528, 183.444, 446.561, 623.755, 699.883, 732.747, 756.612 (728.326)	KLD: 18.936, 15.603, 15.280, 14.247, 13.577, 12.413, 12.702, 12.112, 11.902, 12.108, 11.789, 11.923, 11.502, 11.208, 10.377, 10.662 (205.256)	Grad: 376444.469
[Epoch  72 (196.58s)]	ELBO: -11670.551, -7491.264, -5186.008, -3749.479, -2791.333, -2072.448, -1473.615, -984.280, -575.054, -239.663, 55.320, 307.482, 472.108, 533.399, 553.937, 565.912 (540.797)	Log prob: -11651.606, -7456.671, -5136.148, -3685.321, -2713.558, -1982.293, -1370.833, -869.401, -448.317, -100.917, 205.812, 469.864, 646.008, 718.425, 749.265, 771.850 (746.244)	KLD: 18.946, 15.652, 15.263, 14.295, 13.620, 12.377, 12.629, 12.096, 11.858, 12.009, 11.747, 11.890, 11.517, 11.126, 10.302, 10.610 (205.447)	Grad: 316790.438
[Epoch  73 (177.13s)]	ELBO: -11667.222, -7523.484, -5212.875, -3770.499, -2811.319, -2091.550, -1491.840, -1003.574, -593.550, -257.532, 37.848, 290.555, 461.774, 525.876, 547.733, 561.192 (526.716)	Log prob: -11648.312, -7488.847, -5163.147, -3706.501, -2733.710, -2001.535, -1389.179, -888.853, -466.952, -118.940, 188.189, 452.789, 635.431, 710.748, 742.881, 766.788 (732.770)	KLD: 18.915, 15.724, 15.091, 14.267, 13.612, 12.407, 12.646, 12.059, 11.878, 11.994, 11.749, 11.893, 11.423, 11.215, 10.277, 10.448 (206.054)	Grad: 347080.312
[Epoch  74 (145.61s)]	ELBO: -11665.324, -7534.582, -5220.116, -3777.073, -2817.778, -2095.425, -1494.743, -1004.998, -596.066, -259.862, 35.384, 288.269, 462.614, 525.831, 548.821, 561.998 (532.289)	Log prob: -11646.312, -7499.985, -5170.350, -3713.048, -2740.130, -2005.373, -1392.042, -890.239, -469.443, -121.194, 185.765, 450.512, 636.318, 710.670, 743.964, 767.674 (737.998)	KLD: 19.003, 15.594, 15.170, 14.257, 13.624, 12.403, 12.650, 12.058, 11.864, 12.045, 11.713, 11.862, 11.461, 11.135, 10.304, 10.533 (205.709)	Grad: 375311.688
[Epoch  75 (147.44s)]	ELBO: -11641.604, -7496.523, -5201.766, -3761.099, -2803.229, -2084.824, -1486.539, -997.887, -588.807, -253.000, 42.837, 295.700, 474.377, 535.886, 557.785, 571.277 (547.708)	Log prob: -11622.615, -7461.868, -5151.977, -3697.052, -2725.601, -1994.816, -1383.887, -883.207, -462.266, -114.452, 193.103, 457.825, 647.981, 720.569, 752.774, 776.711 (753.815)	KLD: 18.988, 15.667, 15.135, 14.259, 13.579, 12.381, 12.644, 12.026, 11.862, 12.007, 11.718, 11.859, 11.479, 11.079, 10.305, 10.445 (206.107)	Grad: 358689.469
[Epoch  76 (147.51s)]	ELBO: -11692.878, -7499.243, -5208.559, -3769.418, -2808.725, -2087.638, -1485.755, -997.507, -588.532, -252.315, 42.885, 295.494, 477.870, 536.791, 558.646, 571.908 (525.399)	Log prob: -11673.908, -7464.592, -5158.706, -3705.267, -2731.022, -1997.524, -1383.040, -882.727, -461.904, -113.739, 193.166, 457.593, 651.389, 721.370, 753.466, 777.084 (731.066)	KLD: 18.969, 15.687, 15.195, 14.299, 13.552, 12.411, 12.601, 12.066, 11.847, 11.947, 11.706, 11.818, 11.421, 11.059, 10.240, 10.356 (205.667)	Grad: 368958.531
[Epoch  77 (146.53s)]	ELBO: -11702.445, -7491.074, -5197.818, -3757.357, -2796.382, -2076.958, -1478.214, -990.913, -581.910, -245.756, 50.404, 303.523, 490.671, 545.242, 566.369, 579.094 (539.144)	Log prob: -11683.458, -7456.387, -5147.903, -3693.121, -2718.605, -1986.781, -1375.475, -876.128, -455.309, -107.219, 200.638, 465.555, 664.158, 729.753, 761.018, 784.038 (744.739)	KLD: 18.974, 15.713, 15.226, 14.323, 13.542, 12.399, 12.563, 12.045, 11.817, 11.936, 11.697, 11.797, 11.455, 11.025, 10.138, 10.295 (205.595)	Grad: 356947.750
[Epoch  78 (149.13s)]	ELBO: -11772.311, -7532.667, -5197.876, -3754.697, -2793.557, -2072.609, -1472.705, -985.330, -576.043, -239.164, 57.834, 311.492, 504.015, 553.470, 573.353, 586.151 (558.748)	Log prob: -11753.376, -7498.002, -5148.006, -3690.533, -2715.759, -1982.396, -1369.916, -870.473, -449.391, -100.549, 208.129, 473.628, 677.585, 738.008, 767.867, 790.957 (764.273)	KLD: 18.933, 15.736, 15.203, 14.293, 13.635, 12.413, 12.576, 12.069, 11.795, 11.962, 11.680, 11.842, 11.434, 10.968, 9.976, 10.292 (205.525)	Grad: 317907.625
[Epoch  79 (144.41s)]	ELBO: -11745.460, -7526.014, -5193.787, -3747.032, -2787.307, -2071.382, -1474.539, -986.766, -577.582, -241.070, 55.535, 308.910, 506.036, 553.609, 574.015, 586.868 (528.616)	Log prob: -11726.547, -7491.348, -5143.876, -3682.825, -2709.517, -1981.174, -1371.730, -871.926, -450.951, -102.488, 205.767, 470.770, 679.337, 737.790, 768.139, 791.162 (732.659)	KLD: 18.911, 15.750, 15.248, 14.298, 13.585, 12.415, 12.602, 12.031, 11.791, 11.952, 11.651, 11.627, 11.441, 10.880, 9.942, 10.171 (204.043)	Grad: 339877.406
[Epoch  80 (149.04s)]	ELBO: -11712.158, -7511.890, -5189.456, -3750.685, -2793.477, -2079.104, -1482.983, -995.686, -586.370, -250.943, 45.333, 297.988, 496.665, 547.637, 570.225, 583.348 (537.006)	Log prob: -11693.261, -7477.277, -5139.567, -3686.467, -2715.765, -1988.972, -1380.243, -880.885, -459.761, -112.379, 195.594, 460.082, 670.168, 732.090, 764.813, 788.035 (745.028)	KLD: 18.900, 15.712, 15.277, 14.329, 13.495, 12.420, 12.607, 12.062, 11.807, 11.955, 11.696, 11.833, 11.410, 10.949, 10.135, 10.098 (208.022)	Grad: 362467.000
[Epoch  81 (148.20s)]	ELBO: -11653.357, -7480.061, -5196.551, -3758.285, -2797.627, -2078.104, -1478.973, -991.759, -582.615, -245.937, 51.184, 304.287, 507.121, 554.612, 575.876, 589.138 (541.017)	Log prob: -11634.399, -7445.432, -5146.727, -3694.241, -2719.968, -1988.015, -1376.293, -877.098, -456.184, -107.599, 201.148, 466.068, 680.268, 738.670, 770.016, 793.355 (745.247)	KLD: 18.960, 15.673, 15.192, 14.220, 13.613, 12.432, 12.589, 11.982, 11.770, 11.907, 11.625, 11.818, 11.365, 10.912, 10.081, 10.077 (204.230)	Grad: 331465.500
[Epoch  82 (148.32s)]	ELBO: -11657.008, -7474.410, -5187.023, -3747.023, -2787.172, -2067.880, -1469.613, -983.829, -574.628, -238.361, 58.224, 311.653, 516.323, 563.044, 584.005, 597.178 (562.528)	Log prob: -11638.074, -7439.771, -5137.151, -3682.865, -2709.461, -1977.736, -1366.938, -869.143, -448.155, -100.006, 208.193, 473.381, 689.431, 746.895, 777.813, 800.966 (768.198)	KLD: 18.930, 15.710, 15.235, 14.282, 13.554, 12.432, 12.532, 12.012, 11.787, 11.882, 11.614, 11.760, 11.379, 10.743, 9.956, 9.981 (205.670)	Grad: 303565.250
[Epoch  83 (149.53s)]	ELBO: -11747.992, -7514.639, -5198.738, -3757.194, -2796.852, -2079.310, -1481.701, -993.777, -584.800, -248.426, 48.644, 302.469, 508.770, 557.804, 580.001, 594.281 (567.999)	Log prob: -11729.029, -7479.883, -5148.732, -3692.981, -2719.054, -1989.103, -1378.946, -879.029, -458.269, -109.991, 198.702, 464.250, 681.984, 741.757, 773.898, 798.213 (771.625)	KLD: 18.965, 15.792, 15.248, 14.206, 13.585, 12.409, 12.549, 11.993, 11.783, 11.903, 11.623, 11.723, 11.433, 10.739, 9.943, 10.035 (203.626)	Grad: 354392.781
[Epoch  84 (146.78s)]	ELBO: -11710.147, -7501.188, -5198.383, -3757.381, -2796.387, -2077.969, -1479.305, -991.898, -582.472, -246.100, 51.458, 304.651, 513.418, 561.806, 582.751, 595.767 (569.463)	Log prob: -11691.211, -7466.550, -5148.514, -3693.216, -2718.595, -1987.731, -1376.507, -877.117, -455.935, -107.662, 201.536, 466.426, 686.624, 745.796, 776.711, 799.739 (771.495)	KLD: 18.932, 15.706, 15.234, 14.291, 13.629, 12.445, 12.561, 11.984, 11.755, 11.901, 11.641, 11.697, 11.431, 10.784, 9.970, 10.013 (202.032)	Grad: 344268.438
[Epoch  85 (146.16s)]	ELBO: -11689.204, -7492.478, -5191.062, -3750.805, -2788.008, -2068.938, -1471.627, -985.294, -575.925, -239.164, 58.091, 311.895, 521.629, 568.082, 590.099, 604.272 (567.571)	Log prob: -11670.301, -7457.871, -5141.223, -3686.676, -2710.252, -1978.762, -1368.917, -870.583, -449.476, -100.857, 208.005, 473.490, 694.630, 751.731, 783.628, 807.733 (773.412)	KLD: 18.893, 15.712, 15.235, 14.294, 13.621, 12.420, 12.536, 12.000, 11.737, 11.858, 11.607, 11.681, 11.406, 10.648, 9.881, 9.933 (205.842)	Grad: 334134.031
[Epoch  86 (147.21s)]	ELBO: -11686.413, -7492.732, -5183.474, -3743.311, -2782.832, -2065.040, -1466.390, -980.707, -570.888, -234.534, 62.134, 316.112, 526.921, 574.566, 596.580, 609.808 (576.259)	Log prob: -11667.500, -7458.055, -5133.555, -3679.113, -2705.086, -1974.886, -1363.695, -866.038, -444.499, -96.300, 211.960, 477.584, 699.803, 758.050, 789.844, 812.972 (780.609)	KLD: 18.914, 15.763, 15.241, 14.280, 13.548, 12.408, 12.541, 11.975, 11.719, 11.846, 11.592, 11.646, 11.410, 10.602, 9.781, 9.899 (204.350)	Grad: 345707.750
[Epoch  87 (143.74s)]	ELBO: -11721.687, -7509.234, -5194.917, -3752.440, -2793.827, -2074.380, -1475.589, -987.748, -578.483, -241.573, 55.261, 309.307, 521.459, 571.230, 594.873, 608.689 (570.715)	Log prob: -11702.758, -7474.541, -5144.970, -3688.229, -2716.001, -1984.120, -1372.826, -872.995, -452.016, -103.260, 205.139, 470.817, 694.404, 754.836, 788.372, 812.094 (774.350)	KLD: 18.941, 15.749, 15.256, 14.266, 13.614, 12.435, 12.503, 11.989, 11.714, 11.846, 11.565, 11.632, 11.435, 10.661, 9.893, 9.906 (203.634)	Grad: 358962.875
[Epoch  88 (146.36s)]	ELBO: -11751.813, -7514.336, -5189.097, -3749.149, -2787.887, -2068.912, -1470.460, -983.096, -573.323, -236.601, 60.021, 314.484, 528.117, 576.258, 598.011, 611.680 (566.595)	Log prob: -11732.922, -7479.747, -5139.197, -3684.936, -2710.042, -1978.654, -1367.633, -868.299, -446.784, -98.212, 209.992, 476.102, 701.166, 759.858, 791.372, 814.874 (768.869)	KLD: 18.883, 15.710, 15.308, 14.313, 13.631, 12.415, 12.569, 11.969, 11.741, 11.851, 11.582, 11.647, 11.431, 10.550, 9.761, 9.833 (202.274)	Grad: 327416.719
[Epoch  89 (151.35s)]	ELBO: -11761.195, -7526.133, -5187.038, -3743.030, -2785.313, -2066.485, -1469.659, -983.554, -574.290, -236.941, 60.805, 315.027, 528.701, 578.109, 600.424, 613.874 (575.316)	Log prob: -11742.328, -7491.447, -5137.124, -3678.823, -2707.535, -1976.247, -1366.910, -868.808, -447.796, -98.579, 210.737, 476.569, 701.659, 761.628, 793.705, 816.976 (778.435)	KLD: 18.863, 15.824, 15.229, 14.289, 13.573, 12.459, 12.511, 11.996, 11.749, 11.868, 11.570, 11.609, 11.416, 10.561, 9.762, 9.822 (203.119)	Grad: 317757.000
[Epoch  90 (149.80s)]	ELBO: -11795.768, -7542.845, -5184.635, -3738.781, -2780.564, -2063.969, -1466.838, -981.061, -570.997, -233.451, 63.843, 318.528, 533.200, 583.402, 606.344, 619.760 (575.853)	Log prob: -11776.869, -7508.098, -5134.644, -3674.492, -2702.715, -1973.681, -1364.035, -866.326, -444.531, -95.151, 213.736, 480.013, 706.156, 766.891, 799.552, 822.823 (778.670)	KLD: 18.902, 15.839, 15.253, 14.294, 13.562, 12.438, 12.514, 11.932, 11.731, 11.833, 11.593, 11.592, 11.471, 10.532, 9.719, 9.856 (202.817)	Grad: 325997.000
[Epoch  91 (149.13s)]	ELBO: -11782.007, -7524.723, -5179.404, -3739.387, -2780.009, -2061.138, -1463.669, -977.551, -567.595, -230.426, 66.186, 320.573, 536.047, 587.631, 611.333, 624.709 (588.010)	Log prob: -11763.115, -7490.044, -5129.463, -3675.144, -2702.164, -1970.859, -1360.891, -862.841, -441.160, -92.184, 216.008, 482.032, 708.968, 771.013, 804.393, 827.583 (789.616)	KLD: 18.882, 15.792, 15.265, 14.303, 13.599, 12.437, 12.498, 11.931, 11.726, 11.806, 11.581, 11.637, 11.462, 10.461, 9.678, 9.814 (201.607)	Grad: 325577.031
[Epoch  92 (149.51s)]	ELBO: -11781.527, -7533.068, -5191.107, -3747.161, -2785.705, -2061.647, -1458.325, -972.116, -562.531, -225.362, 71.508, 326.780, 543.160, 595.146, 618.469, 631.452 (582.650)	Log prob: -11762.685, -7498.427, -5141.215, -3682.976, -2707.995, -1971.510, -1355.699, -857.564, -436.268, -87.308, 221.130, 487.942, 715.741, 778.242, 811.329, 833.978 (786.699)	KLD: 18.840, 15.805, 15.244, 14.298, 13.525, 12.426, 12.487, 11.927, 11.711, 11.791, 11.568, 11.540, 11.419, 10.515, 9.764, 9.665 (204.049)	Grad: 321984.625
[Epoch  93 (146.24s)]	ELBO: -11725.570, -7513.088, -5191.708, -3749.652, -2788.627, -2065.396, -1464.590, -977.214, -566.905, -231.324, 65.695, 320.211, 536.799, 592.599, 616.752, 629.921 (589.359)	Log prob: -11706.676, -7478.388, -5141.802, -3685.417, -2710.833, -1975.174, -1361.906, -862.611, -440.618, -93.289, 215.249, 481.308, 709.272, 775.580, 809.371, 832.117 (794.517)	KLD: 18.895, 15.804, 15.212, 14.322, 13.560, 12.429, 12.462, 11.920, 11.683, 11.749, 11.518, 11.544, 11.375, 10.508, 9.640, 9.577 (205.158)	Grad: 327989.844
[Epoch  94 (150.65s)]	ELBO: -11727.572, -7512.961, -5203.750, -3761.662, -2798.396, -2074.326, -1472.691, -985.372, -575.124, -238.241, 59.553, 313.805, 530.410, 588.427, 613.847, 627.042 (562.276)	Log prob: -11708.701, -7478.323, -5153.846, -3697.452, -2720.650, -1984.139, -1370.010, -870.795, -448.856, -100.185, 209.104, 474.870, 702.848, 771.339, 806.471, 829.293 (761.712)	KLD: 18.878, 15.765, 15.262, 14.305, 13.534, 12.442, 12.494, 11.897, 11.691, 11.787, 11.496, 11.514, 11.373, 10.473, 9.711, 9.628 (199.436)	Grad: 327762.344
[Epoch  95 (148.50s)]	ELBO: -11630.229, -7474.538, -5200.907, -3761.927, -2802.930, -2082.435, -1480.848, -992.780, -582.556, -246.425, 49.792, 304.219, 520.677, 584.570, 610.328, 624.040 (596.346)	Log prob: -11611.333, -7439.846, -5151.047, -3697.739, -2725.228, -1992.277, -1378.185, -878.142, -456.167, -108.231, 199.423, 465.409, 693.306, 767.682, 803.179, 826.472 (801.668)	KLD: 18.889, 15.804, 15.169, 14.324, 13.515, 12.456, 12.506, 11.975, 11.751, 11.805, 11.437, 11.559, 11.439, 10.483, 9.739, 9.581 (205.321)	Grad: 345110.438
[Epoch  96 (149.72s)]	ELBO: -11592.949, -7451.100, -5189.026, -3752.290, -2791.304, -2072.807, -1474.262, -987.883, -578.130, -241.275, 55.495, 309.482, 525.879, 590.282, 616.225, 630.457 (592.407)	Log prob: -11574.007, -7416.382, -5139.077, -3688.047, -2713.490, -1982.572, -1371.578, -873.268, -451.824, -103.190, 205.104, 470.598, 698.395, 773.368, 808.947, 832.781 (793.427)	KLD: 18.937, 15.778, 15.235, 14.296, 13.571, 12.419, 12.448, 11.933, 11.691, 11.779, 11.525, 11.507, 11.399, 10.570, 9.638, 9.602 (201.020)	Grad: 345869.562
[Epoch  97 (150.17s)]	ELBO: -11623.060, -7482.431, -5215.381, -3776.488, -2815.451, -2093.420, -1491.162, -1004.238, -593.665, -257.260, 40.329, 294.547, 511.629, 582.939, 611.206, 625.950 (590.691)	Log prob: -11604.114, -7447.742, -5165.488, -3712.260, -2737.725, -2003.262, -1388.515, -889.677, -467.423, -119.195, 189.927, 455.743, 684.212, 766.306, 804.333, 828.704 (794.358)	KLD: 18.939, 15.748, 15.203, 14.335, 13.501, 12.433, 12.490, 11.913, 11.681, 11.823, 11.534, 11.598, 11.386, 10.783, 9.761, 9.627 (203.667)	Grad: 361641.969
[Epoch  98 (145.75s)]	ELBO: -11637.305, -7471.371, -5242.694, -3802.484, -2831.480, -2088.867, -1480.982, -992.418, -582.131, -245.339, 51.752, 306.966, 524.737, 589.588, 616.334, 629.740 (570.813)	Log prob: -11618.363, -7436.557, -5192.758, -3738.166, -2753.516, -1998.467, -1377.979, -877.462, -455.513, -106.889, 201.747, 468.540, 697.686, 773.153, 809.683, 832.689 (776.529)	KLD: 18.947, 15.867, 15.122, 14.382, 13.647, 12.436, 12.602, 11.953, 11.662, 11.832, 11.545, 11.579, 11.375, 10.615, 9.784, 9.600 (205.716)	Grad: 362227.562
[Epoch  99 (148.83s)]	ELBO: -11618.456, -7460.198, -5190.563, -3752.498, -2790.501, -2066.080, -1465.838, -979.594, -569.743, -232.958, 63.968, 318.331, 537.634, 600.441, 626.783, 640.293 (570.739)	Log prob: -11599.555, -7425.555, -5140.689, -3688.386, -2712.781, -1975.963, -1363.272, -865.146, -443.583, -95.030, 213.438, 479.336, 710.023, 783.370, 819.390, 842.388 (771.979)	KLD: 18.898, 15.744, 15.231, 14.239, 13.607, 12.397, 12.449, 11.881, 11.713, 11.768, 11.542, 11.534, 11.385, 10.539, 9.679, 9.488 (201.240)	Grad: 365311.906
[Epoch 100 (145.83s)]	ELBO: -11651.212, -7481.003, -5188.057, -3746.800, -2786.486, -2069.866, -1473.555, -986.849, -576.752, -239.344, 57.626, 312.016, 529.999, 597.182, 623.770, 637.297 (590.864)	Log prob: -11632.328, -7446.296, -5138.197, -3682.642, -2708.752, -1979.730, -1371.014, -872.427, -450.676, -101.568, 206.906, 472.769, 702.131, 779.843, 815.944, 838.858 (792.971)	KLD: 18.884, 15.823, 15.150, 14.301, 13.576, 12.402, 12.405, 11.881, 11.653, 11.701, 11.503, 11.474, 11.378, 10.530, 9.513, 9.386 (202.107)	Grad: 339675.156
Best epoch(s): [95]	Training time(s): 22658.23s (22658.23s)	Best ELBO: 640.293 (596.346)	Best log prob: 842.388 (801.668)
Avg. mu: 0.593, 0.233, 0.156, 0.161, 0.004, -0.104, 0.101, 0.101, 0.031, -0.068, 0.030, 0.105, 0.011, -0.052, 0.038, 0.122, 0.016, -0.019, 0.062, 0.070, 0.057, -0.023, 0.053, 0.124, 0.028, -0.044, -0.007, 0.057, 0.048, -0.007, 0.029, 0.073, 0.032, -0.042, 0.091, 0.109, 0.019, -0.080, -0.040, 0.049, -0.036, -0.068, 0.049, 0.079, -0.040, -0.082, 0.022, 0.050, 0.002, 0.052, 0.053, 0.090, -0.098, 0.010, 0.226, 0.050, 0.052, 0.029, 0.131, 0.127, -0.034, -0.068, 0.097, 0.214
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.002, 0.001, 0.002, 0.002, 0.003, 0.002, 0.003, 0.003, 0.004, 0.003, 0.004, 0.003, 0.003, 0.002
Max. mu: 5.467, 4.960, 3.641, 4.532, 3.197, 2.933, 3.003, 3.579, 3.421, 3.061, 3.210, 3.781, 3.012, 2.777, 3.088, 2.743, 2.782, 2.789, 2.861, 2.554, 3.208, 3.326, 3.483, 3.493, 3.916, 2.792, 2.985, 2.456, 4.394, 5.057, 3.163, 3.705, 3.304, 3.436, 2.967, 2.868, 2.899, 2.509, 2.774, 2.225, 3.917, 2.295, 2.887, 2.776, 2.427, 2.041, 2.698, 2.232, 2.351, 2.021, 2.560, 2.059, 2.618, 1.216, 2.272, 1.417, 2.100, 1.653, 2.314, 1.717, 1.963, 1.328, 2.255, 2.828
Max. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.002, 0.001, 0.002, 0.002, 0.002, 0.001, 0.004, 0.004, 0.005, 0.004, 0.003, 0.002, 0.003, 0.003, 0.007, 0.005, 0.006, 0.005, 0.005, 0.004, 0.005, 0.005, 0.013, 0.012, 0.014, 0.010, 0.035, 0.026, 0.033, 0.029, 0.003, 0.003, 0.004, 0.003, 0.010, 0.009, 0.010, 0.008, 0.008, 0.007, 0.010, 0.006, 0.013, 0.012, 0.019, 0.013, 0.021, 0.018, 0.018, 0.015
Min. mu: -3.358, -2.922, -4.032, -4.268, -3.354, -2.824, -4.083, -3.239, -3.113, -3.743, -3.268, -3.210, -3.419, -2.564, -2.724, -1.834, -2.367, -3.050, -3.545, -2.226, -3.384, -3.275, -4.471, -2.959, -2.895, -2.919, -3.032, -2.975, -3.490, -3.112, -3.716, -2.734, -3.509, -3.849, -2.953, -2.390, -2.922, -2.792, -3.157, -2.614, -2.979, -2.706, -3.179, -2.293, -2.480, -2.539, -2.575, -2.395, -2.098, -2.273, -2.123, -1.644, -1.560, -1.540, -1.653, -1.588, -1.483, -1.365, -2.034, -1.832, -1.219, -1.941, -1.224, -0.868
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.000, 0.001, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[0.664 -0.024 -0.157 -0.001 0.001 -0.017 0.002 0.000 -0.038 -0.028 0.007
  0.000 -0.001 0.006 -0.000 -0.003 0.009 0.005 -0.000 -0.006 -0.008
  -0.007 0.000 0.018 -0.000 0.001 -0.004 -0.013 0.004 0.016 0.004 -0.007
  0.010 0.002 0.002 0.006 0.012 -0.004 -0.010 -0.009 -0.008 0.014 -0.005
  0.001 0.000 0.006 -0.005 -0.009 -0.008 0.006 0.010 -0.004 -0.024 0.006
  -0.004 0.020 -0.016 0.025 0.025 0.006 0.010 0.011 0.013 0.006]
 [-0.024 1.059 -0.102 -0.206 0.003 0.001 -0.009 -0.001 -0.002 -0.002
  0.012 0.005 0.001 0.014 -0.007 -0.012 -0.007 -0.002 0.002 -0.014 -0.011
  -0.019 -0.008 0.025 -0.002 -0.009 0.010 0.009 0.009 -0.005 0.001 0.001
  0.006 -0.019 0.030 -0.009 0.004 0.008 -0.001 0.001 -0.003 -0.004 0.003
  0.001 -0.004 -0.009 0.003 0.017 -0.007 -0.004 0.006 0.002 -0.038 -0.023
  0.002 0.021 -0.029 -0.033 0.026 0.002 -0.016 -0.010 0.005 -0.001]
 [-0.157 -0.102 0.730 0.144 -0.004 0.016 0.021 -0.009 -0.015 -0.013 0.005
  -0.009 -0.004 -0.005 0.000 0.011 -0.007 -0.009 0.009 0.005 -0.002 0.002
  0.021 -0.008 0.021 0.017 0.012 0.005 0.002 0.014 -0.002 0.008 0.011
  -0.008 0.002 0.006 -0.006 -0.001 0.004 0.015 -0.007 -0.013 0.009 0.003
  0.000 -0.009 -0.006 -0.002 -0.000 -0.000 -0.001 0.005 0.045 0.018
  -0.012 -0.040 0.021 -0.026 -0.019 0.009 0.002 0.003 -0.012 -0.003]
 [-0.001 -0.206 0.144 1.317 0.023 0.018 -0.034 0.011 -0.049 -0.056 -0.026
  0.000 0.013 -0.001 0.023 0.002 0.016 -0.012 0.004 -0.012 -0.000 -0.001
  0.002 -0.011 0.024 -0.011 -0.014 -0.005 -0.017 0.029 0.021 -0.013
  -0.004 0.025 -0.011 -0.014 -0.015 -0.006 -0.024 0.001 0.003 0.004 0.020
  -0.009 0.004 -0.003 -0.023 -0.006 0.007 -0.022 -0.008 -0.017 -0.024
  -0.012 -0.011 -0.048 0.001 0.001 0.015 0.010 0.017 0.007 0.003 0.018]
 [0.001 0.003 -0.004 0.023 0.417 0.014 -0.017 0.008 0.006 0.005 -0.004
  0.008 -0.006 -0.007 0.014 -0.002 0.006 0.001 0.001 -0.009 -0.016 0.017
  -0.002 0.002 0.004 -0.005 -0.011 -0.011 0.004 0.008 -0.011 -0.001
  -0.005 0.018 0.004 0.007 0.009 0.006 -0.006 -0.000 0.008 -0.004 -0.005
  0.002 -0.004 -0.007 -0.003 -0.007 0.004 0.004 -0.001 0.001 0.004 0.002
  -0.004 -0.000 -0.011 -0.002 0.002 -0.006 -0.014 0.003 -0.015 0.002]
 [-0.017 0.001 0.016 0.018 0.014 0.426 -0.007 -0.017 -0.006 0.003 0.000
  -0.000 0.010 -0.002 0.009 -0.017 -0.006 -0.004 0.001 -0.012 -0.012
  -0.008 -0.002 -0.004 0.006 0.003 -0.002 -0.006 0.001 0.007 0.000 -0.005
  -0.003 0.006 0.003 0.008 0.008 -0.001 0.001 -0.000 -0.015 -0.001 0.003
  -0.008 -0.006 -0.001 0.010 -0.007 0.002 0.005 -0.011 0.003 0.008 -0.005
  -0.006 -0.010 -0.000 0.002 -0.018 -0.007 0.003 0.004 0.016 0.000]
 [0.002 -0.009 0.021 -0.034 -0.017 -0.007 0.461 0.019 0.014 0.008 0.012
  0.012 -0.002 0.000 0.000 0.003 -0.003 -0.004 0.007 -0.001 -0.000 0.001
  0.010 -0.001 0.008 0.003 -0.008 -0.003 0.004 -0.006 -0.020 0.005 0.001
  -0.005 0.008 0.005 0.006 -0.005 0.005 -0.002 -0.008 -0.001 0.007 -0.002
  -0.008 0.007 -0.002 0.008 -0.000 0.007 0.005 -0.001 -0.015 -0.000 0.003
  0.005 -0.016 0.006 -0.020 0.009 -0.008 -0.004 0.020 -0.007]
 [0.000 -0.001 -0.009 0.011 0.008 -0.017 0.019 0.389 0.010 -0.009 0.004
  0.000 -0.003 -0.004 -0.003 0.011 -0.002 0.006 0.000 -0.003 0.001 0.004
  0.002 -0.004 0.001 0.003 -0.004 0.006 -0.010 0.002 -0.001 0.004 -0.005
  -0.008 0.004 -0.005 0.002 -0.004 -0.001 -0.012 -0.002 -0.000 -0.012
  0.003 0.001 0.006 -0.000 -0.003 -0.015 -0.003 0.002 -0.004 0.001 -0.005
  0.005 -0.009 0.007 -0.006 0.006 -0.010 -0.003 0.002 -0.006 0.005]
 [-0.038 -0.002 -0.015 -0.049 0.006 -0.006 0.014 0.010 0.435 0.031 0.012
  0.013 0.009 0.003 0.003 -0.003 -0.003 -0.003 -0.002 0.001 -0.002 0.006
  -0.010 -0.005 0.001 -0.001 -0.003 0.001 -0.003 0.001 -0.006 0.000 0.011
  0.014 -0.001 0.001 0.001 0.001 -0.004 0.009 0.005 0.000 0.000 -0.003
  0.002 -0.012 0.002 0.009 -0.002 -0.005 0.003 0.002 -0.006 0.011 0.010
  0.006 -0.001 0.005 -0.013 -0.011 -0.005 -0.010 -0.005 -0.011]
 [-0.028 -0.002 -0.013 -0.056 0.005 0.003 0.008 -0.009 0.031 0.361 -0.005
  0.029 0.001 -0.005 -0.002 0.012 0.002 0.003 -0.012 -0.000 0.001 0.013
  0.002 -0.006 0.002 -0.003 -0.001 0.002 -0.011 -0.009 -0.012 0.003 0.006
  0.012 0.000 -0.003 -0.004 0.006 0.003 -0.011 -0.003 0.008 0.002 0.004
  -0.004 -0.004 0.001 0.002 -0.008 0.002 -0.008 -0.003 0.005 0.004 0.006
  -0.005 -0.016 0.010 0.019 -0.008 0.004 -0.005 0.004 -0.008]
 [0.007 0.012 0.005 -0.026 -0.004 0.000 0.012 0.004 0.012 -0.005 0.399
  -0.019 0.009 0.000 0.023 0.003 0.004 -0.005 0.005 0.008 0.005 -0.002
  -0.008 0.002 -0.004 0.006 0.010 0.002 0.001 -0.005 0.005 -0.004 -0.013
  -0.018 0.008 0.005 -0.003 -0.002 -0.001 0.006 0.005 -0.002 0.002 -0.005
  0.004 0.004 -0.000 -0.003 -0.003 -0.007 -0.003 -0.004 0.005 -0.006
  -0.003 -0.005 -0.004 -0.003 0.009 -0.006 0.006 0.008 0.002 -0.010]
 [0.000 0.005 -0.009 0.000 0.008 -0.000 0.012 0.000 0.013 0.029 -0.019
  0.289 -0.001 -0.010 0.004 -0.006 -0.004 0.002 -0.009 -0.001 0.014
  -0.001 0.012 -0.002 0.003 -0.002 -0.007 0.004 -0.003 -0.008 -0.001
  0.006 0.016 -0.002 0.010 0.003 -0.001 0.001 0.003 -0.007 -0.009 0.002
  -0.001 -0.005 -0.007 -0.016 -0.001 0.005 -0.001 -0.000 0.006 -0.004
  -0.001 0.006 0.007 0.001 0.004 -0.000 -0.012 0.001 0.001 -0.009 -0.011
  -0.010]
 [-0.001 0.001 -0.004 0.013 -0.006 0.010 -0.002 -0.003 0.009 0.001 0.009
  -0.001 0.332 -0.000 0.008 -0.011 0.021 0.007 -0.009 -0.001 0.005 -0.002
  0.001 0.000 -0.010 0.002 -0.011 0.002 -0.005 0.003 -0.006 -0.001 0.010
  -0.005 -0.004 -0.005 0.002 0.004 -0.002 0.004 -0.006 -0.001 0.003 0.001
  0.003 -0.000 -0.006 0.004 0.003 0.002 0.006 -0.005 0.000 -0.005 0.002
  -0.003 0.005 0.005 0.006 -0.005 -0.001 -0.002 -0.002 0.002]
 [0.006 0.014 -0.005 -0.001 -0.007 -0.002 0.000 -0.004 0.003 -0.005 0.000
  -0.010 -0.000 0.288 -0.014 0.009 0.004 0.001 -0.008 0.002 -0.003 0.001
  -0.001 -0.002 0.001 0.003 0.011 -0.000 0.011 -0.000 0.000 0.000 -0.003
  0.007 -0.006 -0.002 -0.007 -0.005 0.002 -0.003 0.000 -0.003 0.005
  -0.004 0.005 -0.002 0.005 -0.008 0.000 -0.002 -0.001 0.004 0.002 -0.001
  0.003 -0.003 0.007 0.002 -0.012 -0.001 -0.000 0.014 0.001 0.002]
 [-0.000 -0.007 0.000 0.023 0.014 0.009 0.000 -0.003 0.003 -0.002 0.023
  0.004 0.008 -0.014 0.361 -0.000 0.006 0.008 0.020 -0.005 -0.014 0.004
  0.003 -0.002 0.001 0.004 -0.004 0.004 -0.001 0.000 -0.001 -0.008 -0.007
  0.004 -0.000 0.005 -0.011 0.002 -0.004 0.004 -0.000 -0.002 -0.002
  -0.001 0.004 -0.002 -0.001 0.003 -0.001 -0.002 0.005 0.002 0.004 0.005
  0.000 -0.004 -0.006 -0.007 -0.007 0.005 -0.001 0.012 -0.006 0.007]
 [-0.003 -0.012 0.011 0.002 -0.002 -0.017 0.003 0.011 -0.003 0.012 0.003
  -0.006 -0.011 0.009 -0.000 0.274 0.008 -0.003 0.005 0.002 -0.001 0.011
  0.002 -0.010 -0.004 0.000 0.002 0.003 0.004 -0.004 -0.006 0.004 -0.002
  0.000 0.005 0.004 -0.004 -0.003 0.000 0.001 -0.006 -0.004 -0.001 0.003
  0.003 0.003 -0.007 0.006 0.003 0.007 0.002 0.004 0.005 -0.002 -0.007
  -0.003 0.005 0.002 -0.000 0.008 0.007 0.005 -0.001 0.002]
 [0.009 -0.007 -0.007 0.016 0.006 -0.006 -0.003 -0.002 -0.003 0.002 0.004
  -0.004 0.021 0.004 0.006 0.008 0.312 0.003 -0.018 -0.012 0.002 0.009
  0.010 0.006 0.010 -0.006 -0.007 -0.000 0.006 -0.008 -0.010 0.002 0.009
  -0.013 -0.012 -0.009 0.004 -0.008 -0.008 0.001 -0.007 -0.000 -0.001
  -0.003 -0.001 -0.005 -0.005 -0.006 0.003 -0.004 0.003 -0.006 -0.000
  0.003 -0.006 0.004 0.013 0.004 0.000 0.001 0.007 0.007 -0.009 0.007]
 [0.005 -0.002 -0.009 -0.012 0.001 -0.004 -0.004 0.006 -0.003 0.003
  -0.005 0.002 0.007 0.001 0.008 -0.003 0.003 0.290 -0.018 0.012 -0.011
  0.002 0.007 -0.007 -0.002 -0.002 0.000 -0.007 -0.001 0.001 -0.007
  -0.001 0.013 0.009 0.003 0.000 -0.002 -0.004 0.001 -0.001 -0.001 0.000
  0.000 -0.002 -0.000 0.003 0.004 -0.006 0.001 0.002 0.001 0.001 0.001
  0.003 -0.006 -0.006 0.003 0.002 -0.002 0.000 0.004 -0.008 0.002 -0.002]
 [-0.000 0.002 0.009 0.004 0.001 0.001 0.007 0.000 -0.002 -0.012 0.005
  -0.009 -0.009 -0.008 0.020 0.005 -0.018 -0.018 0.327 0.002 -0.023 0.001
  -0.001 -0.001 0.011 0.000 0.012 0.000 -0.007 -0.009 -0.003 0.004 0.001
  -0.003 0.008 0.002 0.007 -0.001 0.002 0.005 0.002 -0.004 0.006 0.002
  0.006 -0.001 -0.004 -0.001 -0.001 -0.006 -0.002 -0.001 -0.003 0.004
  0.008 0.002 -0.004 0.002 -0.006 0.002 0.004 0.004 -0.002 0.002]
 [-0.006 -0.014 0.005 -0.012 -0.009 -0.012 -0.001 -0.003 0.001 -0.000
  0.008 -0.001 -0.001 0.002 -0.005 0.002 -0.012 0.012 0.002 0.272 -0.004
  0.002 0.002 -0.001 -0.005 -0.010 0.003 -0.004 -0.007 -0.008 -0.006
  0.004 0.004 0.012 0.002 0.004 0.001 -0.001 0.001 0.001 0.001 0.005
  0.005 -0.002 0.000 0.001 -0.003 0.004 -0.002 0.003 -0.007 0.004 -0.004
  0.004 0.001 -0.000 0.002 -0.005 0.002 0.001 -0.003 0.010 -0.004 0.006]
 [-0.008 -0.011 -0.002 -0.000 -0.016 -0.012 -0.000 0.001 -0.002 0.001
  0.005 0.014 0.005 -0.003 -0.014 -0.001 0.002 -0.011 -0.023 -0.004 0.490
  -0.001 -0.008 -0.003 -0.004 0.001 0.002 0.005 -0.011 -0.005 -0.002
  0.009 -0.000 -0.007 0.007 -0.003 -0.011 -0.004 0.009 0.000 0.009 -0.011
  0.005 0.003 0.002 0.002 -0.005 0.001 -0.002 0.002 -0.005 -0.002 -0.010
  0.004 0.003 0.003 -0.004 -0.006 0.002 -0.004 0.007 0.004 0.008 0.001]
 [-0.007 -0.019 0.002 -0.001 0.017 -0.008 0.001 0.004 0.006 0.013 -0.002
  -0.001 -0.002 0.001 0.004 0.011 0.009 0.002 0.001 0.002 -0.001 0.455
  0.006 -0.001 -0.008 -0.004 -0.009 -0.004 0.003 0.008 0.001 -0.002 0.008
  0.008 -0.006 0.007 -0.004 0.001 0.002 -0.005 -0.004 0.010 -0.007 0.003
  -0.000 -0.006 -0.003 0.010 0.008 0.002 0.002 0.005 0.007 0.013 -0.002
  0.002 0.001 -0.002 -0.001 -0.003 -0.001 -0.007 0.005 -0.003]
 [0.000 -0.008 0.021 0.002 -0.002 -0.002 0.010 0.002 -0.010 0.002 -0.008
  0.012 0.001 -0.001 0.003 0.002 0.010 0.007 -0.001 0.002 -0.008 0.006
  0.443 0.003 -0.009 0.009 -0.002 -0.001 -0.005 0.004 -0.000 -0.007 0.001
  0.009 -0.007 -0.021 0.000 -0.007 -0.005 -0.007 -0.005 0.003 -0.003
  0.002 0.002 -0.001 0.006 0.003 0.001 -0.002 0.004 0.007 -0.000 0.006
  0.005 -0.001 0.002 0.000 -0.006 0.000 0.001 -0.003 -0.002 -0.004]
 [0.018 0.025 -0.008 -0.011 0.002 -0.004 -0.001 -0.004 -0.005 -0.006
  0.002 -0.002 0.000 -0.002 -0.002 -0.010 0.006 -0.007 -0.001 -0.001
  -0.003 -0.001 0.003 0.386 -0.003 0.002 -0.010 0.004 -0.006 -0.006
  -0.003 0.010 0.006 -0.004 0.007 -0.010 -0.003 0.004 -0.008 -0.006
  -0.006 -0.000 -0.000 0.006 -0.005 0.004 0.001 0.006 -0.002 0.001 -0.003
  0.001 0.005 -0.004 -0.000 0.004 0.003 -0.005 0.005 -0.001 -0.001 0.002
  0.001 0.006]
 [-0.000 -0.002 0.021 0.024 0.004 0.006 0.008 0.001 0.001 0.002 -0.004
  0.003 -0.010 0.001 0.001 -0.004 0.010 -0.002 0.011 -0.005 -0.004 -0.008
  -0.009 -0.003 0.362 0.014 -0.003 -0.010 -0.012 -0.009 0.005 -0.001
  0.001 -0.008 -0.009 0.005 -0.008 0.002 -0.001 0.003 -0.004 0.006 0.003
  0.008 0.003 -0.002 0.006 -0.003 0.003 0.010 0.001 -0.000 -0.003 -0.003
  -0.003 -0.001 -0.001 -0.003 -0.008 0.001 -0.002 0.005 -0.012 0.007]
 [0.001 -0.009 0.017 -0.011 -0.005 0.003 0.003 0.003 -0.001 -0.003 0.006
  -0.002 0.002 0.003 0.004 0.000 -0.006 -0.002 0.000 -0.010 0.001 -0.004
  0.009 0.002 0.014 0.321 -0.002 0.013 -0.004 -0.004 0.006 0.000 -0.001
  0.002 -0.003 -0.001 -0.008 0.001 0.001 -0.000 -0.001 0.002 -0.008
  -0.001 -0.002 0.003 0.001 0.002 -0.000 0.002 0.003 0.005 -0.001 -0.003
  -0.005 -0.004 -0.001 -0.000 -0.008 0.004 -0.007 -0.003 -0.000 -0.003]
 [-0.004 0.010 0.012 -0.014 -0.011 -0.002 -0.008 -0.004 -0.003 -0.001
  0.010 -0.007 -0.011 0.011 -0.004 0.002 -0.007 0.000 0.012 0.003 0.002
  -0.009 -0.002 -0.010 -0.003 -0.002 0.368 0.004 0.000 0.001 0.008 -0.005
  0.005 0.004 0.000 0.005 0.005 0.001 0.011 -0.003 0.005 -0.003 0.004
  -0.001 0.005 0.001 -0.003 -0.004 -0.001 -0.008 -0.003 0.009 -0.001
  -0.004 0.003 -0.004 0.000 -0.001 -0.018 0.003 -0.008 -0.003 0.006 0.000]
 [-0.013 0.009 0.005 -0.005 -0.011 -0.006 -0.003 0.006 0.001 0.002 0.002
  0.004 0.002 -0.000 0.004 0.003 -0.000 -0.007 0.000 -0.004 0.005 -0.004
  -0.001 0.004 -0.010 0.013 0.004 0.316 0.015 -0.002 -0.007 -0.000 0.004
  0.003 0.003 -0.001 0.000 -0.000 0.006 0.005 -0.001 -0.001 -0.001 -0.005
  0.006 0.005 -0.005 0.001 -0.004 -0.002 0.001 -0.004 0.005 -0.002 -0.005
  0.000 -0.004 -0.005 -0.003 0.001 0.001 -0.000 -0.000 0.000]
 [0.004 0.009 0.002 -0.017 0.004 0.001 0.004 -0.010 -0.003 -0.011 0.001
  -0.003 -0.005 0.011 -0.001 0.004 0.006 -0.001 -0.007 -0.007 -0.011
  0.003 -0.005 -0.006 -0.012 -0.004 0.000 0.015 0.462 0.009 0.000 -0.007
  -0.006 -0.001 -0.006 0.020 0.005 0.003 -0.008 0.011 0.002 0.002 -0.006
  -0.002 -0.000 0.002 0.003 0.007 0.002 -0.005 -0.005 0.002 -0.005 0.002
  0.001 0.004 0.005 -0.003 -0.000 -0.006 -0.005 -0.003 0.002 -0.005]
 [0.016 -0.005 0.014 0.029 0.008 0.007 -0.006 0.002 0.001 -0.009 -0.005
  -0.008 0.003 -0.000 0.000 -0.004 -0.008 0.001 -0.009 -0.008 -0.005
  0.008 0.004 -0.006 -0.009 -0.004 0.001 -0.002 0.009 0.414 -0.004 0.005
  -0.006 0.018 0.010 0.003 0.003 0.008 0.004 0.000 0.008 -0.008 -0.002
  0.007 0.004 -0.001 0.009 -0.007 -0.001 0.001 -0.004 0.004 0.004 0.001
  -0.005 -0.001 -0.001 -0.005 -0.003 -0.006 0.006 0.002 -0.001 0.002]
 [0.004 0.001 -0.002 0.021 -0.011 0.000 -0.020 -0.001 -0.006 -0.012 0.005
  -0.001 -0.006 0.000 -0.001 -0.006 -0.010 -0.007 -0.003 -0.006 -0.002
  0.001 -0.000 -0.003 0.005 0.006 0.008 -0.007 0.000 -0.004 0.430 0.013
  0.011 -0.012 0.008 -0.005 -0.007 -0.004 0.001 0.004 -0.005 0.000 0.002
  0.006 -0.000 -0.001 -0.006 0.001 0.003 0.008 -0.013 -0.007 -0.003 0.001
  0.008 0.005 -0.002 -0.007 -0.003 0.001 -0.002 0.006 -0.002 -0.002]
 [-0.007 0.001 0.008 -0.013 -0.001 -0.005 0.005 0.004 0.000 0.003 -0.004
  0.006 -0.001 0.000 -0.008 0.004 0.002 -0.001 0.004 0.004 0.009 -0.002
  -0.007 0.010 -0.001 0.000 -0.005 -0.000 -0.007 0.005 0.013 0.375 -0.006
  -0.004 0.005 -0.001 0.002 -0.004 -0.001 -0.003 0.004 -0.003 -0.002
  0.004 -0.011 -0.008 -0.004 0.007 -0.002 0.004 -0.000 0.000 0.002 0.003
  0.006 0.003 0.000 -0.002 -0.003 -0.000 -0.002 -0.002 -0.002 -0.002]
 [0.010 0.006 0.011 -0.004 -0.005 -0.003 0.001 -0.005 0.011 0.006 -0.013
  0.016 0.010 -0.003 -0.007 -0.002 0.009 0.013 0.001 0.004 -0.000 0.008
  0.001 0.006 0.001 -0.001 0.005 0.004 -0.006 -0.006 0.011 -0.006 0.413
  0.015 -0.016 -0.008 -0.005 0.003 0.004 -0.001 0.013 -0.004 0.003 -0.004
  0.004 0.003 -0.006 0.005 0.004 -0.001 0.000 -0.006 -0.002 0.005 0.002
  -0.001 0.011 0.004 -0.008 0.006 0.002 -0.002 0.002 -0.001]
 [0.002 -0.019 -0.008 0.025 0.018 0.006 -0.005 -0.008 0.014 0.012 -0.018
  -0.002 -0.005 0.007 0.004 0.000 -0.013 0.009 -0.003 0.012 -0.007 0.008
  0.009 -0.004 -0.008 0.002 0.004 0.003 -0.001 0.018 -0.012 -0.004 0.015
  0.379 0.003 0.001 -0.003 -0.003 -0.012 0.005 0.003 -0.001 0.001 0.003
  -0.007 -0.003 0.002 0.004 -0.004 0.000 -0.002 0.001 0.002 0.002 -0.001
  -0.002 -0.010 0.001 -0.000 -0.003 -0.003 -0.000 0.003 0.003]
 [0.002 0.030 0.002 -0.011 0.004 0.003 0.008 0.004 -0.001 0.000 0.008
  0.010 -0.004 -0.006 -0.000 0.005 -0.012 0.003 0.008 0.002 0.007 -0.006
  -0.007 0.007 -0.009 -0.003 0.000 0.003 -0.006 0.010 0.008 0.005 -0.016
  0.003 0.370 0.009 0.005 0.000 0.003 -0.001 -0.004 -0.002 -0.006 0.005
  -0.003 0.007 -0.006 -0.004 0.007 0.000 0.000 -0.005 -0.002 0.001 -0.007
  0.002 0.002 -0.005 0.002 0.000 0.003 -0.005 0.000 0.002]
 [0.006 -0.009 0.006 -0.014 0.007 0.008 0.005 -0.005 0.001 -0.003 0.005
  0.003 -0.005 -0.002 0.005 0.004 -0.009 0.000 0.002 0.004 -0.003 0.007
  -0.021 -0.010 0.005 -0.001 0.005 -0.001 0.020 0.003 -0.005 -0.001
  -0.008 0.001 0.009 0.348 0.009 -0.002 0.009 0.008 -0.002 -0.002 -0.000
  -0.000 -0.003 -0.001 0.001 -0.002 -0.006 -0.001 -0.004 0.004 -0.005
  -0.000 0.002 0.000 -0.002 0.003 0.001 -0.006 -0.004 0.002 -0.004 0.003]
 [0.012 0.004 -0.006 -0.015 0.009 0.008 0.006 0.002 0.001 -0.004 -0.003
  -0.001 0.002 -0.007 -0.011 -0.004 0.004 -0.002 0.007 0.001 -0.011
  -0.004 0.000 -0.003 -0.008 -0.008 0.005 0.000 0.005 0.003 -0.007 0.002
  -0.005 -0.003 0.005 0.009 0.310 0.003 -0.006 -0.001 -0.003 0.005 0.004
  -0.006 -0.001 -0.007 0.000 0.008 -0.005 0.002 -0.002 -0.004 -0.005
  0.002 0.004 0.007 0.002 -0.000 0.012 -0.011 0.005 0.008 -0.003 0.002]
 [-0.004 0.008 -0.001 -0.006 0.006 -0.001 -0.005 -0.004 0.001 0.006
  -0.002 0.001 0.004 -0.005 0.002 -0.003 -0.008 -0.004 -0.001 -0.001
  -0.004 0.001 -0.007 0.004 0.002 0.001 0.001 -0.000 0.003 0.008 -0.004
  -0.004 0.003 -0.003 0.000 -0.002 0.003 0.283 -0.006 0.011 -0.003 0.003
  0.002 -0.003 -0.001 0.001 0.000 0.002 0.003 -0.003 -0.000 0.002 -0.003
  -0.000 0.002 -0.005 -0.002 -0.002 0.002 -0.004 -0.006 -0.001 0.002
  0.002]
 [-0.010 -0.001 0.004 -0.024 -0.006 0.001 0.005 -0.001 -0.004 0.003
  -0.001 0.003 -0.002 0.002 -0.004 0.000 -0.008 0.001 0.002 0.001 0.009
  0.002 -0.005 -0.008 -0.001 0.001 0.011 0.006 -0.008 0.004 0.001 -0.001
  0.004 -0.012 0.003 0.009 -0.006 -0.006 0.316 0.007 0.012 -0.004 -0.000
  -0.000 -0.001 0.006 -0.004 0.001 0.005 0.003 0.001 -0.000 0.003 -0.001
  0.005 -0.001 -0.001 0.002 -0.003 0.002 -0.002 0.002 -0.001 -0.000]
 [-0.009 0.001 0.015 0.001 -0.000 -0.000 -0.002 -0.012 0.009 -0.011 0.006
  -0.007 0.004 -0.003 0.004 0.001 0.001 -0.001 0.005 0.001 0.000 -0.005
  -0.007 -0.006 0.003 -0.000 -0.003 0.005 0.011 0.000 0.004 -0.003 -0.001
  0.005 -0.001 0.008 -0.001 0.011 0.007 0.250 -0.002 -0.000 0.005 0.001
  -0.001 -0.003 0.001 -0.006 0.002 -0.004 -0.002 0.001 0.006 0.004 0.003
  0.002 0.001 -0.003 -0.001 -0.000 0.002 -0.004 -0.004 -0.000]
 [-0.008 -0.003 -0.007 0.003 0.008 -0.015 -0.008 -0.002 0.005 -0.003
  0.005 -0.009 -0.006 0.000 -0.000 -0.006 -0.007 -0.001 0.002 0.001 0.009
  -0.004 -0.005 -0.006 -0.004 -0.001 0.005 -0.001 0.002 0.008 -0.005
  0.004 0.013 0.003 -0.004 -0.002 -0.003 -0.003 0.012 -0.002 0.352 0.004
  0.007 -0.003 0.006 0.006 0.005 0.006 -0.000 -0.001 -0.002 0.004 -0.000
  0.002 0.007 0.004 -0.002 0.004 -0.007 -0.004 0.002 -0.000 -0.001 -0.001]
 [0.014 -0.004 -0.013 0.004 -0.004 -0.001 -0.001 -0.000 0.000 0.008
  -0.002 0.002 -0.001 -0.003 -0.002 -0.004 -0.000 0.000 -0.004 0.005
  -0.011 0.010 0.003 -0.000 0.006 0.002 -0.003 -0.001 0.002 -0.008 0.000
  -0.003 -0.004 -0.001 -0.002 -0.002 0.005 0.003 -0.004 -0.000 0.004
  0.298 0.016 0.028 0.000 0.008 0.009 -0.005 0.002 0.002 0.003 -0.003
  -0.002 0.000 0.009 -0.004 -0.006 -0.001 -0.002 0.002 0.005 0.008 -0.005
  0.002]
 [-0.005 0.003 0.009 0.020 -0.005 0.003 0.007 -0.012 0.000 0.002 0.002
  -0.001 0.003 0.005 -0.002 -0.001 -0.001 0.000 0.006 0.005 0.005 -0.007
  -0.003 -0.000 0.003 -0.008 0.004 -0.001 -0.006 -0.002 0.002 -0.002
  0.003 0.001 -0.006 -0.000 0.004 0.002 -0.000 0.005 0.007 0.016 0.297
  -0.008 0.003 0.006 -0.005 0.004 0.001 -0.004 -0.007 0.004 0.002 0.003
  0.003 -0.003 -0.004 -0.002 -0.000 0.007 0.003 0.002 0.003 -0.000]
 [0.001 0.001 0.003 -0.009 0.002 -0.008 -0.002 0.003 -0.003 0.004 -0.005
  -0.005 0.001 -0.004 -0.001 0.003 -0.003 -0.002 0.002 -0.002 0.003 0.003
  0.002 0.006 0.008 -0.001 -0.001 -0.005 -0.002 0.007 0.006 0.004 -0.004
  0.003 0.005 -0.000 -0.006 -0.003 -0.000 0.001 -0.003 0.028 -0.008 0.249
  -0.000 0.000 -0.003 0.001 -0.005 0.001 0.002 0.001 0.001 -0.004 -0.003
  -0.001 0.003 -0.003 -0.004 0.001 -0.002 -0.002 -0.002 0.002]
 [0.000 -0.004 0.000 0.004 -0.004 -0.006 -0.008 0.001 0.002 -0.004 0.004
  -0.007 0.003 0.005 0.004 0.003 -0.001 -0.000 0.006 0.000 0.002 -0.000
  0.002 -0.005 0.003 -0.002 0.005 0.006 -0.000 0.004 -0.000 -0.011 0.004
  -0.007 -0.003 -0.003 -0.001 -0.001 -0.001 -0.001 0.006 0.000 0.003
  -0.000 0.246 0.001 -0.004 -0.007 0.001 0.003 -0.003 -0.005 -0.003 0.003
  -0.000 -0.001 0.001 0.001 -0.004 0.000 0.005 0.002 -0.006 0.000]
 [0.006 -0.009 -0.009 -0.003 -0.007 -0.001 0.007 0.006 -0.012 -0.004
  0.004 -0.016 -0.000 -0.002 -0.002 0.003 -0.005 0.003 -0.001 0.001 0.002
  -0.006 -0.001 0.004 -0.002 0.003 0.001 0.005 0.002 -0.001 -0.001 -0.008
  0.003 -0.003 0.007 -0.001 -0.007 0.001 0.006 -0.003 0.006 0.008 0.006
  0.000 0.001 0.232 0.010 0.010 -0.005 -0.000 0.004 0.006 -0.005 -0.007
  0.000 -0.005 -0.001 0.002 0.006 0.000 0.005 0.002 0.000 -0.004]
 [-0.005 0.003 -0.006 -0.023 -0.003 0.010 -0.002 -0.000 0.002 0.001
  -0.000 -0.001 -0.006 0.005 -0.001 -0.007 -0.005 0.004 -0.004 -0.003
  -0.005 -0.003 0.006 0.001 0.006 0.001 -0.003 -0.005 0.003 0.009 -0.006
  -0.004 -0.006 0.002 -0.006 0.001 0.000 0.000 -0.004 0.001 0.005 0.009
  -0.005 -0.003 -0.004 0.010 0.243 -0.002 0.001 -0.004 -0.000 -0.001
  0.003 -0.003 0.002 0.001 0.003 0.003 -0.000 -0.004 0.001 0.000 -0.000
  -0.000]
 [-0.009 0.017 -0.002 -0.006 -0.007 -0.007 0.008 -0.003 0.009 0.002
  -0.003 0.005 0.004 -0.008 0.003 0.006 -0.006 -0.006 -0.001 0.004 0.001
  0.010 0.003 0.006 -0.003 0.002 -0.004 0.001 0.007 -0.007 0.001 0.007
  0.005 0.004 -0.004 -0.002 0.008 0.002 0.001 -0.006 0.006 -0.005 0.004
  0.001 -0.007 0.010 -0.002 0.230 -0.004 -0.005 0.000 -0.002 0.001 -0.003
  0.003 0.001 0.001 0.001 -0.004 0.001 -0.001 -0.002 0.001 -0.003]
 [-0.008 -0.007 -0.000 0.007 0.004 0.002 -0.000 -0.015 -0.002 -0.008
  -0.003 -0.001 0.003 0.000 -0.001 0.003 0.003 0.001 -0.001 -0.002 -0.002
  0.008 0.001 -0.002 0.003 -0.000 -0.001 -0.004 0.002 -0.001 0.003 -0.002
  0.004 -0.004 0.007 -0.006 -0.005 0.003 0.005 0.002 -0.000 0.002 0.001
  -0.005 0.001 -0.005 0.001 -0.004 0.228 0.009 0.011 -0.024 -0.001 0.001
  0.019 0.017 0.004 -0.003 0.003 -0.000 0.001 0.000 0.001 0.001]
 [0.006 -0.004 -0.000 -0.022 0.004 0.005 0.007 -0.003 -0.005 0.002 -0.007
  -0.000 0.002 -0.002 -0.002 0.007 -0.004 0.002 -0.006 0.003 0.002 0.002
  -0.002 0.001 0.010 0.002 -0.008 -0.002 -0.005 0.001 0.008 0.004 -0.001
  0.000 0.000 -0.001 0.002 -0.003 0.003 -0.004 -0.001 0.002 -0.004 0.001
  0.003 -0.000 -0.004 -0.005 0.009 0.199 0.012 0.013 0.003 -0.000 0.001
  0.005 -0.004 0.001 -0.002 -0.001 0.005 0.000 0.003 0.001]
 [0.010 0.006 -0.001 -0.008 -0.001 -0.011 0.005 0.002 0.003 -0.008 -0.003
  0.006 0.006 -0.001 0.005 0.002 0.003 0.001 -0.002 -0.007 -0.005 0.002
  0.004 -0.003 0.001 0.003 -0.003 0.001 -0.005 -0.004 -0.013 -0.000 0.000
  -0.002 0.000 -0.004 -0.002 -0.000 0.001 -0.002 -0.002 0.003 -0.007
  0.002 -0.003 0.004 -0.000 0.000 0.011 0.012 0.182 0.014 -0.001 -0.002
  -0.011 -0.013 -0.001 0.002 0.000 -0.003 0.001 -0.006 0.001 -0.004]
 [-0.004 0.002 0.005 -0.017 0.001 0.003 -0.001 -0.004 0.002 -0.003 -0.004
  -0.004 -0.005 0.004 0.002 0.004 -0.006 0.001 -0.001 0.004 -0.002 0.005
  0.007 0.001 -0.000 0.005 0.009 -0.004 0.002 0.004 -0.007 0.000 -0.006
  0.001 -0.005 0.004 -0.004 0.002 -0.000 0.001 0.004 -0.003 0.004 0.001
  -0.005 0.006 -0.001 -0.002 -0.024 0.013 0.014 0.168 0.002 0.005 0.008
  0.005 -0.002 0.001 -0.002 0.001 0.003 0.002 -0.001 -0.003]
 [-0.024 -0.038 0.045 -0.024 0.004 0.008 -0.015 0.001 -0.006 0.005 0.005
  -0.001 0.000 0.002 0.004 0.005 -0.000 0.001 -0.003 -0.004 -0.010 0.007
  -0.000 0.005 -0.003 -0.001 -0.001 0.005 -0.005 0.004 -0.003 0.002
  -0.002 0.002 -0.002 -0.005 -0.005 -0.003 0.003 0.006 -0.000 -0.002
  0.002 0.001 -0.003 -0.005 0.003 0.001 -0.001 0.003 -0.001 0.002 0.047
  -0.013 0.009 -0.014 0.003 0.001 -0.004 0.008 -0.008 -0.000 -0.004 0.003]
 [0.006 -0.023 0.018 -0.012 0.002 -0.005 -0.000 -0.005 0.011 0.004 -0.006
  0.006 -0.005 -0.001 0.005 -0.002 0.003 0.003 0.004 0.004 0.004 0.013
  0.006 -0.004 -0.003 -0.003 -0.004 -0.002 0.002 0.001 0.001 0.003 0.005
  0.002 0.001 -0.000 0.002 -0.000 -0.001 0.004 0.002 0.000 0.003 -0.004
  0.003 -0.007 -0.003 -0.003 0.001 -0.000 -0.002 0.005 -0.013 0.076
  -0.030 0.024 -0.001 -0.005 -0.002 -0.017 0.018 -0.002 0.000 -0.003]
 [-0.004 0.002 -0.012 -0.011 -0.004 -0.006 0.003 0.005 0.010 0.006 -0.003
  0.007 0.002 0.003 0.000 -0.007 -0.006 -0.006 0.008 0.001 0.003 -0.002
  0.005 -0.000 -0.003 -0.005 0.003 -0.005 0.001 -0.005 0.008 0.006 0.002
  -0.001 -0.007 0.002 0.004 0.002 0.005 0.003 0.007 0.009 0.003 -0.003
  -0.000 0.000 0.002 0.003 0.019 0.001 -0.011 0.008 0.009 -0.030 0.126
  -0.016 -0.000 0.000 -0.001 0.004 -0.018 0.002 0.003 -0.005]
 [0.020 0.021 -0.040 -0.048 -0.000 -0.010 0.005 -0.009 0.006 -0.005
  -0.005 0.001 -0.003 -0.003 -0.004 -0.003 0.004 -0.006 0.002 -0.000
  0.003 0.002 -0.001 0.004 -0.001 -0.004 -0.004 0.000 0.004 -0.001 0.005
  0.003 -0.001 -0.002 0.002 0.000 0.007 -0.005 -0.001 0.002 0.004 -0.004
  -0.003 -0.001 -0.001 -0.005 0.001 0.001 0.017 0.005 -0.013 0.005 -0.014
  0.024 -0.016 0.068 -0.003 -0.005 0.003 -0.004 0.011 0.005 -0.001 -0.000]
 [-0.016 -0.029 0.021 0.001 -0.011 -0.000 -0.016 0.007 -0.001 -0.016
  -0.004 0.004 0.005 0.007 -0.006 0.005 0.013 0.003 -0.004 0.002 -0.004
  0.001 0.002 0.003 -0.001 -0.001 0.000 -0.004 0.005 -0.001 -0.002 0.000
  0.011 -0.010 0.002 -0.002 0.002 -0.002 -0.001 0.001 -0.002 -0.006
  -0.004 0.003 0.001 -0.001 0.003 0.001 0.004 -0.004 -0.001 -0.002 0.003
  -0.001 -0.000 -0.003 0.069 0.007 -0.003 -0.004 -0.002 -0.002 -0.004
  -0.003]
 [0.025 -0.033 -0.026 0.001 -0.002 0.002 0.006 -0.006 0.005 0.010 -0.003
  -0.000 0.005 0.002 -0.007 0.002 0.004 0.002 0.002 -0.005 -0.006 -0.002
  0.000 -0.005 -0.003 -0.000 -0.001 -0.005 -0.003 -0.005 -0.007 -0.002
  0.004 0.001 -0.005 0.003 -0.000 -0.002 0.002 -0.003 0.004 -0.001 -0.002
  -0.003 0.001 0.002 0.003 0.001 -0.003 0.001 0.002 0.001 0.001 -0.005
  0.000 -0.005 0.007 0.043 -0.001 0.005 -0.002 -0.003 0.000 0.002]
 [0.025 0.026 -0.019 0.015 0.002 -0.018 -0.020 0.006 -0.013 0.019 0.009
  -0.012 0.006 -0.012 -0.007 -0.000 0.000 -0.002 -0.006 0.002 0.002
  -0.001 -0.006 0.005 -0.008 -0.008 -0.018 -0.003 -0.000 -0.003 -0.003
  -0.003 -0.008 -0.000 0.002 0.001 0.012 0.002 -0.003 -0.001 -0.007
  -0.002 -0.000 -0.004 -0.004 0.006 -0.000 -0.004 0.003 -0.002 0.000
  -0.002 -0.004 -0.002 -0.001 0.003 -0.003 -0.001 0.063 -0.005 0.005
  0.008 -0.000 0.004]
 [0.006 0.002 0.009 0.010 -0.006 -0.007 0.009 -0.010 -0.011 -0.008 -0.006
  0.001 -0.005 -0.001 0.005 0.008 0.001 0.000 0.002 0.001 -0.004 -0.003
  0.000 -0.001 0.001 0.004 0.003 0.001 -0.006 -0.006 0.001 -0.000 0.006
  -0.003 0.000 -0.006 -0.011 -0.004 0.002 -0.000 -0.004 0.002 0.007 0.001
  0.000 0.000 -0.004 0.001 -0.000 -0.001 -0.003 0.001 0.008 -0.017 0.004
  -0.004 -0.004 0.005 -0.005 0.057 -0.007 0.005 -0.007 0.009]
 [0.010 -0.016 0.002 0.017 -0.014 0.003 -0.008 -0.003 -0.005 0.004 0.006
  0.001 -0.001 -0.000 -0.001 0.007 0.007 0.004 0.004 -0.003 0.007 -0.001
  0.001 -0.001 -0.002 -0.007 -0.008 0.001 -0.005 0.006 -0.002 -0.002
  0.002 -0.003 0.003 -0.004 0.005 -0.006 -0.002 0.002 0.002 0.005 0.003
  -0.002 0.005 0.005 0.001 -0.001 0.001 0.005 0.001 0.003 -0.008 0.018
  -0.018 0.011 -0.002 -0.002 0.005 -0.007 0.053 0.009 -0.005 -0.003]
 [0.011 -0.010 0.003 0.007 0.003 0.004 -0.004 0.002 -0.010 -0.005 0.008
  -0.009 -0.002 0.014 0.012 0.005 0.007 -0.008 0.004 0.010 0.004 -0.007
  -0.003 0.002 0.005 -0.003 -0.003 -0.000 -0.003 0.002 0.006 -0.002
  -0.002 -0.000 -0.005 0.002 0.008 -0.001 0.002 -0.004 -0.000 0.008 0.002
  -0.002 0.002 0.002 0.000 -0.002 0.000 0.000 -0.006 0.002 -0.000 -0.002
  0.002 0.005 -0.002 -0.003 0.008 0.005 0.009 0.047 0.003 0.003]
 [0.013 0.005 -0.012 0.003 -0.015 0.016 0.020 -0.006 -0.005 0.004 0.002
  -0.011 -0.002 0.001 -0.006 -0.001 -0.009 0.002 -0.002 -0.004 0.008
  0.005 -0.002 0.001 -0.012 -0.000 0.006 -0.000 0.002 -0.001 -0.002
  -0.002 0.002 0.003 0.000 -0.004 -0.003 0.002 -0.001 -0.004 -0.001
  -0.005 0.003 -0.002 -0.006 0.000 -0.000 0.001 0.001 0.003 0.001 -0.001
  -0.004 0.000 0.003 -0.001 -0.004 0.000 -0.000 -0.007 -0.005 0.003 0.048
  -0.006]
 [0.006 -0.001 -0.003 0.018 0.002 0.000 -0.007 0.005 -0.011 -0.008 -0.010
  -0.010 0.002 0.002 0.007 0.002 0.007 -0.002 0.002 0.006 0.001 -0.003
  -0.004 0.006 0.007 -0.003 0.000 0.000 -0.005 0.002 -0.002 -0.002 -0.001
  0.003 0.002 0.003 0.002 0.002 -0.000 -0.000 -0.001 0.002 -0.000 0.002
  0.000 -0.004 -0.000 -0.003 0.001 0.001 -0.004 -0.003 0.003 -0.003
  -0.005 -0.000 -0.003 0.002 0.004 0.009 -0.003 0.003 -0.006 0.030]]
