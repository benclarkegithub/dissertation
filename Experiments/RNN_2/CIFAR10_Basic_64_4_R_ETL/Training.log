=================================================================
Layer (type:depth-idx)                   Param #
=================================================================
RNN                                      --
├─Canvas: 1-1                            --
│    └─Linear: 2-1                       3,072
├─Encoder: 1-2                           --
│    └─Linear: 2-2                       786,688
│    └─Linear: 2-3                       65,792
├─EncoderEncoderToEncoder: 1-3           --
│    └─Linear: 2-4                       131,328
│    └─Linear: 2-5                       65,792
├─ModuleList: 1-4                        --
│    └─EncoderToLatents: 2-6             --
│    │    └─Linear: 3-1                  1,028
│    │    └─Linear: 3-2                  1,028
│    └─EncoderToLatents: 2-7             --
│    │    └─Linear: 3-3                  1,028
│    │    └─Linear: 3-4                  1,028
│    └─EncoderToLatents: 2-8             --
│    │    └─Linear: 3-5                  1,028
│    │    └─Linear: 3-6                  1,028
│    └─EncoderToLatents: 2-9             --
│    │    └─Linear: 3-7                  1,028
│    │    └─Linear: 3-8                  1,028
│    └─EncoderToLatents: 2-10            --
│    │    └─Linear: 3-9                  1,028
│    │    └─Linear: 3-10                 1,028
│    └─EncoderToLatents: 2-11            --
│    │    └─Linear: 3-11                 1,028
│    │    └─Linear: 3-12                 1,028
│    └─EncoderToLatents: 2-12            --
│    │    └─Linear: 3-13                 1,028
│    │    └─Linear: 3-14                 1,028
│    └─EncoderToLatents: 2-13            --
│    │    └─Linear: 3-15                 1,028
│    │    └─Linear: 3-16                 1,028
│    └─EncoderToLatents: 2-14            --
│    │    └─Linear: 3-17                 1,028
│    │    └─Linear: 3-18                 1,028
│    └─EncoderToLatents: 2-15            --
│    │    └─Linear: 3-19                 1,028
│    │    └─Linear: 3-20                 1,028
│    └─EncoderToLatents: 2-16            --
│    │    └─Linear: 3-21                 1,028
│    │    └─Linear: 3-22                 1,028
│    └─EncoderToLatents: 2-17            --
│    │    └─Linear: 3-23                 1,028
│    │    └─Linear: 3-24                 1,028
│    └─EncoderToLatents: 2-18            --
│    │    └─Linear: 3-25                 1,028
│    │    └─Linear: 3-26                 1,028
│    └─EncoderToLatents: 2-19            --
│    │    └─Linear: 3-27                 1,028
│    │    └─Linear: 3-28                 1,028
│    └─EncoderToLatents: 2-20            --
│    │    └─Linear: 3-29                 1,028
│    │    └─Linear: 3-30                 1,028
│    └─EncoderToLatents: 2-21            --
│    │    └─Linear: 3-31                 1,028
│    │    └─Linear: 3-32                 1,028
├─ModuleList: 1-5                        --
│    └─LatentsToDecoder: 2-22            --
│    │    └─Linear: 3-33                 1,280
│    └─LatentsToDecoder: 2-23            --
│    │    └─Linear: 3-34                 1,280
│    └─LatentsToDecoder: 2-24            --
│    │    └─Linear: 3-35                 1,280
│    └─LatentsToDecoder: 2-25            --
│    │    └─Linear: 3-36                 1,280
│    └─LatentsToDecoder: 2-26            --
│    │    └─Linear: 3-37                 1,280
│    └─LatentsToDecoder: 2-27            --
│    │    └─Linear: 3-38                 1,280
│    └─LatentsToDecoder: 2-28            --
│    │    └─Linear: 3-39                 1,280
│    └─LatentsToDecoder: 2-29            --
│    │    └─Linear: 3-40                 1,280
│    └─LatentsToDecoder: 2-30            --
│    │    └─Linear: 3-41                 1,280
│    └─LatentsToDecoder: 2-31            --
│    │    └─Linear: 3-42                 1,280
│    └─LatentsToDecoder: 2-32            --
│    │    └─Linear: 3-43                 1,280
│    └─LatentsToDecoder: 2-33            --
│    │    └─Linear: 3-44                 1,280
│    └─LatentsToDecoder: 2-34            --
│    │    └─Linear: 3-45                 1,280
│    └─LatentsToDecoder: 2-35            --
│    │    └─Linear: 3-46                 1,280
│    └─LatentsToDecoder: 2-36            --
│    │    └─Linear: 3-47                 1,280
│    └─LatentsToDecoder: 2-37            --
│    │    └─Linear: 3-48                 1,280
├─Decoder: 1-6                           --
│    └─Linear: 2-38                      65,792
│    └─Linear: 2-39                      789,504
=================================================================
Total params: 1,961,344
Trainable params: 1,961,344
Non-trainable params: 0
=================================================================
Training model 1/16...
[Epoch   1 (8.85s)]	ELBO: -16426.104 (-13731.907)	Log prob: -16409.008 (-13713.520)	KLD: 17.094 (18.387)	Grad: 22440.504
[Epoch   2 (9.09s)]	ELBO: -12476.658 (-12096.738)	Log prob: -12457.847 (-12078.346)	KLD: 18.812 (18.393)	Grad: 16458.334
[Epoch   3 (9.14s)]	ELBO: -11922.379 (-12042.204)	Log prob: -11903.397 (-12022.565)	KLD: 18.981 (19.639)	Grad: 14325.632
[Epoch   4 (8.74s)]	ELBO: -11657.201 (-11523.112)	Log prob: -11637.621 (-11503.918)	KLD: 19.581 (19.193)	Grad: 13857.611
[Epoch   5 (9.52s)]	ELBO: -11516.196 (-11630.397)	Log prob: -11496.301 (-11611.195)	KLD: 19.895 (19.202)	Grad: 16202.917
[Epoch   6 (9.40s)]	ELBO: -11370.225 (-11273.015)	Log prob: -11349.882 (-11252.128)	KLD: 20.341 (20.887)	Grad: 14385.878
[Epoch   7 (9.08s)]	ELBO: -11290.817 (-11696.113)	Log prob: -11270.322 (-11675.340)	KLD: 20.496 (20.774)	Grad: 15221.664
[Epoch   8 (9.31s)]	ELBO: -11257.850 (-11371.688)	Log prob: -11237.172 (-11351.306)	KLD: 20.678 (20.381)	Grad: 16739.215
[Epoch   9 (9.41s)]	ELBO: -11136.656 (-11060.349)	Log prob: -11116.106 (-11039.560)	KLD: 20.550 (20.789)	Grad: 14809.017
[Epoch  10 (9.26s)]	ELBO: -11064.246 (-11067.901)	Log prob: -11043.578 (-11046.569)	KLD: 20.668 (21.332)	Grad: 14103.307
Training model 2/16...
[Epoch   1 (14.23s)]	ELBO: -10175.732 (-8658.580)	Log prob: -10141.980 (-8621.978)	KLD: 12.335 (36.602)	Grad: 35432.703
[Epoch   2 (13.78s)]	ELBO: -8547.642 (-8307.332)	Log prob: -8509.486 (-8269.995)	KLD: 17.861 (37.337)	Grad: 27758.326
[Epoch   3 (14.24s)]	ELBO: -8304.358 (-8262.284)	Log prob: -8265.507 (-8225.017)	KLD: 18.394 (37.267)	Grad: 27256.113
[Epoch   4 (12.93s)]	ELBO: -8225.739 (-8248.049)	Log prob: -8187.217 (-8209.605)	KLD: 18.151 (38.444)	Grad: 28041.270
[Epoch   5 (13.91s)]	ELBO: -8170.543 (-8379.407)	Log prob: -8131.645 (-8337.714)	KLD: 18.406 (41.693)	Grad: 31451.322
[Epoch   6 (13.27s)]	ELBO: -8316.744 (-7844.393)	Log prob: -8275.876 (-7802.417)	KLD: 19.086 (41.975)	Grad: 43772.785
[Epoch   7 (13.35s)]	ELBO: -7944.579 (-7724.314)	Log prob: -7904.720 (-7684.138)	KLD: 19.047 (40.176)	Grad: 31726.227
[Epoch   8 (14.46s)]	ELBO: -7809.827 (-7836.104)	Log prob: -7770.389 (-7792.197)	KLD: 18.712 (43.908)	Grad: 32205.363
[Epoch   9 (14.18s)]	ELBO: -7843.817 (-7602.740)	Log prob: -7804.122 (-7563.374)	KLD: 18.856 (39.366)	Grad: 32522.529
[Epoch  10 (16.43s)]	ELBO: -7783.454 (-7643.495)	Log prob: -7743.816 (-7600.963)	KLD: 18.705 (42.531)	Grad: 31523.951
Training model 3/16...
[Epoch   1 (26.76s)]	ELBO: -7671.661 (-7207.941)	Log prob: -7622.823 (-7154.927)	KLD: 9.305 (53.014)	Grad: 67594.805
[Epoch   2 (27.20s)]	ELBO: -7211.614 (-7018.789)	Log prob: -7159.332 (-6963.894)	KLD: 13.737 (54.895)	Grad: 50858.238
[Epoch   3 (27.58s)]	ELBO: -7132.809 (-7005.198)	Log prob: -7078.088 (-6950.935)	KLD: 15.380 (54.263)	Grad: 49154.656
[Epoch   4 (29.42s)]	ELBO: -7038.559 (-6953.023)	Log prob: -6984.076 (-6900.166)	KLD: 15.772 (52.857)	Grad: 45938.996
[Epoch   5 (33.23s)]	ELBO: -6976.175 (-6889.573)	Log prob: -6921.516 (-6836.405)	KLD: 16.065 (53.168)	Grad: 46818.746
[Epoch   6 (32.00s)]	ELBO: -6870.412 (-6713.795)	Log prob: -6815.968 (-6659.403)	KLD: 16.092 (54.392)	Grad: 49135.488
[Epoch   7 (31.48s)]	ELBO: -6712.414 (-6691.756)	Log prob: -6657.195 (-6634.728)	KLD: 16.801 (57.028)	Grad: 48362.152
[Epoch   8 (31.69s)]	ELBO: -6621.609 (-6652.658)	Log prob: -6566.655 (-6595.781)	KLD: 16.602 (56.877)	Grad: 49915.996
[Epoch   9 (30.66s)]	ELBO: -6594.911 (-6483.693)	Log prob: -6539.927 (-6430.675)	KLD: 16.763 (53.017)	Grad: 55288.172
[Epoch  10 (31.98s)]	ELBO: -6520.359 (-6481.497)	Log prob: -6465.064 (-6425.001)	KLD: 16.828 (56.497)	Grad: 53317.473
Training model 4/16...
[Epoch   1 (42.79s)]	ELBO: -6657.356 (-6419.295)	Log prob: -6594.571 (-6359.050)	KLD: 8.236 (60.245)	Grad: 76225.375
[Epoch   2 (43.34s)]	ELBO: -6250.024 (-6104.559)	Log prob: -6183.843 (-6039.832)	KLD: 12.187 (64.727)	Grad: 63587.727
[Epoch   3 (44.18s)]	ELBO: -6058.802 (-6010.128)	Log prob: -5990.614 (-5944.309)	KLD: 14.480 (65.819)	Grad: 68263.984
[Epoch   4 (45.87s)]	ELBO: -5979.366 (-5877.029)	Log prob: -5909.996 (-5810.304)	KLD: 15.574 (66.725)	Grad: 67213.086
[Epoch   5 (45.46s)]	ELBO: -5937.878 (-5809.617)	Log prob: -5868.103 (-5742.982)	KLD: 16.080 (66.635)	Grad: 60725.285
[Epoch   6 (44.64s)]	ELBO: -5866.679 (-5867.649)	Log prob: -5797.397 (-5794.404)	KLD: 15.854 (73.245)	Grad: 66472.094
[Epoch   7 (44.87s)]	ELBO: -5819.187 (-5799.716)	Log prob: -5750.130 (-5730.681)	KLD: 15.646 (69.035)	Grad: 77030.352
[Epoch   8 (45.99s)]	ELBO: -5714.206 (-5641.625)	Log prob: -5644.878 (-5572.029)	KLD: 16.048 (69.596)	Grad: 73058.266
[Epoch   9 (44.20s)]	ELBO: -5543.770 (-5422.893)	Log prob: -5475.015 (-5353.669)	KLD: 15.715 (69.224)	Grad: 67928.469
[Epoch  10 (44.04s)]	ELBO: -5483.835 (-5300.961)	Log prob: -5415.071 (-5232.225)	KLD: 15.724 (68.736)	Grad: 82308.141
Training model 5/16...
[Epoch   1 (56.98s)]	ELBO: -5540.640 (-5374.560)	Log prob: -5466.239 (-5300.169)	KLD: 5.797 (74.391)	Grad: 113663.117
[Epoch   2 (56.56s)]	ELBO: -5283.432 (-5089.118)	Log prob: -5206.309 (-5010.624)	KLD: 9.106 (78.494)	Grad: 102506.773
[Epoch   3 (57.74s)]	ELBO: -5176.762 (-5108.124)	Log prob: -5098.312 (-5028.991)	KLD: 10.765 (79.133)	Grad: 93715.609
[Epoch   4 (58.68s)]	ELBO: -5145.049 (-5169.873)	Log prob: -5064.868 (-5088.993)	KLD: 12.467 (80.880)	Grad: 106161.031
[Epoch   5 (57.90s)]	ELBO: -5074.475 (-5086.132)	Log prob: -4992.774 (-5002.314)	KLD: 13.896 (83.818)	Grad: 104380.258
[Epoch   6 (56.81s)]	ELBO: -5010.046 (-4886.773)	Log prob: -4928.022 (-4808.303)	KLD: 14.695 (78.470)	Grad: 106075.031
[Epoch   7 (57.53s)]	ELBO: -4885.777 (-4759.421)	Log prob: -4803.573 (-4678.940)	KLD: 15.083 (80.480)	Grad: 95232.172
[Epoch   8 (56.42s)]	ELBO: -4834.713 (-4734.916)	Log prob: -4752.823 (-4652.421)	KLD: 14.875 (82.495)	Grad: 108289.453
[Epoch   9 (60.70s)]	ELBO: -4769.741 (-4717.275)	Log prob: -4687.955 (-4634.621)	KLD: 14.987 (82.655)	Grad: 92592.859
[Epoch  10 (57.98s)]	ELBO: -4726.846 (-4701.740)	Log prob: -4645.291 (-4619.539)	KLD: 14.873 (82.201)	Grad: 95654.594
Training model 6/16...
[Epoch   1 (71.48s)]	ELBO: -4993.773 (-4735.635)	Log prob: -4905.717 (-4646.313)	KLD: 6.935 (89.322)	Grad: 155321.047
[Epoch   2 (72.66s)]	ELBO: -1483588480.000 (-5788.007)	Log prob: -7307.271 (-5680.864)	KLD: 1413697920.000 (107.142)	Grad: 387855.062
[Epoch   3 (70.56s)]	ELBO: -5407.123 (-4979.695)	Log prob: -5303.255 (-4873.161)	KLD: 15.235 (106.534)	Grad: 181750.922
[Epoch   4 (68.86s)]	ELBO: -4885.767 (-4667.740)	Log prob: -4784.869 (-4568.014)	KLD: 15.594 (99.726)	Grad: 178192.781
[Epoch   5 (71.16s)]	ELBO: -4741.926 (-4616.553)	Log prob: -4643.933 (-4520.149)	KLD: 14.356 (96.404)	Grad: 141158.562
[Epoch   6 (68.93s)]	ELBO: -4713.100 (-4629.672)	Log prob: -4616.603 (-4532.494)	KLD: 13.771 (97.178)	Grad: 141620.094
[Epoch   7 (71.68s)]	ELBO: -4680.305 (-4599.333)	Log prob: -4584.319 (-4504.738)	KLD: 14.103 (94.595)	Grad: 144766.484
[Epoch   8 (73.01s)]	ELBO: -4626.968 (-4875.494)	Log prob: -4531.004 (-4781.999)	KLD: 13.938 (93.495)	Grad: 154854.766
[Epoch   9 (74.80s)]	ELBO: -4540.754 (-4380.360)	Log prob: -4446.207 (-4285.966)	KLD: 13.680 (94.394)	Grad: 155113.328
[Epoch  10 (74.65s)]	ELBO: -4398.182 (-4287.348)	Log prob: -4303.333 (-4192.812)	KLD: 13.870 (94.535)	Grad: 133684.641
Training model 7/16...
[Epoch   1 (90.98s)]	ELBO: -4584.252 (-4388.147)	Log prob: -4479.799 (-4281.573)	KLD: 9.761 (106.574)	Grad: 188872.797
[Epoch   2 (94.66s)]	ELBO: -4309.339 (-4151.735)	Log prob: -4202.590 (-4045.898)	KLD: 12.337 (105.837)	Grad: 159985.188
[Epoch   3 (92.81s)]	ELBO: -4182.823 (-4060.580)	Log prob: -4075.548 (-3957.589)	KLD: 12.948 (102.991)	Grad: 153532.203
[Epoch   4 (92.73s)]	ELBO: -4107.106 (-4023.834)	Log prob: -4000.282 (-3919.200)	KLD: 12.922 (104.634)	Grad: 148118.297
[Epoch   5 (95.59s)]	ELBO: -4076.843 (-3996.817)	Log prob: -3970.257 (-3892.024)	KLD: 13.001 (104.793)	Grad: 151777.938
[Epoch   6 (94.09s)]	ELBO: -4047.041 (-4130.181)	Log prob: -3940.581 (-4022.582)	KLD: 13.078 (107.600)	Grad: 160297.812
[Epoch   7 (93.05s)]	ELBO: -4033.183 (-3903.489)	Log prob: -3926.627 (-3797.138)	KLD: 13.262 (106.351)	Grad: 168446.250
[Epoch   8 (90.17s)]	ELBO: -3964.846 (-3967.420)	Log prob: -3858.283 (-3853.779)	KLD: 13.270 (113.641)	Grad: 155718.391
[Epoch   9 (89.02s)]	ELBO: -3944.052 (-3787.479)	Log prob: -3837.717 (-3678.102)	KLD: 13.442 (109.377)	Grad: 164474.703
[Epoch  10 (90.48s)]	ELBO: -3900.279 (-3930.301)	Log prob: -3792.009 (-3822.720)	KLD: 13.891 (107.581)	Grad: 180656.391
Training model 8/16...
[Epoch   1 (112.41s)]	ELBO: -4122.118 (-3750.535)	Log prob: -4008.174 (-3633.840)	KLD: 7.429 (116.695)	Grad: 209949.391
[Epoch   2 (109.37s)]	ELBO: -3784.126 (-3744.574)	Log prob: -3668.332 (-3629.723)	KLD: 10.121 (114.851)	Grad: 192242.641
[Epoch   3 (107.90s)]	ELBO: -3705.896 (-3661.668)	Log prob: -3589.284 (-3535.013)	KLD: 11.254 (126.656)	Grad: 172567.703
[Epoch   4 (106.63s)]	ELBO: -3638.892 (-3603.908)	Log prob: -3521.815 (-3486.024)	KLD: 12.111 (117.884)	Grad: 163626.562
[Epoch   5 (112.33s)]	ELBO: -3599.732 (-3578.592)	Log prob: -3483.050 (-3464.953)	KLD: 12.463 (113.638)	Grad: 167848.891
[Epoch   6 (108.23s)]	ELBO: -3663.365 (-3665.726)	Log prob: -3546.073 (-3550.039)	KLD: 13.199 (115.687)	Grad: 228495.375
[Epoch   7 (107.92s)]	ELBO: -3570.102 (-3605.478)	Log prob: -3453.579 (-3493.943)	KLD: 13.133 (111.535)	Grad: 180017.812
[Epoch   8 (110.15s)]	ELBO: -3588.507 (-3538.988)	Log prob: -3469.235 (-3421.810)	KLD: 13.007 (117.178)	Grad: 204481.250
[Epoch   9 (110.96s)]	ELBO: -3557.698 (-3531.765)	Log prob: -3440.434 (-3417.517)	KLD: 12.890 (114.248)	Grad: 194938.016
[Epoch  10 (102.10s)]	ELBO: -3498.583 (-3509.116)	Log prob: -3383.321 (-3392.121)	KLD: 12.912 (116.995)	Grad: 185104.859
Training model 9/16...
[Epoch   1 (118.69s)]	ELBO: -3826.902 (-3519.661)	Log prob: -3705.053 (-3394.243)	KLD: 7.457 (125.418)	Grad: 263022.094
[Epoch   2 (121.27s)]	ELBO: -3473.886 (-3372.475)	Log prob: -3349.392 (-3249.067)	KLD: 10.188 (123.408)	Grad: 245584.953
[Epoch   3 (119.48s)]	ELBO: -3391.860 (-3493.145)	Log prob: -3266.465 (-3357.875)	KLD: 11.323 (135.269)	Grad: 239181.297
[Epoch   4 (116.91s)]	ELBO: -3486.598 (-3355.577)	Log prob: -3360.829 (-3236.308)	KLD: 12.442 (119.269)	Grad: 369277.750
[Epoch   5 (120.24s)]	ELBO: -3315.501 (-3290.962)	Log prob: -3190.520 (-3164.724)	KLD: 12.091 (126.238)	Grad: 247339.891
[Epoch   6 (115.69s)]	ELBO: -3339.689 (-3423.175)	Log prob: -3214.027 (-3298.996)	KLD: 12.311 (124.179)	Grad: 266023.594
[Epoch   7 (114.29s)]	ELBO: -3301.155 (-3326.448)	Log prob: -3175.418 (-3202.096)	KLD: 12.692 (124.352)	Grad: 266504.562
[Epoch   8 (112.99s)]	ELBO: -3264.732 (-3216.910)	Log prob: -3139.832 (-3089.444)	KLD: 12.557 (127.466)	Grad: 232565.125
[Epoch   9 (120.60s)]	ELBO: -3275.503 (-3218.918)	Log prob: -3150.550 (-3095.663)	KLD: 12.524 (123.254)	Grad: 260167.953
[Epoch  10 (115.87s)]	ELBO: -3216.808 (-3241.690)	Log prob: -3092.943 (-3114.979)	KLD: 12.433 (126.711)	Grad: 213834.125
Training model 10/16...
[Epoch   1 (132.79s)]	ELBO: -3530.089 (-3239.820)	Log prob: -3398.405 (-3105.137)	KLD: 7.660 (134.683)	Grad: 246196.500
[Epoch   2 (134.22s)]	ELBO: -3271.485 (-3238.027)	Log prob: -3138.636 (-3102.480)	KLD: 9.839 (135.547)	Grad: 305890.031
[Epoch   3 (139.20s)]	ELBO: -3214.792 (-3294.553)	Log prob: -3081.169 (-3152.493)	KLD: 10.682 (142.060)	Grad: 244152.438
[Epoch   4 (132.54s)]	ELBO: -3192.063 (-3448.458)	Log prob: -3058.480 (-3309.458)	KLD: 10.766 (139.000)	Grad: 237552.172
[Epoch   5 (135.29s)]	ELBO: -3190.305 (-3206.356)	Log prob: -3056.268 (-3068.865)	KLD: 11.385 (137.490)	Grad: 241445.125
[Epoch   6 (134.83s)]	ELBO: -3143.989 (-3139.527)	Log prob: -3010.269 (-3004.416)	KLD: 11.783 (135.111)	Grad: 244382.625
[Epoch   7 (136.06s)]	ELBO: -3146.312 (-3083.137)	Log prob: -3013.218 (-2948.569)	KLD: 11.787 (134.567)	Grad: 303766.250
[Epoch   8 (148.21s)]	ELBO: -3118.986 (-3164.616)	Log prob: -2986.727 (-3032.258)	KLD: 11.326 (132.358)	Grad: 298331.719
[Epoch   9 (149.62s)]	ELBO: -3073.706 (-3071.848)	Log prob: -2937.628 (-2940.773)	KLD: 11.364 (131.075)	Grad: 252617.500
[Epoch  10 (147.22s)]	ELBO: -3033.317 (-2960.888)	Log prob: -2899.008 (-2827.745)	KLD: 11.367 (133.144)	Grad: 258815.656
Training model 11/16...
[Epoch   1 (170.93s)]	ELBO: -3301.379 (-3039.935)	Log prob: -3160.783 (-2895.203)	KLD: 8.200 (144.732)	Grad: 293056.344
[Epoch   2 (162.54s)]	ELBO: -2975.225 (-2886.490)	Log prob: -2832.096 (-2740.622)	KLD: 10.916 (145.867)	Grad: 250830.062
[Epoch   3 (165.57s)]	ELBO: -2910.728 (-2873.535)	Log prob: -2766.798 (-2728.164)	KLD: 11.878 (145.371)	Grad: 239588.000
[Epoch   4 (164.45s)]	ELBO: -2876.636 (-2846.718)	Log prob: -2732.135 (-2700.313)	KLD: 12.124 (146.405)	Grad: 296324.688
[Epoch   5 (160.67s)]	ELBO: -2845.638 (-2832.649)	Log prob: -2700.829 (-2688.001)	KLD: 12.769 (144.648)	Grad: 286889.562
[Epoch   6 (160.51s)]	ELBO: -2805.963 (-2830.815)	Log prob: -2662.748 (-2688.413)	KLD: 12.202 (142.402)	Grad: 264532.312
[Epoch   7 (165.85s)]	ELBO: -2790.972 (-2881.257)	Log prob: -2647.559 (-2734.670)	KLD: 12.367 (146.587)	Grad: 268140.531
[Epoch   8 (163.27s)]	ELBO: -2804.454 (-2821.309)	Log prob: -2661.976 (-2679.442)	KLD: 11.914 (141.867)	Grad: 330085.344
[Epoch   9 (164.96s)]	ELBO: -2797.041 (-2837.119)	Log prob: -2654.868 (-2691.477)	KLD: 11.765 (145.642)	Grad: 297985.750
[Epoch  10 (160.71s)]	ELBO: -2787.914 (-2799.929)	Log prob: -2645.596 (-2657.105)	KLD: 11.636 (142.824)	Grad: 301642.719
Training model 12/16...
[Epoch   1 (181.28s)]	ELBO: -3207.266 (-2855.566)	Log prob: -3056.123 (-2706.093)	KLD: 8.579 (149.473)	Grad: 327919.875
[Epoch   2 (180.36s)]	ELBO: -2808.361 (-2794.747)	Log prob: -2657.030 (-2641.397)	KLD: 10.977 (153.349)	Grad: 265349.844
[Epoch   3 (188.57s)]	ELBO: -2762.688 (-2811.955)	Log prob: -2610.402 (-2658.285)	KLD: 12.373 (153.670)	Grad: 284278.188
[Epoch   4 (186.34s)]	ELBO: -2775.362 (-2807.876)	Log prob: -2623.163 (-2652.655)	KLD: 12.874 (155.221)	Grad: 335127.625
[Epoch   5 (190.30s)]	ELBO: -2700.720 (-2688.175)	Log prob: -2550.000 (-2538.607)	KLD: 12.042 (149.568)	Grad: 257049.203
[Epoch   6 (186.83s)]	ELBO: -2718.687 (-2856.051)	Log prob: -2567.839 (-2705.358)	KLD: 12.094 (150.693)	Grad: 271856.969
[Epoch   7 (187.95s)]	ELBO: -2692.582 (-2683.550)	Log prob: -2541.532 (-2533.094)	KLD: 12.326 (150.457)	Grad: 295493.906
[Epoch   8 (186.11s)]	ELBO: -2695.181 (-2684.412)	Log prob: -2543.784 (-2536.384)	KLD: 11.730 (148.029)	Grad: 360912.875
[Epoch   9 (187.66s)]	ELBO: -2647.675 (-2671.981)	Log prob: -2496.732 (-2521.992)	KLD: 11.818 (149.989)	Grad: 348500.375
[Epoch  10 (183.06s)]	ELBO: -2599.069 (-2639.241)	Log prob: -2447.541 (-2486.985)	KLD: 11.776 (152.256)	Grad: 341022.844
Training model 13/16...
[Epoch   1 (209.09s)]	ELBO: -3201.241 (-2572.439)	Log prob: -3041.711 (-2416.408)	KLD: 6.957 (156.032)	Grad: 404202.250
[Epoch   2 (206.23s)]	ELBO: -2558.259 (-2551.267)	Log prob: -2397.673 (-2391.471)	KLD: 9.817 (159.796)	Grad: 339282.938
[Epoch   3 (209.21s)]	ELBO: -2519.183 (-2591.126)	Log prob: -2359.515 (-2430.773)	KLD: 9.959 (160.353)	Grad: 398562.062
[Epoch   4 (206.24s)]	ELBO: -2469.467 (-2446.948)	Log prob: -2309.104 (-2285.704)	KLD: 11.046 (161.244)	Grad: 362369.625
[Epoch   5 (206.83s)]	ELBO: -2441.753 (-2458.754)	Log prob: -2281.438 (-2301.220)	KLD: 11.286 (157.534)	Grad: 389719.531
[Epoch   6 (214.16s)]	ELBO: -2410.062 (-2393.970)	Log prob: -2250.071 (-2240.305)	KLD: 11.480 (153.665)	Grad: 339265.688
[Epoch   7 (211.70s)]	ELBO: -2394.077 (-2470.375)	Log prob: -2234.598 (-2311.322)	KLD: 11.325 (159.052)	Grad: 337518.844
[Epoch   8 (215.89s)]	ELBO: -3129762.750 (-2610.093)	Log prob: -3333.733 (-2446.556)	KLD: 17.210 (163.537)	Grad: 546975.625
[Epoch   9 (217.80s)]	ELBO: -2511.834 (-2425.231)	Log prob: -2350.316 (-2264.329)	KLD: 12.652 (160.902)	Grad: 323808.531
[Epoch  10 (220.07s)]	ELBO: -2416.213 (-2405.415)	Log prob: -2255.985 (-2247.057)	KLD: 12.119 (158.358)	Grad: 317209.562
Training model 14/16...
[Epoch   1 (245.19s)]	ELBO: -2917.488 (-2486.262)	Log prob: -2748.502 (-2320.024)	KLD: 10.388 (166.238)	Grad: 527962.000
[Epoch   2 (239.86s)]	ELBO: -2451.071 (-2426.573)	Log prob: -2282.238 (-2258.448)	KLD: 12.014 (168.125)	Grad: 378085.594
[Epoch   3 (234.45s)]	ELBO: -2456.713 (-2491.566)	Log prob: -2288.258 (-2317.224)	KLD: 11.189 (174.342)	Grad: 415274.938
[Epoch   4 (232.76s)]	ELBO: -2464.950 (-2928.937)	Log prob: -2296.341 (-2751.811)	KLD: 12.887 (177.125)	Grad: 541715.500
[Epoch   5 (232.07s)]	ELBO: -2415.262 (-2406.478)	Log prob: -2247.593 (-2238.575)	KLD: 12.082 (167.903)	Grad: 402721.375
[Epoch   6 (228.16s)]	ELBO: -2390.976 (-2354.658)	Log prob: -2224.075 (-2188.010)	KLD: 11.761 (166.649)	Grad: 404301.844
[Epoch   7 (228.09s)]	ELBO: -2391.364 (-2469.911)	Log prob: -2223.588 (-2305.173)	KLD: 11.760 (164.738)	Grad: 416805.812
[Epoch   8 (230.21s)]	ELBO: -2345.704 (-2414.878)	Log prob: -2179.492 (-2245.427)	KLD: 11.318 (169.451)	Grad: 377973.812
[Epoch   9 (226.94s)]	ELBO: -2318.031 (-2365.825)	Log prob: -2153.061 (-2199.753)	KLD: 10.996 (166.071)	Grad: 353914.344
[Epoch  10 (228.97s)]	ELBO: -2279.489 (-2302.252)	Log prob: -2115.513 (-2138.785)	KLD: 10.860 (163.467)	Grad: 291610.156
Training model 15/16...
[Epoch   1 (254.15s)]	ELBO: -2739.092 (-2370.915)	Log prob: -2567.796 (-2197.692)	KLD: 8.040 (173.223)	Grad: 342426.938
[Epoch   2 (256.15s)]	ELBO: -2334.681 (-2269.602)	Log prob: -2161.896 (-2093.907)	KLD: 10.065 (175.695)	Grad: 292942.750
[Epoch   3 (253.26s)]	ELBO: -2243.271 (-2235.649)	Log prob: -2067.907 (-2060.972)	KLD: 11.565 (174.677)	Grad: 331636.438
[Epoch   4 (256.01s)]	ELBO: -2221.216 (-2269.078)	Log prob: -2045.675 (-2091.175)	KLD: 12.001 (177.902)	Grad: 363156.594
[Epoch   5 (193.38s)]	ELBO: -2151.546 (-2184.846)	Log prob: -1976.946 (-2010.289)	KLD: 12.257 (174.556)	Grad: 310790.875
[Epoch   6 (179.24s)]	ELBO: -2159.246 (-2147.153)	Log prob: -1982.934 (-1973.643)	KLD: 12.382 (173.510)	Grad: 315192.969
[Epoch   7 (180.20s)]	ELBO: -2128.436 (-2235.056)	Log prob: -1954.459 (-2060.960)	KLD: 12.076 (174.095)	Grad: 306272.875
[Epoch   8 (178.99s)]	ELBO: -2121.236 (-2178.589)	Log prob: -1948.240 (-2007.231)	KLD: 11.875 (171.358)	Grad: 302473.000
[Epoch   9 (179.60s)]	ELBO: -2178.849 (-2231.788)	Log prob: -2005.039 (-2057.670)	KLD: 12.173 (174.117)	Grad: 384805.969
[Epoch  10 (181.32s)]	ELBO: -2185.768 (-2247.016)	Log prob: -2013.079 (-2074.150)	KLD: 12.132 (172.866)	Grad: 438999.219
Training model 16/16...
[Epoch   1 (200.32s)]	ELBO: -2775.095 (-2314.582)	Log prob: -2595.784 (-2137.231)	KLD: 7.186 (177.351)	Grad: 425126.938
[Epoch   2 (198.82s)]	ELBO: -2211.812 (-2227.392)	Log prob: -2032.667 (-2051.401)	KLD: 9.761 (175.992)	Grad: 399922.062
[Epoch   3 (199.85s)]	ELBO: -2152.209 (-2253.889)	Log prob: -1973.576 (-2077.832)	KLD: 11.166 (176.058)	Grad: 376303.562
[Epoch   4 (198.50s)]	ELBO: -2145.997 (-2186.224)	Log prob: -1968.577 (-2008.086)	KLD: 11.343 (178.138)	Grad: 416566.375
[Epoch   5 (199.69s)]	ELBO: -2168.976 (-2257.667)	Log prob: -1992.498 (-2082.662)	KLD: 11.477 (175.006)	Grad: 439797.812
[Epoch   6 (201.97s)]	ELBO: -2111.479 (-2165.171)	Log prob: -1936.458 (-1991.562)	KLD: 12.007 (173.609)	Grad: 365382.188
[Epoch   7 (201.29s)]	ELBO: -2102.911 (-2257.881)	Log prob: -1929.066 (-2080.909)	KLD: 12.001 (176.972)	Grad: 356608.469
[Epoch   8 (201.12s)]	ELBO: -2125.190 (-2172.790)	Log prob: -1951.215 (-2000.960)	KLD: 12.085 (171.830)	Grad: 395765.844
[Epoch   9 (198.68s)]	ELBO: -2095.129 (-2247.854)	Log prob: -1922.725 (-2077.120)	KLD: 11.954 (170.734)	Grad: 386410.656
[Epoch  10 (201.11s)]	ELBO: -2076.879 (-2125.131)	Log prob: -1905.651 (-1955.688)	KLD: 11.619 (169.443)	Grad: 356919.375
Best epoch(s): [9, 9, 10, 10, 10, 10, 9, 10, 8, 10, 10, 10, 6, 10, 6, 10]	Training time(s): 91.80s, 140.78s, 302.00s, 445.39s, 577.30s, 717.77s, 923.59s, 1088.00s, 1176.02s, 1389.98s, 1639.46s, 1858.47s, 2117.22s, 2326.70s, 2112.30s, 2001.35s (18908.12s)	Best ELBO: -2076.879 (-2125.131)	Best log prob: -1905.651 (-1955.688)
Avg. mu: -0.720, 0.852, -0.819, -0.466, 0.103, -0.264, -0.254, -0.236, -0.002, -0.169, 0.070, 0.127, 0.118, 0.231, -0.026, -0.173, 0.027, -0.281, 0.063, 0.174, 0.054, 0.145, -0.057, -0.513, -0.152, -0.155, -0.328, -0.646, 0.387, 0.143, -0.271, -0.291, -0.373, -0.125, -0.763, -0.236, 0.274, 0.062, 0.396, 0.806, 0.176, -0.666, 0.271, 0.536, -0.386, 0.407, -0.161, -0.144, -0.667, 0.120, 0.864, 0.292, 0.917, -0.990, -0.588, 0.669, -0.271, 0.187, 0.003, -0.328, -0.259, 0.332, -0.100, -0.031
Avg. var: 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.002, 0.002, 0.003, 0.003, 0.003, 0.003, 0.003, 0.004, 0.003, 0.002, 0.008, 0.007, 0.003, 0.003, 0.008, 0.015, 0.005, 0.010, 0.013, 0.020, 0.004, 0.021, 0.018, 0.014, 0.014, 0.009, 0.024, 0.007, 0.012, 0.010, 0.007, 0.004, 0.012, 0.027, 0.021, 0.036, 0.034, 0.041, 0.095, 0.116, 0.089, 0.059, 0.006, 0.005, 0.013, 0.003, 0.002, 0.002, 0.004, 0.005
Max. mu: 5.921, 7.633, 4.712, 5.240, 6.595, 5.898, 5.377, 5.644, 8.125, 5.653, 6.065, 5.657, 6.164, 5.648, 5.497, 6.729, 5.125, 5.490, 4.691, 5.683, 5.042, 4.235, 4.465, 4.150, 5.063, 3.554, 2.888, 3.041, 3.896, 5.582, 3.128, 5.688, 3.373, 4.860, 1.195, 3.626, 3.496, 4.110, 6.645, 4.380, 5.029, 1.734, 2.871, 3.155, 1.008, 3.380, 2.430, 4.760, 0.610, 2.003, 2.950, 3.032, 2.405, -0.289, 0.299, 1.898, 0.066, 0.550, 0.777, 0.011, -0.057, 0.681, 0.177, 0.266
Max. var: 0.002, 0.003, 0.003, 0.002, 0.003, 0.004, 0.005, 0.002, 0.004, 0.004, 0.004, 0.005, 0.006, 0.004, 0.005, 0.003, 0.005, 0.006, 0.004, 0.006, 0.006, 0.008, 0.008, 0.004, 0.027, 0.017, 0.007, 0.005, 0.013, 0.025, 0.011, 0.020, 0.020, 0.030, 0.008, 0.032, 0.031, 0.026, 0.031, 0.022, 0.068, 0.018, 0.039, 0.062, 0.026, 0.029, 0.053, 0.082, 0.082, 0.130, 0.142, 0.110, 0.366, 0.200, 0.223, 0.147, 0.144, 0.098, 0.155, 0.089, 0.129, 0.120, 0.182, 0.138
Min. mu: -7.398, -3.705, -6.949, -9.351, -6.345, -6.773, -8.184, -8.081, -5.177, -6.504, -4.790, -6.456, -4.995, -4.751, -5.996, -5.700, -4.287, -5.395, -6.324, -5.144, -6.298, -6.347, -7.337, -5.064, -5.426, -5.874, -3.495, -4.159, -4.430, -6.794, -6.204, -5.948, -3.405, -5.078, -3.185, -5.727, -2.815, -3.402, -4.247, -3.241, -2.916, -4.653, -2.883, -1.307, -2.813, -0.800, -2.705, -2.303, -3.982, -1.713, -0.184, -1.339, 0.383, -2.967, -1.420, 0.384, -0.565, -0.919, -0.457, -0.770, -0.553, 0.115, -0.811, -0.594
Min. var: 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.000, 0.005, 0.003, 0.002, 0.001, 0.003, 0.005, 0.002, 0.005, 0.002, 0.007, 0.000, 0.005, 0.002, 0.002, 0.000, 0.001, 0.003, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.003, 0.000, 0.000, 0.000, 0.000, 0.000, 0.001, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000
Cov. mu:
[[3.310 -0.421 -0.406 0.418 0.107 0.009 -0.082 -0.005 -0.036 -0.033
  -0.227 -0.235 0.085 -0.021 0.046 -0.078 0.135 0.102 -0.096 0.071 -0.008
  -0.277 -0.375 -0.106 0.252 0.143 0.130 -0.129 0.208 0.078 0.116 -0.161
  -0.017 -0.005 -0.004 0.179 0.011 -0.020 -0.100 -0.021 0.087 0.023
  -0.009 0.041 0.022 0.056 -0.055 0.024 -0.004 0.007 -0.031 0.014 0.002
  0.063 0.004 -0.026 -0.008 0.009 -0.039 0.022 0.005 -0.020 0.008 -0.003]
 [-0.421 1.692 0.448 -0.274 -0.009 0.000 -0.038 -0.038 -0.036 0.053 0.101
  0.111 0.044 0.008 0.017 -0.011 -0.060 0.023 0.033 -0.019 0.014 0.059
  0.073 0.015 -0.156 -0.181 0.017 -0.043 -0.048 0.004 -0.096 0.079 0.032
  0.012 -0.025 -0.081 -0.041 0.062 0.062 -0.036 -0.048 -0.017 0.077
  -0.005 0.006 0.036 -0.006 0.030 0.018 0.045 -0.029 -0.019 -0.011 0.023
  0.005 -0.016 0.010 -0.013 -0.009 0.006 0.001 -0.007 -0.002 0.001]
 [-0.406 0.448 2.060 -0.787 -0.027 0.001 -0.104 -0.044 -0.032 0.031 0.170
  0.141 -0.001 -0.039 -0.033 0.075 -0.016 0.008 0.078 0.003 0.075 0.107
  0.103 0.016 -0.178 -0.219 -0.133 0.037 -0.089 0.030 -0.020 0.063 0.067
  0.026 0.025 -0.049 0.006 0.085 0.023 -0.047 -0.015 0.031 0.055 -0.015
  -0.001 -0.022 0.008 0.051 0.011 0.088 -0.060 0.012 -0.071 0.088 0.048
  -0.049 0.023 -0.007 -0.014 0.020 0.007 -0.025 0.004 0.006]
 [0.418 -0.274 -0.787 3.261 0.044 -0.045 -0.123 0.033 0.019 -0.012 -0.214
  -0.068 0.014 0.056 -0.003 -0.097 0.098 0.021 -0.083 0.049 -0.109 -0.200
  -0.334 -0.015 0.372 0.436 0.179 -0.177 0.233 0.148 0.135 -0.046 -0.066
  0.002 0.001 -0.004 -0.064 -0.050 0.001 0.020 -0.119 0.053 -0.043 0.028
  -0.001 0.038 0.036 -0.093 -0.103 -0.051 0.030 -0.023 0.063 -0.095
  -0.041 0.028 -0.035 -0.001 -0.003 -0.019 -0.015 0.032 -0.002 -0.012]
 [0.107 -0.009 -0.027 0.044 1.912 0.115 0.050 -0.020 0.000 -0.062 0.001
  -0.067 0.048 0.004 -0.019 -0.022 0.060 -0.015 -0.033 -0.007 -0.089
  -0.046 -0.036 -0.014 0.100 0.043 -0.031 -0.015 0.161 0.146 0.010 0.057
  -0.040 -0.006 0.017 0.025 0.009 0.013 0.029 -0.023 0.027 0.034 -0.032
  0.027 -0.016 -0.002 0.003 0.037 0.018 -0.011 -0.007 0.050 -0.005 0.006
  0.001 0.001 -0.003 -0.001 -0.001 0.001 0.000 0.003 -0.001 0.003]
 [0.009 0.000 0.001 -0.045 0.115 2.013 0.024 -0.125 -0.019 -0.014 0.042
  -0.003 -0.062 -0.026 -0.023 0.012 0.054 0.036 -0.001 -0.061 -0.047
  -0.019 -0.028 -0.001 -0.000 0.114 0.069 -0.060 -0.041 -0.032 -0.004
  -0.058 -0.010 -0.019 -0.011 -0.026 -0.017 0.011 0.020 -0.002 -0.001
  -0.010 0.003 -0.074 -0.060 -0.005 -0.066 -0.059 0.207 -0.120 -0.041
  -0.002 -0.011 0.046 0.009 0.004 0.024 0.005 0.005 0.002 0.006 -0.011
  0.003 -0.001]
 [-0.082 -0.038 -0.104 -0.123 0.050 0.024 1.924 -0.006 -0.024 0.028
  -0.023 -0.049 -0.074 -0.049 0.016 -0.004 -0.043 0.025 -0.030 -0.054
  0.029 0.097 0.064 0.017 -0.077 0.033 0.038 0.003 -0.041 -0.006 0.026
  -0.042 0.022 -0.039 0.030 0.095 -0.005 0.013 0.025 -0.027 -0.009 -0.036
  -0.002 -0.008 -0.045 0.001 -0.105 -0.042 0.132 -0.137 -0.023 0.049
  -0.018 0.048 0.015 -0.008 0.008 0.007 -0.004 0.009 0.002 -0.014 0.006
  -0.005]
 [-0.005 -0.038 -0.044 0.033 -0.020 -0.125 -0.006 1.923 -0.054 -0.020
  -0.025 -0.002 -0.004 -0.005 -0.011 0.008 0.018 0.000 0.018 0.037 -0.008
  -0.021 -0.017 0.042 0.038 -0.012 -0.003 0.002 0.088 0.077 -0.001 0.053
  -0.039 0.014 -0.018 0.002 0.039 0.071 0.056 -0.047 -0.011 -0.039 -0.069
  -0.015 -0.044 0.023 -0.033 -0.036 0.154 -0.133 0.046 -0.050 -0.007
  0.055 -0.011 -0.005 0.014 0.013 -0.003 -0.000 -0.001 -0.008 0.003
  -0.001]
 [-0.036 -0.036 -0.032 0.019 0.000 -0.019 -0.024 -0.054 1.513 -0.012
  -0.060 -0.096 -0.034 0.049 0.010 -0.005 -0.017 0.019 -0.034 -0.036
  -0.006 0.036 -0.035 0.008 0.070 0.005 0.030 0.022 -0.025 0.005 0.062
  -0.109 0.077 0.078 -0.063 -0.044 -0.005 -0.006 0.100 0.043 -0.001 0.003
  -0.063 0.002 0.007 0.022 -0.007 0.002 -0.013 0.002 0.021 0.007 0.001
  -0.006 0.003 0.004 0.003 0.006 0.004 0.002 -0.001 0.001 0.000 -0.002]
 [-0.033 0.053 0.031 -0.012 -0.062 -0.014 0.028 -0.020 -0.012 1.301 0.064
  -0.018 0.019 -0.024 0.006 -0.010 -0.019 -0.013 0.009 0.007 -0.005 0.027
  -0.013 -0.001 -0.114 0.081 0.013 -0.014 -0.006 -0.073 -0.021 -0.023
  -0.098 0.194 -0.034 -0.105 -0.002 0.069 -0.035 -0.016 -0.017 -0.014
  0.006 0.005 0.013 -0.018 0.017 -0.058 0.017 0.014 -0.003 -0.003 0.010
  -0.005 -0.004 0.001 0.008 -0.002 0.003 -0.005 -0.000 0.001 -0.000
  -0.003]
 [-0.227 0.101 0.170 -0.214 0.001 0.042 -0.023 -0.025 -0.060 0.064 1.477
  0.118 -0.016 0.013 0.040 -0.012 -0.066 -0.042 0.053 -0.014 0.002 0.032
  0.105 -0.009 -0.179 0.001 0.025 0.034 -0.010 -0.021 -0.076 0.151 0.136
  0.004 0.014 -0.131 -0.058 0.008 0.053 -0.021 -0.013 -0.040 -0.012
  -0.024 -0.002 -0.018 0.025 0.017 0.029 -0.001 -0.022 -0.010 -0.025
  0.000 -0.000 0.005 0.003 -0.014 -0.000 0.002 -0.002 0.007 0.001 0.002]
 [-0.235 0.111 0.141 -0.068 -0.067 -0.003 -0.049 -0.002 -0.096 -0.018
  0.118 1.364 0.001 0.052 -0.056 -0.006 -0.044 -0.006 0.036 -0.003 -0.005
  0.068 0.104 -0.015 -0.147 -0.044 -0.033 0.055 -0.024 -0.046 -0.059
  0.118 -0.055 0.069 0.004 -0.060 0.005 0.023 -0.001 -0.026 -0.027 0.001
  0.037 -0.002 -0.001 -0.012 0.004 0.031 0.027 0.018 -0.009 0.004 0.001
  -0.004 0.002 -0.005 0.006 -0.006 0.012 -0.002 0.001 0.002 -0.005 0.001]
 [0.085 0.044 -0.001 0.014 0.048 -0.062 -0.074 -0.004 -0.034 0.019 -0.016
  0.001 1.318 0.034 0.014 -0.070 0.054 0.023 0.009 0.023 -0.053 -0.027
  0.026 -0.027 0.002 0.025 -0.040 0.009 -0.063 0.064 0.017 0.059 0.074
  0.166 -0.054 0.263 0.019 -0.099 0.005 0.059 0.034 0.031 0.054 -0.074
  0.004 -0.006 -0.032 0.016 -0.022 -0.019 0.018 0.022 0.006 0.013 -0.024
  0.005 -0.001 -0.012 0.001 -0.004 -0.002 0.001 -0.001 -0.002]
 [-0.021 0.008 -0.039 0.056 0.004 -0.026 -0.049 -0.005 0.049 -0.024 0.013
  0.052 0.034 1.274 0.103 0.010 -0.011 -0.059 -0.007 -0.014 0.047 0.025
  0.014 0.014 -0.088 -0.024 0.042 0.004 -0.015 0.045 -0.046 0.020 -0.008
  0.075 0.053 -0.126 -0.032 0.055 0.080 -0.032 -0.008 0.010 -0.026 0.075
  -0.030 0.006 0.065 0.023 -0.013 0.029 -0.014 -0.001 0.002 -0.015 0.003
  0.007 -0.003 -0.010 -0.001 -0.009 0.001 0.002 0.008 0.001]
 [0.046 0.017 -0.033 -0.003 -0.019 -0.023 0.016 -0.011 0.010 0.006 0.040
  -0.056 0.014 0.103 1.323 -0.026 0.024 0.013 0.036 0.017 0.047 0.035
  -0.009 -0.008 0.008 0.045 -0.019 0.007 -0.010 0.088 -0.039 -0.003
  -0.040 -0.008 -0.035 -0.119 -0.038 0.029 -0.010 -0.017 0.017 0.035
  0.029 0.003 0.008 0.007 -0.009 0.006 0.010 -0.016 0.008 0.014 -0.020
  -0.000 0.015 0.001 0.003 0.000 0.002 0.006 0.005 -0.003 0.000 -0.002]
 [-0.078 -0.011 0.075 -0.097 -0.022 0.012 -0.004 0.008 -0.005 -0.010
  -0.012 -0.006 -0.070 0.010 -0.026 1.253 0.011 -0.011 0.005 0.006 0.056
  -0.012 -0.019 0.014 0.006 0.031 -0.025 0.001 -0.096 -0.046 -0.008 0.110
  0.082 -0.070 -0.074 -0.046 0.085 0.050 -0.082 0.022 0.041 0.025 0.000
  0.023 -0.026 0.007 -0.020 0.011 -0.009 -0.004 -0.015 0.022 -0.010
  -0.005 0.002 -0.012 0.005 -0.003 0.006 0.000 -0.000 -0.001 -0.004
  -0.004]
 [0.135 -0.060 -0.016 0.098 0.060 0.054 -0.043 0.018 -0.017 -0.019 -0.066
  -0.044 0.054 -0.011 0.024 0.011 1.244 0.015 -0.068 -0.127 0.006 -0.011
  -0.052 -0.008 0.125 -0.066 -0.013 -0.019 0.045 0.016 -0.005 -0.021
  0.016 0.129 0.015 -0.084 0.055 -0.040 -0.065 -0.055 0.037 0.087 0.087
  0.000 0.029 0.013 -0.090 0.016 0.012 -0.023 -0.000 -0.046 0.008 0.035
  -0.007 -0.010 0.004 0.007 -0.018 -0.001 0.001 -0.001 0.009 -0.001]
 [0.102 0.023 0.008 0.021 -0.015 0.036 0.025 0.000 0.019 -0.013 -0.042
  -0.006 0.023 -0.059 0.013 -0.011 0.015 1.224 0.057 0.075 0.018 -0.037
  -0.005 0.010 -0.009 -0.023 -0.021 0.002 -0.065 -0.001 0.034 -0.022
  0.046 0.155 -0.005 -0.004 -0.097 -0.065 -0.050 0.046 0.051 -0.017
  -0.033 0.043 -0.006 0.028 -0.058 0.042 -0.001 -0.022 0.005 0.021 0.002
  0.011 -0.007 -0.009 0.004 0.004 -0.014 0.009 0.001 -0.008 0.001 -0.007]
 [-0.096 0.033 0.078 -0.083 -0.033 -0.001 -0.030 0.018 -0.034 0.009 0.053
  0.036 0.009 -0.007 0.036 0.005 -0.068 0.057 1.254 -0.014 -0.026 0.025
  -0.014 0.017 -0.097 0.020 0.043 -0.043 -0.008 -0.103 -0.036 -0.013
  0.024 -0.090 -0.032 0.136 0.058 -0.097 0.070 -0.143 0.007 -0.012 -0.018
  -0.004 -0.008 -0.042 0.077 -0.044 -0.023 0.068 -0.022 -0.029 -0.001
  0.018 0.009 -0.010 0.001 -0.006 0.004 -0.004 0.000 0.003 0.001 0.000]
 [0.071 -0.019 0.003 0.049 -0.007 -0.061 -0.054 0.037 -0.036 0.007 -0.014
  -0.003 0.023 -0.014 0.017 0.006 -0.127 0.075 -0.014 1.272 -0.006 0.003
  0.002 -0.020 0.009 0.023 -0.019 0.020 0.031 -0.066 0.024 -0.074 0.023
  -0.048 -0.031 0.078 -0.117 0.013 -0.068 0.012 -0.041 0.047 0.011 0.056
  -0.082 -0.020 0.048 0.087 0.020 0.019 0.012 -0.034 0.012 -0.009 0.003
  0.007 0.000 0.002 0.012 0.001 -0.004 0.005 -0.007 0.000]
 [-0.008 0.014 0.075 -0.109 -0.089 -0.047 0.029 -0.008 -0.006 -0.005
  0.002 -0.005 -0.053 0.047 0.047 0.056 0.006 0.018 -0.026 -0.006 1.275
  -0.039 -0.013 0.091 0.128 0.024 0.072 0.006 0.031 0.094 0.040 -0.009
  0.031 -0.016 -0.003 0.006 0.029 -0.002 0.012 0.081 0.040 -0.016 0.131
  0.047 -0.018 -0.009 0.017 -0.053 -0.015 -0.047 -0.006 0.032 -0.004
  -0.006 -0.001 0.004 0.001 0.011 -0.005 0.003 0.002 -0.002 -0.004 0.007]
 [-0.277 0.059 0.107 -0.200 -0.046 -0.019 0.097 -0.021 0.036 0.027 0.032
  0.068 -0.027 0.025 0.035 -0.012 -0.011 -0.037 0.025 0.003 -0.039 0.970
  -0.329 -0.011 0.019 -0.060 -0.088 -0.045 -0.074 -0.039 -0.035 -0.019
  0.010 0.017 0.013 -0.107 0.060 -0.046 -0.001 0.004 -0.195 -0.030 0.002
  -0.010 -0.017 0.025 -0.034 0.043 0.010 0.023 0.013 0.001 -0.017 0.008
  0.011 0.005 0.003 -0.001 0.004 0.005 -0.001 0.000 0.003 -0.005]
 [-0.375 0.073 0.103 -0.334 -0.036 -0.028 0.064 -0.017 -0.035 -0.013
  0.105 0.104 0.026 0.014 -0.009 -0.019 -0.052 -0.005 -0.014 0.002 -0.013
  -0.329 1.020 -0.068 -0.136 0.034 -0.043 -0.018 -0.029 0.010 -0.055
  0.141 -0.009 -0.023 0.022 0.057 -0.062 0.006 0.015 -0.018 0.117 0.044
  0.008 0.046 0.042 -0.053 -0.027 -0.049 0.035 -0.006 -0.027 -0.016 0.010
  0.002 -0.012 -0.003 -0.000 0.000 0.006 -0.007 0.000 0.002 -0.003 0.000]
 [-0.106 0.015 0.016 -0.015 -0.014 -0.001 0.017 0.042 0.008 -0.001 -0.009
  -0.015 -0.027 0.014 -0.008 0.014 -0.008 0.010 0.017 -0.020 0.091 -0.011
  -0.068 0.879 0.023 -0.069 -0.070 0.078 -0.032 0.012 0.001 -0.038 0.061
  0.009 0.004 0.053 -0.055 0.008 -0.020 -0.055 -0.075 0.027 0.035 -0.012
  0.020 0.008 0.058 -0.060 0.015 0.001 0.019 -0.011 -0.014 0.007 0.014
  -0.001 0.003 -0.001 0.002 -0.007 0.002 -0.006 -0.002 0.005]
 [0.252 -0.156 -0.178 0.372 0.100 -0.000 -0.077 0.038 0.070 -0.114 -0.179
  -0.147 0.002 -0.088 0.008 0.006 0.125 -0.009 -0.097 0.009 0.128 0.019
  -0.136 0.023 0.598 0.132 -0.104 -0.167 0.087 0.051 0.028 -0.018 0.014
  0.017 0.008 0.045 0.005 -0.033 -0.029 0.029 -0.014 0.036 0.015 0.005
  0.000 0.002 -0.013 -0.024 -0.033 -0.012 0.026 -0.031 -0.007 0.006
  -0.004 -0.004 -0.010 0.006 -0.005 0.002 -0.001 0.001 -0.002 -0.002]
 [0.143 -0.181 -0.219 0.436 0.043 0.114 0.033 -0.012 0.005 0.081 0.001
  -0.044 0.025 -0.024 0.045 0.031 -0.066 -0.023 0.020 0.023 0.024 -0.060
  0.034 -0.069 0.132 0.639 -0.188 -0.289 0.128 0.015 0.025 0.027 -0.007
  0.008 0.008 0.035 -0.040 -0.065 -0.002 0.025 -0.010 0.036 -0.026 0.006
  -0.001 -0.017 0.002 -0.098 -0.000 0.002 -0.022 0.008 -0.019 -0.009
  0.011 0.000 -0.007 -0.007 0.007 -0.006 0.003 0.004 0.004 -0.006]
 [0.130 0.017 -0.133 0.179 -0.031 0.069 0.038 -0.003 0.030 0.013 0.025
  -0.033 -0.040 0.042 -0.019 -0.025 -0.013 -0.021 0.043 -0.019 0.072
  -0.088 -0.043 -0.070 -0.104 -0.188 0.515 0.225 -0.039 0.021 0.051
  -0.056 -0.012 -0.044 -0.026 -0.019 0.023 0.025 0.017 -0.001 0.038
  -0.044 -0.006 0.018 -0.004 0.013 -0.004 0.024 0.009 -0.077 -0.009 0.018
  0.027 -0.011 -0.014 0.005 0.004 0.009 -0.005 0.001 -0.004 0.002 -0.002
  0.003]
 [-0.129 -0.043 0.037 -0.177 -0.015 -0.060 0.003 0.002 0.022 -0.014 0.034
  0.055 0.009 0.004 0.007 0.001 -0.019 0.002 -0.043 0.020 0.006 -0.045
  -0.018 0.078 -0.167 -0.289 0.225 0.452 -0.092 -0.010 0.017 -0.047 0.001
  -0.012 -0.015 -0.018 0.013 0.045 0.011 -0.015 0.027 -0.029 0.003 0.005
  -0.003 -0.004 0.008 0.054 0.008 -0.052 0.010 0.032 0.006 -0.003 0.003
  0.003 0.009 0.008 0.003 0.002 -0.000 -0.001 -0.003 0.006]
 [0.208 -0.048 -0.089 0.233 0.161 -0.041 -0.041 0.088 -0.025 -0.006
  -0.010 -0.024 -0.063 -0.015 -0.010 -0.096 0.045 -0.065 -0.008 0.031
  0.031 -0.074 -0.029 -0.032 0.087 0.128 -0.039 -0.092 0.359 0.003 0.040
  0.080 -0.014 0.003 0.000 0.007 -0.009 -0.011 0.001 -0.011 -0.004 0.015
  -0.002 0.028 0.000 0.006 0.013 -0.029 -0.006 -0.002 -0.004 -0.003 0.000
  -0.004 0.002 -0.000 -0.008 0.002 -0.003 0.000 -0.000 0.004 0.001 0.001]
 [0.078 0.004 0.030 0.148 0.146 -0.032 -0.006 0.077 0.005 -0.073 -0.021
  -0.046 0.064 0.045 0.088 -0.046 0.016 -0.001 -0.103 -0.066 0.094 -0.039
  0.010 0.012 0.051 0.015 0.021 -0.010 0.003 0.607 0.054 0.049 -0.021
  -0.000 0.020 0.005 0.022 0.016 -0.003 0.011 -0.009 0.018 0.002 0.023
  0.001 0.010 0.007 0.009 -0.009 -0.044 0.003 0.026 -0.006 0.004 -0.009
  -0.002 -0.004 0.001 -0.012 0.005 -0.002 -0.001 0.002 0.002]
 [0.116 -0.096 -0.020 0.135 0.010 -0.004 0.026 -0.001 0.062 -0.021 -0.076
  -0.059 0.017 -0.046 -0.039 -0.008 -0.005 0.034 -0.036 0.024 0.040
  -0.035 -0.055 0.001 0.028 0.025 0.051 0.017 0.040 0.054 0.241 -0.072
  -0.005 0.011 -0.017 0.042 0.012 -0.000 -0.001 0.011 -0.004 0.015 -0.009
  0.003 -0.006 -0.002 -0.008 -0.006 -0.018 -0.014 0.006 0.016 0.010
  -0.002 -0.005 -0.001 -0.001 0.007 -0.004 0.002 -0.003 0.001 -0.001
  0.002]
 [-0.161 0.079 0.063 -0.046 0.057 -0.058 -0.042 0.053 -0.109 -0.023 0.151
  0.118 0.059 0.020 -0.003 0.110 -0.021 -0.022 -0.013 -0.074 -0.009
  -0.019 0.141 -0.038 -0.018 0.027 -0.056 -0.047 0.080 0.049 -0.072 0.501
  0.040 0.006 0.013 -0.015 0.010 0.017 0.003 -0.017 0.018 0.014 0.016
  0.013 0.005 -0.008 0.018 -0.001 -0.001 -0.000 -0.008 0.000 -0.013
  -0.002 -0.008 -0.011 -0.004 -0.009 0.003 0.004 -0.002 0.004 -0.004
  -0.002]
 [-0.017 0.032 0.067 -0.066 -0.040 -0.010 0.022 -0.039 0.077 -0.098 0.136
  -0.055 0.074 -0.008 -0.040 0.082 0.016 0.046 0.024 0.023 0.031 0.010
  -0.009 0.061 0.014 -0.007 -0.012 0.001 -0.014 -0.021 -0.005 0.040 0.313
  -0.031 -0.036 0.008 -0.013 -0.021 0.011 0.008 -0.008 -0.016 -0.005
  0.002 -0.005 -0.004 0.003 -0.001 0.002 0.009 -0.004 -0.007 -0.009 0.006
  -0.000 0.001 0.000 -0.004 -0.004 0.002 -0.001 -0.002 0.000 0.000]
 [-0.005 0.012 0.026 0.002 -0.006 -0.019 -0.039 0.014 0.078 0.194 0.004
  0.069 0.166 0.075 -0.008 -0.070 0.129 0.155 -0.090 -0.048 -0.016 0.017
  -0.023 0.009 0.017 0.008 -0.044 -0.012 0.003 -0.000 0.011 0.006 -0.031
  0.493 -0.020 -0.039 0.012 -0.005 -0.014 0.012 0.018 0.025 0.014 0.004
  0.008 0.006 -0.003 0.005 -0.005 0.008 0.014 0.004 0.004 0.014 -0.007
  -0.002 0.004 -0.000 -0.007 -0.001 0.000 -0.003 0.002 -0.003]
 [-0.004 -0.025 0.025 0.001 0.017 -0.011 0.030 -0.018 -0.063 -0.034 0.014
  0.004 -0.054 0.053 -0.035 -0.074 0.015 -0.005 -0.032 -0.031 -0.003
  0.013 0.022 0.004 0.008 0.008 -0.026 -0.015 0.000 0.020 -0.017 0.013
  -0.036 -0.020 0.101 -0.003 -0.016 0.002 -0.004 -0.002 -0.011 0.003
  -0.000 0.002 0.001 -0.005 0.006 0.003 0.002 0.003 -0.011 -0.007 -0.009
  0.008 0.005 -0.003 -0.003 -0.001 -0.004 -0.001 0.001 -0.001 0.002 0.000]
 [0.179 -0.081 -0.049 -0.004 0.025 -0.026 0.095 0.002 -0.044 -0.105
  -0.131 -0.060 0.263 -0.126 -0.119 -0.046 -0.084 -0.004 0.136 0.078
  0.006 -0.107 0.057 0.053 0.045 0.035 -0.019 -0.018 0.007 0.005 0.042
  -0.015 0.008 -0.039 -0.003 0.608 0.015 -0.054 -0.025 -0.000 0.040 0.001
  0.012 -0.009 0.006 0.002 -0.019 -0.002 -0.017 -0.023 0.010 0.024 0.001
  0.018 -0.004 -0.006 -0.006 0.001 0.002 0.001 -0.001 -0.004 -0.005 0.001]
 [0.011 -0.041 0.006 -0.064 0.009 -0.017 -0.005 0.039 -0.005 -0.002
  -0.058 0.005 0.019 -0.032 -0.038 0.085 0.055 -0.097 0.058 -0.117 0.029
  0.060 -0.062 -0.055 0.005 -0.040 0.023 0.013 -0.009 0.022 0.012 0.010
  -0.013 0.012 -0.016 0.015 0.283 -0.018 -0.078 -0.010 0.014 0.005 0.005
  -0.006 -0.002 0.011 -0.001 0.000 -0.007 -0.001 0.001 0.005 0.008 0.009
  -0.004 -0.002 -0.001 0.005 -0.002 0.002 -0.002 0.001 -0.002 -0.000]
 [-0.020 0.062 0.085 -0.050 0.013 0.011 0.013 0.071 -0.006 0.069 0.008
  0.023 -0.099 0.055 0.029 0.050 -0.040 -0.065 -0.097 0.013 -0.002 -0.046
  0.006 0.008 -0.033 -0.065 0.025 0.045 -0.011 0.016 -0.000 0.017 -0.021
  -0.005 0.002 -0.054 -0.018 0.325 -0.023 -0.045 0.009 -0.015 -0.014
  0.024 -0.001 -0.001 -0.006 -0.001 0.015 -0.006 0.002 0.008 -0.004
  -0.002 0.004 -0.002 0.004 0.002 0.001 0.000 -0.000 -0.002 -0.005 0.002]
 [-0.100 0.062 0.023 0.001 0.029 0.020 0.025 0.056 0.100 -0.035 0.053
  -0.001 0.005 0.080 -0.010 -0.082 -0.065 -0.050 0.070 -0.068 0.012
  -0.001 0.015 -0.020 -0.029 -0.002 0.017 0.011 0.001 -0.003 -0.001 0.003
  0.011 -0.014 -0.004 -0.025 -0.078 -0.023 0.279 -0.047 -0.018 -0.021
  -0.031 -0.005 -0.004 -0.002 0.020 -0.003 0.007 0.011 0.007 -0.005 0.001
  -0.006 -0.001 0.007 -0.000 -0.005 0.003 -0.003 -0.000 0.003 0.003 0.001]
 [-0.021 -0.036 -0.047 0.020 -0.023 -0.002 -0.027 -0.047 0.043 -0.016
  -0.021 -0.026 0.059 -0.032 -0.017 0.022 -0.055 0.046 -0.143 0.012 0.081
  0.004 -0.018 -0.055 0.029 0.025 -0.001 -0.015 -0.011 0.011 0.011 -0.017
  0.008 0.012 -0.002 -0.000 -0.010 -0.045 -0.047 0.196 0.013 -0.003 0.011
  -0.016 -0.005 0.005 -0.008 0.009 -0.011 -0.020 0.007 0.015 0.005 -0.016
  -0.007 0.011 -0.003 0.004 0.004 0.001 -0.001 0.002 -0.003 -0.001]
 [0.087 -0.048 -0.015 -0.119 0.027 -0.001 -0.009 -0.011 -0.001 -0.017
  -0.013 -0.027 0.034 -0.008 0.017 0.041 0.037 0.051 0.007 -0.041 0.040
  -0.195 0.117 -0.075 -0.014 -0.010 0.038 0.027 -0.004 -0.009 -0.004
  0.018 -0.008 0.018 -0.011 0.040 0.014 0.009 -0.018 0.013 0.246 0.021
  -0.001 -0.001 0.007 -0.004 -0.013 0.005 -0.010 -0.024 -0.007 0.010
  0.006 0.004 -0.004 -0.008 -0.001 0.004 -0.004 0.003 0.003 -0.003 0.000
  0.002]
 [0.023 -0.017 0.031 0.053 0.034 -0.010 -0.036 -0.039 0.003 -0.014 -0.040
  0.001 0.031 0.010 0.035 0.025 0.087 -0.017 -0.012 0.047 -0.016 -0.030
  0.044 0.027 0.036 0.036 -0.044 -0.029 0.015 0.018 0.015 0.014 -0.016
  0.025 0.003 0.001 0.005 -0.015 -0.021 -0.003 0.021 0.110 0.058 0.008
  0.003 0.002 -0.001 0.014 -0.008 0.014 -0.002 -0.018 -0.002 0.007 0.003
  -0.006 -0.002 -0.000 -0.000 -0.002 -0.000 0.001 0.001 0.000]
 [-0.009 0.077 0.055 -0.043 -0.032 0.003 -0.002 -0.069 -0.063 0.006
  -0.012 0.037 0.054 -0.026 0.029 0.000 0.087 -0.033 -0.018 0.011 0.131
  0.002 0.008 0.035 0.015 -0.026 -0.006 0.003 -0.002 0.002 -0.009 0.016
  -0.005 0.014 -0.000 0.012 0.005 -0.014 -0.031 0.011 -0.001 0.058 0.166
  -0.007 0.004 0.004 -0.023 0.002 -0.010 -0.007 -0.001 0.005 0.002 0.001
  -0.000 -0.000 0.001 0.001 -0.002 0.000 0.001 0.000 -0.001 0.003]
 [0.041 -0.005 -0.015 0.028 0.027 -0.074 -0.008 -0.015 0.002 0.005 -0.024
  -0.002 -0.074 0.075 0.003 0.023 0.000 0.043 -0.004 0.056 0.047 -0.010
  0.046 -0.012 0.005 0.006 0.018 0.005 0.028 0.023 0.003 0.013 0.002
  0.004 0.002 -0.009 -0.006 0.024 -0.005 -0.016 -0.001 0.008 -0.007 0.100
  -0.010 0.006 0.007 0.010 -0.006 0.005 -0.004 0.006 0.004 -0.006 0.000
  -0.001 -0.003 0.003 -0.001 0.000 -0.001 0.001 -0.001 -0.000]
 [0.022 0.006 -0.001 -0.001 -0.016 -0.060 -0.045 -0.044 0.007 0.013
  -0.002 -0.001 0.004 -0.030 0.008 -0.026 0.029 -0.006 -0.008 -0.082
  -0.018 -0.017 0.042 0.020 0.000 -0.001 -0.004 -0.003 0.000 0.001 -0.006
  0.005 -0.005 0.008 0.001 0.006 -0.002 -0.001 -0.004 -0.005 0.007 0.003
  0.004 -0.010 0.057 0.001 -0.019 -0.024 -0.013 0.006 -0.002 -0.004 0.001
  0.002 -0.002 -0.003 -0.001 0.001 -0.003 -0.001 0.001 -0.000 0.000 0.000]
 [0.056 0.036 -0.022 0.038 -0.002 -0.005 0.001 0.023 0.022 -0.018 -0.018
  -0.012 -0.006 0.006 0.007 0.007 0.013 0.028 -0.042 -0.020 -0.009 0.025
  -0.053 0.008 0.002 -0.017 0.013 -0.004 0.006 0.010 -0.002 -0.008 -0.004
  0.006 -0.005 0.002 0.011 -0.001 -0.002 0.005 -0.004 0.002 0.004 0.006
  0.001 0.046 -0.014 0.032 -0.008 -0.011 0.007 0.006 -0.002 -0.002 0.003
  -0.001 -0.001 0.000 -0.003 0.003 0.000 -0.001 0.001 -0.001]
 [-0.055 -0.006 0.008 0.036 0.003 -0.066 -0.105 -0.033 -0.007 0.017 0.025
  0.004 -0.032 0.065 -0.009 -0.020 -0.090 -0.058 0.077 0.048 0.017 -0.034
  -0.027 0.058 -0.013 0.002 -0.004 0.008 0.013 0.007 -0.008 0.018 0.003
  -0.003 0.006 -0.019 -0.001 -0.006 0.020 -0.008 -0.013 -0.001 -0.023
  0.007 -0.019 -0.014 0.123 -0.020 -0.015 0.031 -0.003 -0.020 -0.003
  -0.009 0.005 0.001 -0.002 -0.004 0.004 -0.002 -0.000 0.002 -0.002 0.004]
 [0.024 0.030 0.051 -0.093 0.037 -0.059 -0.042 -0.036 0.002 -0.058 0.017
  0.031 0.016 0.023 0.006 0.011 0.016 0.042 -0.044 0.087 -0.053 0.043
  -0.049 -0.060 -0.024 -0.098 0.024 0.054 -0.029 0.009 -0.006 -0.001
  -0.001 0.005 0.003 -0.002 0.000 -0.001 -0.003 0.009 0.005 0.014 0.002
  0.010 -0.024 0.032 -0.020 0.151 -0.024 -0.006 0.009 0.013 0.002 -0.000
  0.000 0.000 0.001 -0.001 -0.000 0.003 -0.002 0.001 0.001 -0.000]
 [-0.004 0.018 0.011 -0.103 0.018 0.207 0.132 0.154 -0.013 0.017 0.029
  0.027 -0.022 -0.013 0.010 -0.009 0.012 -0.001 -0.023 0.020 -0.015 0.010
  0.035 0.015 -0.033 -0.000 0.009 0.008 -0.006 -0.009 -0.018 -0.001 0.002
  -0.005 0.002 -0.017 -0.007 0.015 0.007 -0.011 -0.010 -0.008 -0.010
  -0.006 -0.013 -0.008 -0.015 -0.024 0.086 -0.029 -0.014 -0.009 -0.006
  0.023 0.003 -0.001 0.008 0.002 0.000 0.001 0.002 -0.005 0.001 -0.000]
 [0.007 0.045 0.088 -0.051 -0.011 -0.120 -0.137 -0.133 0.002 0.014 -0.001
  0.018 -0.019 0.029 -0.016 -0.004 -0.023 -0.022 0.068 0.019 -0.047 0.023
  -0.006 0.001 -0.012 0.002 -0.077 -0.052 -0.002 -0.044 -0.014 -0.000
  0.009 0.008 0.003 -0.023 -0.001 -0.006 0.011 -0.020 -0.024 0.014 -0.007
  0.005 0.006 -0.011 0.031 -0.006 -0.029 0.101 0.001 -0.020 -0.001 -0.005
  0.004 0.000 -0.004 -0.007 0.001 -0.001 0.001 0.001 0.000 -0.001]
 [-0.031 -0.029 -0.060 0.030 -0.007 -0.041 -0.023 0.046 0.021 -0.003
  -0.022 -0.009 0.018 -0.014 0.008 -0.015 -0.000 0.005 -0.022 0.012
  -0.006 0.013 -0.027 0.019 0.026 -0.022 -0.009 0.010 -0.004 0.003 0.006
  -0.008 -0.004 0.014 -0.011 0.010 0.001 0.002 0.007 0.007 -0.007 -0.002
  -0.001 -0.004 -0.002 0.007 -0.003 0.009 -0.014 0.001 0.032 -0.004 0.006
  -0.011 -0.005 0.007 -0.003 0.001 0.004 -0.000 -0.002 0.002 -0.002
  -0.000]
 [0.014 -0.019 0.012 -0.023 0.050 -0.002 0.049 -0.050 0.007 -0.003 -0.010
  0.004 0.022 -0.001 0.014 0.022 -0.046 0.021 -0.029 -0.034 0.032 0.001
  -0.016 -0.011 -0.031 0.008 0.018 0.032 -0.003 0.026 0.016 0.000 -0.007
  0.004 -0.007 0.024 0.005 0.008 -0.005 0.015 0.010 -0.018 0.005 0.006
  -0.004 0.006 -0.020 0.013 -0.009 -0.020 -0.004 0.056 -0.003 -0.008
  0.004 0.002 0.001 0.000 0.002 0.002 0.001 -0.001 0.000 0.000]
 [0.002 -0.011 -0.071 0.063 -0.005 -0.011 -0.018 -0.007 0.001 0.010
  -0.025 0.001 0.006 0.002 -0.020 -0.010 0.008 0.002 -0.001 0.012 -0.004
  -0.017 0.010 -0.014 -0.007 -0.019 0.027 0.006 0.000 -0.006 0.010 -0.013
  -0.009 0.004 -0.009 0.001 0.008 -0.004 0.001 0.005 0.006 -0.002 0.002
  0.004 0.001 -0.002 -0.003 0.002 -0.006 -0.001 0.006 -0.003 0.020 -0.012
  -0.013 0.007 -0.001 0.002 0.000 -0.003 -0.003 0.004 -0.001 0.000]
 [0.063 0.023 0.088 -0.095 0.006 0.046 0.048 0.055 -0.006 -0.005 0.000
  -0.004 0.013 -0.015 -0.000 -0.005 0.035 0.011 0.018 -0.009 -0.006 0.008
  0.002 0.007 0.006 -0.009 -0.011 -0.003 -0.004 0.004 -0.002 -0.002 0.006
  0.014 0.008 0.018 0.009 -0.002 -0.006 -0.016 0.004 0.007 0.001 -0.006
  0.002 -0.002 -0.009 -0.000 0.023 -0.005 -0.011 -0.008 -0.012 0.027
  0.006 -0.010 0.004 0.001 -0.005 0.003 0.002 -0.005 0.002 -0.000]
 [0.004 0.005 0.048 -0.041 0.001 0.009 0.015 -0.011 0.003 -0.004 -0.000
  0.002 -0.024 0.003 0.015 0.002 -0.007 -0.007 0.009 0.003 -0.001 0.011
  -0.012 0.014 -0.004 0.011 -0.014 0.003 0.002 -0.009 -0.005 -0.008
  -0.000 -0.007 0.005 -0.004 -0.004 0.004 -0.001 -0.007 -0.004 0.003
  -0.000 0.000 -0.002 0.003 0.005 0.000 0.003 0.004 -0.005 0.004 -0.013
  0.006 0.013 -0.004 0.002 -0.000 0.002 0.001 0.002 -0.002 0.001 0.000]
 [-0.026 -0.016 -0.049 0.028 0.001 0.004 -0.008 -0.005 0.004 0.001 0.005
  -0.005 0.005 0.007 0.001 -0.012 -0.010 -0.009 -0.010 0.007 0.004 0.005
  -0.003 -0.001 -0.004 0.000 0.005 0.003 -0.000 -0.002 -0.001 -0.011
  0.001 -0.002 -0.003 -0.006 -0.002 -0.002 0.007 0.011 -0.008 -0.006
  -0.000 -0.001 -0.003 -0.001 0.001 0.000 -0.001 0.000 0.007 0.002 0.007
  -0.010 -0.004 0.008 -0.001 -0.000 0.003 -0.002 -0.001 0.002 -0.000
  -0.000]
 [-0.008 0.010 0.023 -0.035 -0.003 0.024 0.008 0.014 0.003 0.008 0.003
  0.006 -0.001 -0.003 0.003 0.005 0.004 0.004 0.001 0.000 0.001 0.003
  -0.000 0.003 -0.010 -0.007 0.004 0.009 -0.008 -0.004 -0.001 -0.004
  0.000 0.004 -0.003 -0.006 -0.001 0.004 -0.000 -0.003 -0.001 -0.002
  0.001 -0.003 -0.001 -0.001 -0.002 0.001 0.008 -0.004 -0.003 0.001
  -0.001 0.004 0.002 -0.001 0.003 0.000 0.000 0.000 0.001 -0.001 0.000
  0.000]
 [0.009 -0.013 -0.007 -0.001 -0.001 0.005 0.007 0.013 0.006 -0.002 -0.014
  -0.006 -0.012 -0.010 0.000 -0.003 0.007 0.004 -0.006 0.002 0.011 -0.001
  0.000 -0.001 0.006 -0.007 0.009 0.008 0.002 0.001 0.007 -0.009 -0.004
  -0.000 -0.001 0.001 0.005 0.002 -0.005 0.004 0.004 -0.000 0.001 0.003
  0.001 0.000 -0.004 -0.001 0.002 -0.007 0.001 0.000 0.002 0.001 -0.000
  -0.000 0.000 0.003 -0.000 0.000 -0.000 -0.000 -0.000 0.000]
 [-0.039 -0.009 -0.014 -0.003 -0.001 0.005 -0.004 -0.003 0.004 0.003
  -0.000 0.012 0.001 -0.001 0.002 0.006 -0.018 -0.014 0.004 0.012 -0.005
  0.004 0.006 0.002 -0.005 0.007 -0.005 0.003 -0.003 -0.012 -0.004 0.003
  -0.004 -0.007 -0.004 0.002 -0.002 0.001 0.003 0.004 -0.004 -0.000
  -0.002 -0.001 -0.003 -0.003 0.004 -0.000 0.000 0.001 0.004 0.002 0.000
  -0.005 0.002 0.003 0.000 -0.000 0.004 -0.001 0.000 0.001 -0.001 -0.000]
 [0.022 0.006 0.020 -0.019 0.001 0.002 0.009 -0.000 0.002 -0.005 0.002
  -0.002 -0.004 -0.009 0.006 0.000 -0.001 0.009 -0.004 0.001 0.003 0.005
  -0.007 -0.007 0.002 -0.006 0.001 0.002 0.000 0.005 0.002 0.004 0.002
  -0.001 -0.001 0.001 0.002 0.000 -0.003 0.001 0.003 -0.002 0.000 0.000
  -0.001 0.003 -0.002 0.003 0.001 -0.001 -0.000 0.002 -0.003 0.003 0.001
  -0.002 0.000 0.000 -0.001 0.002 0.000 -0.001 -0.000 -0.000]
 [0.005 0.001 0.007 -0.015 0.000 0.006 0.002 -0.001 -0.001 -0.000 -0.002
  0.001 -0.002 0.001 0.005 -0.000 0.001 0.001 0.000 -0.004 0.002 -0.001
  0.000 0.002 -0.001 0.003 -0.004 -0.000 -0.000 -0.002 -0.003 -0.002
  -0.001 0.000 0.001 -0.001 -0.002 -0.000 -0.000 -0.001 0.003 -0.000
  0.001 -0.001 0.001 0.000 -0.000 -0.002 0.002 0.001 -0.002 0.001 -0.003
  0.002 0.002 -0.001 0.001 -0.000 0.000 0.000 0.001 -0.001 0.000 0.000]
 [-0.020 -0.007 -0.025 0.032 0.003 -0.011 -0.014 -0.008 0.001 0.001 0.007
  0.002 0.001 0.002 -0.003 -0.001 -0.001 -0.008 0.003 0.005 -0.002 0.000
  0.002 -0.006 0.001 0.004 0.002 -0.001 0.004 -0.001 0.001 0.004 -0.002
  -0.003 -0.001 -0.004 0.001 -0.002 0.003 0.002 -0.003 0.001 0.000 0.001
  -0.000 -0.001 0.002 0.001 -0.005 0.001 0.002 -0.001 0.004 -0.005 -0.002
  0.002 -0.001 -0.000 0.001 -0.001 -0.001 0.002 -0.000 -0.000]
 [0.008 -0.002 0.004 -0.002 -0.001 0.003 0.006 0.003 0.000 -0.000 0.001
  -0.005 -0.001 0.008 0.000 -0.004 0.009 0.001 0.001 -0.007 -0.004 0.003
  -0.003 -0.002 -0.002 0.004 -0.002 -0.003 0.001 0.002 -0.001 -0.004
  0.000 0.002 0.002 -0.005 -0.002 -0.005 0.003 -0.003 0.000 0.001 -0.001
  -0.001 0.000 0.001 -0.002 0.001 0.001 0.000 -0.002 0.000 -0.001 0.002
  0.001 -0.000 0.000 -0.000 -0.001 -0.000 0.000 -0.000 0.001 -0.000]
 [-0.003 0.001 0.006 -0.012 0.003 -0.001 -0.005 -0.001 -0.002 -0.003
  0.002 0.001 -0.002 0.001 -0.002 -0.004 -0.001 -0.007 0.000 0.000 0.007
  -0.005 0.000 0.005 -0.002 -0.006 0.003 0.006 0.001 0.002 0.002 -0.002
  0.000 -0.003 0.000 0.001 -0.000 0.002 0.001 -0.001 0.002 0.000 0.003
  -0.000 0.000 -0.001 0.004 -0.000 -0.000 -0.001 -0.000 0.000 0.000
  -0.000 0.000 -0.000 0.000 0.000 -0.000 -0.000 0.000 -0.000 -0.000 0.001]]
